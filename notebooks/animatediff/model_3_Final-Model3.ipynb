{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc3f08c-e3ce-47fa-bd0f-92ab0e071d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ANIMATEDIFF LORA FINE-TUNING - 50GB OPTIMIZED\n",
      "================================================================================\n",
      "\n",
      "PyTorch: 2.8.0+cu128\n",
      "CUDA: True\n",
      "GPU: NVIDIA H200\n",
      "VRAM: 139.7 GB\n",
      "\n",
      "Installing packages...\n",
      "  Installing diffusers...\n",
      "  Installing transformers...\n",
      "  Installing accelerate...\n",
      "  Installing safetensors...\n",
      "  Installing opencv-python...\n",
      "  Installing google-cloud-storage...\n",
      "  Installing peft...\n",
      "  Installing bitsandbytes...\n",
      "[OK] Packages installed\n",
      "\n",
      "Disk space:\n",
      "Filesystem                   Size  Used Avail Use% Mounted on\n",
      "mfs#us-nc-1.runpod.net:9421  699T  615T   84T  89% /workspace\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SETUP COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 1: ENVIRONMENT SETUP (LORA)\n",
    "# ==============================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANIMATEDIFF LORA FINE-TUNING - 50GB OPTIMIZED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f} GB\")\n",
    "\n",
    "# Install packages\n",
    "print(\"\\nInstalling packages...\")\n",
    "packages = [\n",
    "    \"diffusers==0.30.3\",\n",
    "    \"transformers==4.44.2\",\n",
    "    \"accelerate==0.34.2\",\n",
    "    \"safetensors\",\n",
    "    \"opencv-python\",\n",
    "    \"google-cloud-storage\",\n",
    "    \"peft==0.11.1\",  # LoRA library\n",
    "    \"bitsandbytes\",  # For 8-bit optimization\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    print(f\"  Installing {pkg.split('==')[0]}...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], check=False)\n",
    "\n",
    "print(\"[OK] Packages installed\")\n",
    "\n",
    "# Create directories\n",
    "dirs = [\n",
    "    '/workspace/anime_dataset/videos',\n",
    "    '/workspace/models',\n",
    "    '/workspace/training_outputs/checkpoints',\n",
    "    '/workspace/lora_outputs',\n",
    "]\n",
    "for d in dirs:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "os.environ['HF_HOME'] = '/workspace/models'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/models'\n",
    "\n",
    "result = subprocess.run(['df', '-h', '/workspace'], capture_output=True, text=True)\n",
    "print(\"\\nDisk space:\")\n",
    "print(result.stdout)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SETUP COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14e4e4fd-7e4c-4b8a-b1d5-1f3ffa9d87b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOWNLOADING DATASET\n",
      "================================================================================\n",
      "[OK] Connected to GCS\n",
      "\n",
      "Downloading videos (200 videos)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:29<00:00,  6.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Downloaded 200 videos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 851/851 [00:00<00:00, 2442.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Training dataset: 200 videos\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 2: DOWNLOAD DATASET (SKIP IF ALREADY DONE)\n",
    "# ==============================================================================\n",
    "\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if dataset already exists\n",
    "video_dir = Path('/workspace/anime_dataset/videos')\n",
    "metadata_path = Path('/workspace/anime_dataset/training_metadata.json')\n",
    "\n",
    "if video_dir.exists() and len(list(video_dir.glob('*.*'))) > 0:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DATASET ALREADY EXISTS - SKIPPING DOWNLOAD\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    video_count = len(list(video_dir.glob('*.*')))\n",
    "    print(f\"\\nVideos found: {video_count}\")\n",
    "    \n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        print(f\"Metadata entries: {len(metadata)}\")\n",
    "    \n",
    "    print(\"\\n[OK] Using existing dataset\")\n",
    "    print(\"Proceed to Cell 3\")\n",
    "\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DOWNLOADING DATASET\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    GCS_CREDENTIALS_PATH = '/workspace/fashiont2vteam5-14565b6f64d7.json'\n",
    "    \n",
    "    if not os.path.exists(GCS_CREDENTIALS_PATH):\n",
    "        print(f\"[ERROR] Upload credentials file to: {GCS_CREDENTIALS_PATH}\")\n",
    "        raise FileNotFoundError(f\"Missing: {GCS_CREDENTIALS_PATH}\")\n",
    "    \n",
    "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = GCS_CREDENTIALS_PATH\n",
    "    \n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket('model-3-dataset')\n",
    "    print(\"[OK] Connected to GCS\")\n",
    "    \n",
    "    print(\"\\nDownloading videos (200 videos)...\")\n",
    "    blobs = list(bucket.list_blobs(prefix='animation_videos/'))\n",
    "    video_blobs = [b for b in blobs if b.name.endswith(('.mp4', '.avi', '.mov', '.webm', '.gif'))][:200]\n",
    "    \n",
    "    for blob in tqdm(video_blobs, desc=\"Downloading\"):\n",
    "        local_path = video_dir / os.path.basename(blob.name)\n",
    "        if not local_path.exists():\n",
    "            blob.download_to_filename(str(local_path))\n",
    "    \n",
    "    print(f\"[OK] Downloaded {len(video_blobs)} videos\")\n",
    "    \n",
    "    metadata_blob = bucket.blob('dataset_pairs_valid.json')\n",
    "    metadata_blob.download_to_filename(str(metadata_path))\n",
    "    \n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    training_data = []\n",
    "    for entry in tqdm(metadata, desc=\"Processing\"):\n",
    "        video_id = entry['video_id']\n",
    "        caption = entry['caption']\n",
    "        \n",
    "        for ext in ['.mp4', '.avi', '.mov', '.webm', '.gif']:\n",
    "            potential_path = video_dir / f\"{video_id}{ext}\"\n",
    "            if potential_path.exists():\n",
    "                training_data.append({\n",
    "                    'video_path': str(potential_path),\n",
    "                    'caption': caption\n",
    "                })\n",
    "                break\n",
    "    \n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(training_data, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n[OK] Training dataset: {len(training_data)} videos\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ae606c0-a424-4e18-9dd3-76ae303c3cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOWNLOADING MODELS\n",
      "================================================================================\n",
      "\n",
      "[1/2] Stable Diffusion v1.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  vae/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 547/547 [00:00<00:00, 4.82MB/s]\n",
      "  vae/diffusion_pytorch_model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 335M/335M [00:01<00:00, 283MB/s] \n",
      "  text_encoder/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 617/617 [00:00<00:00, 6.79MB/s]\n",
      "  text_encoder/model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 492M/492M [00:01<00:00, 374MB/s] \n",
      "  tokenizer/tokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 806/806 [00:00<00:00, 10.1MB/s]\n",
      "  tokenizer/vocab.json: 1.06MB [00:00, 42.6MB/s]\n",
      "  tokenizer/merges.txt: 525kB [00:00, 14.9MB/s]\n",
      "  tokenizer/special_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 472/472 [00:00<00:00, 5.50MB/s]\n",
      "  unet/config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 743/743 [00:00<00:00, 9.71MB/s]\n",
      "  unet/diffusion_pytorch_model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.44G/3.44G [00:08<00:00, 387MB/s]\n",
      "  scheduler/scheduler_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 308/308 [00:00<00:00, 3.93MB/s]\n",
      "  model_index.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 541/541 [00:00<00:00, 7.42MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/2] AnimateDiff Motion Adapter...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 455/455 [00:00<00:00, 5.84MB/s]\n",
      "  diffusion_pytorch_model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.82G/1.82G [00:06<00:00, 300MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[OK] Models downloaded\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 3: DOWNLOAD MODELS (SKIP IF ALREADY DONE)\n",
    "# ==============================================================================\n",
    "\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "sd_dir = Path('/workspace/models/stable-diffusion-v1-5')\n",
    "motion_dir = Path('/workspace/models/animatediff-motion-adapter')\n",
    "\n",
    "# Check if already downloaded\n",
    "if (sd_dir / 'unet' / 'diffusion_pytorch_model.safetensors').exists() and \\\n",
    "   (motion_dir / 'diffusion_pytorch_model.safetensors').exists():\n",
    "    print(\"=\" * 80)\n",
    "    print(\"MODELS ALREADY DOWNLOADED - SKIPPING\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\n[OK] Using existing models\")\n",
    "    print(\"Proceed to Cell 4\")\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"DOWNLOADING MODELS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    def download_file(url, local_path, desc):\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        \n",
    "        with open(local_path, 'wb') as f:\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True, desc=desc) as pbar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "    \n",
    "    for d in [sd_dir, motion_dir]:\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    sd_base_url = \"https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/main\"\n",
    "    sd_files = {\n",
    "        'vae/config.json': f\"{sd_base_url}/vae/config.json\",\n",
    "        'vae/diffusion_pytorch_model.safetensors': f\"{sd_base_url}/vae/diffusion_pytorch_model.safetensors\",\n",
    "        'text_encoder/config.json': f\"{sd_base_url}/text_encoder/config.json\",\n",
    "        'text_encoder/model.safetensors': f\"{sd_base_url}/text_encoder/model.safetensors\",\n",
    "        'tokenizer/tokenizer_config.json': f\"{sd_base_url}/tokenizer/tokenizer_config.json\",\n",
    "        'tokenizer/vocab.json': f\"{sd_base_url}/tokenizer/vocab.json\",\n",
    "        'tokenizer/merges.txt': f\"{sd_base_url}/tokenizer/merges.txt\",\n",
    "        'tokenizer/special_tokens_map.json': f\"{sd_base_url}/tokenizer/special_tokens_map.json\",\n",
    "        'unet/config.json': f\"{sd_base_url}/unet/config.json\",\n",
    "        'unet/diffusion_pytorch_model.safetensors': f\"{sd_base_url}/unet/diffusion_pytorch_model.safetensors\",\n",
    "        'scheduler/scheduler_config.json': f\"{sd_base_url}/scheduler/scheduler_config.json\",\n",
    "        'model_index.json': f\"{sd_base_url}/model_index.json\",\n",
    "    }\n",
    "    \n",
    "    print(\"\\n[1/2] Stable Diffusion v1.5...\")\n",
    "    for rel_path, url in sd_files.items():\n",
    "        local_path = sd_dir / rel_path\n",
    "        local_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if not local_path.exists():\n",
    "            try:\n",
    "                download_file(url, str(local_path), f\"  {rel_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  [ERROR] {rel_path}: {e}\")\n",
    "    \n",
    "    print(\"\\n[2/2] AnimateDiff Motion Adapter...\")\n",
    "    motion_base_url = \"https://huggingface.co/guoyww/animatediff-motion-adapter-v1-5-2/resolve/main\"\n",
    "    motion_files = {\n",
    "        'config.json': f\"{motion_base_url}/config.json\",\n",
    "        'diffusion_pytorch_model.safetensors': f\"{motion_base_url}/diffusion_pytorch_model.safetensors\",\n",
    "    }\n",
    "    \n",
    "    for filename, url in motion_files.items():\n",
    "        local_path = motion_dir / filename\n",
    "        if not local_path.exists():\n",
    "            try:\n",
    "                download_file(url, str(local_path), f\"  {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  [ERROR] {filename}: {e}\")\n",
    "    \n",
    "    print(\"\\n[OK] Models downloaded\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ea6a42-fa45-42f8-b3b9-70ee5976489b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FILTERING BAD VIDEOS (FIXED)\n",
      "================================================================================\n",
      "\n",
      "Loading VAE for validation...\n",
      "[OK] VAE loaded with FP32 encoder on CUDA\n",
      "\n",
      "Validating 200 videos...\n",
      "This will test:\n",
      "  1. Video can be opened\n",
      "  2. Has enough frames (16+)\n",
      "  3. Has valid dimensions\n",
      "  4. Frames can be read\n",
      "  5. VAE can encode without NaN/Inf\n",
      "  6. Latent values are in reasonable range\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:02<00:00, 78.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "FILTERING RESULTS\n",
      "================================================================================\n",
      "‚úÖ Good videos: 0\n",
      "‚ùå Bad videos: 200\n",
      "\n",
      "Issues breakdown:\n",
      "  vae_encoding_failed: 200\n",
      "\n",
      "‚ùå NO VALID VIDEOS FOUND!\n",
      "\n",
      "This is unexpected since the diagnostic showed videos work fine.\n",
      "Please share this output so we can debug further.\n",
      "\n",
      "üìù Bad videos log: /workspace/anime_dataset/bad_videos_log.json\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 6: ENHANCED VIDEO FILTER - FIXED FOR YOUR DATASET\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from diffusers import AutoencoderKL\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FILTERING BAD VIDEOS (FIXED)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "SD_PATH = '/workspace/models/stable-diffusion-v1-5'\n",
    "\n",
    "# Load VAE for testing\n",
    "print(\"\\nLoading VAE for validation...\")\n",
    "vae_test = AutoencoderKL.from_pretrained(\n",
    "    SD_PATH, subfolder=\"vae\", torch_dtype=torch.float16, local_files_only=True\n",
    ").to(\"cuda\").eval()\n",
    "\n",
    "# CRITICAL: Ensure encoder is in float32 and on CUDA\n",
    "vae_test.encoder = vae_test.encoder.to(dtype=torch.float32, device=\"cuda\")\n",
    "print(\"[OK] VAE loaded with FP32 encoder on CUDA\")\n",
    "\n",
    "# Load metadata\n",
    "metadata_path = Path('/workspace/anime_dataset/training_metadata.json')\n",
    "with open(metadata_path, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"\\nValidating {len(metadata)} videos...\")\n",
    "print(\"This will test:\")\n",
    "print(\"  1. Video can be opened\")\n",
    "print(\"  2. Has enough frames (16+)\")\n",
    "print(\"  3. Has valid dimensions\")\n",
    "print(\"  4. Frames can be read\")\n",
    "print(\"  5. VAE can encode without NaN/Inf\")\n",
    "print(\"  6. Latent values are in reasonable range\")\n",
    "print()\n",
    "\n",
    "good_videos = []\n",
    "bad_videos = []\n",
    "issues = {\n",
    "    'cannot_open': 0,\n",
    "    'too_few_frames': 0,\n",
    "    'invalid_dimensions': 0,\n",
    "    'read_error': 0,\n",
    "    'vae_encoding_failed': 0,\n",
    "    'nan_in_latents': 0,\n",
    "    'extreme_values': 0,\n",
    "}\n",
    "\n",
    "for item in tqdm(metadata, desc=\"Filtering\"):\n",
    "    video_path = item['video_path']\n",
    "    \n",
    "    try:\n",
    "        # Test 1: Can open video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            issues['cannot_open'] += 1\n",
    "            bad_videos.append({'path': video_path, 'reason': 'cannot_open'})\n",
    "            cap.release()\n",
    "            continue\n",
    "        \n",
    "        # Test 2: Has enough frames\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames < 16:\n",
    "            issues['too_few_frames'] += 1\n",
    "            bad_videos.append({'path': video_path, 'reason': f'only_{total_frames}_frames'})\n",
    "            cap.release()\n",
    "            continue\n",
    "        \n",
    "        # Test 3: Valid dimensions\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        if width <= 0 or height <= 0:\n",
    "            issues['invalid_dimensions'] += 1\n",
    "            bad_videos.append({'path': video_path, 'reason': 'invalid_dimensions'})\n",
    "            cap.release()\n",
    "            continue\n",
    "        \n",
    "        # Test 4: Can read frames\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        \n",
    "        if not ret or frame is None:\n",
    "            issues['read_error'] += 1\n",
    "            bad_videos.append({'path': video_path, 'reason': 'cannot_read_frame'})\n",
    "            continue\n",
    "        \n",
    "        # Test 5: Frame has valid pixel values\n",
    "        if np.isnan(frame).any() or np.isinf(frame).any():\n",
    "            issues['read_error'] += 1\n",
    "            bad_videos.append({'path': video_path, 'reason': 'nan_in_pixels'})\n",
    "            continue\n",
    "        \n",
    "        # Test 6: VAE can encode - FIXED VERSION\n",
    "        try:\n",
    "            # Convert and normalize properly\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_resized = cv2.resize(frame_rgb, (256, 256), interpolation=cv2.INTER_LINEAR)\n",
    "            \n",
    "            # Normalize to [-1, 1] carefully\n",
    "            frame_normalized = frame_resized.astype(np.float32) / 127.5 - 1.0\n",
    "            frame_normalized = np.clip(frame_normalized, -1.0, 1.0)\n",
    "            \n",
    "            # Convert to tensor - CRITICAL: Use float32 for VAE encoder\n",
    "            frame_t = torch.from_numpy(frame_normalized).permute(2, 0, 1).unsqueeze(0)\n",
    "            frame_t = frame_t.to(device=\"cuda\", dtype=torch.float32)\n",
    "            \n",
    "            # Encode with VAE (encoder is in FP32)\n",
    "            with torch.no_grad():\n",
    "                latent_dist = vae_test.encode(frame_t).latent_dist\n",
    "                latent = latent_dist.sample()\n",
    "                \n",
    "                # Scale\n",
    "                latent = latent * 0.18215\n",
    "                \n",
    "                # Test 7: No NaN/Inf in latents\n",
    "                if torch.isnan(latent).any() or torch.isinf(latent).any():\n",
    "                    issues['nan_in_latents'] += 1\n",
    "                    bad_videos.append({'path': video_path, 'reason': 'nan_in_latents'})\n",
    "                    continue\n",
    "                \n",
    "                # Test 8: Reasonable latent values\n",
    "                max_val = latent.abs().max().item()\n",
    "                if max_val > 100:\n",
    "                    issues['extreme_values'] += 1\n",
    "                    bad_videos.append({'path': video_path, 'reason': f'extreme_values_{max_val:.1f}'})\n",
    "                    continue\n",
    "                \n",
    "        except RuntimeError as e:\n",
    "            # Catch CUDA/memory errors\n",
    "            issues['vae_encoding_failed'] += 1\n",
    "            bad_videos.append({'path': video_path, 'reason': f'vae_runtime_error'})\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            issues['vae_encoding_failed'] += 1\n",
    "            bad_videos.append({'path': video_path, 'reason': f'vae_error_{type(e).__name__}'})\n",
    "            continue\n",
    "        \n",
    "        # All tests passed!\n",
    "        good_videos.append(item)\n",
    "        \n",
    "    except Exception as e:\n",
    "        bad_videos.append({'path': video_path, 'reason': f'unknown_{type(e).__name__}'})\n",
    "        continue\n",
    "\n",
    "# Cleanup\n",
    "del vae_test\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Report\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FILTERING RESULTS\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"‚úÖ Good videos: {len(good_videos)}\")\n",
    "print(f\"‚ùå Bad videos: {len(bad_videos)}\")\n",
    "\n",
    "if len(good_videos) > 0:\n",
    "    print(f\"\\n‚úì Success rate: {len(good_videos)/len(metadata)*100:.1f}%\")\n",
    "\n",
    "if len(bad_videos) > 0:\n",
    "    print(\"\\nIssues breakdown:\")\n",
    "    for issue, count in issues.items():\n",
    "        if count > 0:\n",
    "            print(f\"  {issue}: {count}\")\n",
    "\n",
    "# Save filtered metadata\n",
    "if len(good_videos) > 0:\n",
    "    output_path = Path('/workspace/anime_dataset/training_metadata_filtered.json')\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(good_videos, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Saved {len(good_videos)} valid videos to:\")\n",
    "    print(f\"   {output_path}\")\n",
    "    print()\n",
    "    print(\"‚úì You can now re-run Cell 4 - it will automatically use the filtered dataset\")\n",
    "    print(\"‚úì Then proceed to Cell 5 for training\")\n",
    "else:\n",
    "    print(\"\\n‚ùå NO VALID VIDEOS FOUND!\")\n",
    "    print(\"\\nThis is unexpected since the diagnostic showed videos work fine.\")\n",
    "    print(\"Please share this output so we can debug further.\")\n",
    "\n",
    "# Save bad videos log\n",
    "if len(bad_videos) > 0:\n",
    "    bad_log_path = Path('/workspace/anime_dataset/bad_videos_log.json')\n",
    "    with open(bad_log_path, 'w') as f:\n",
    "        json.dump(bad_videos, f, indent=2)\n",
    "    print(f\"\\nüìù Bad videos log: {bad_log_path}\")\n",
    "\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2392bcdc-fd5d-4c1a-b01e-9fba0f0b509d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 total video files\n",
      "\n",
      "Detailed check of first 5 videos:\n",
      "\n",
      "File: video303.mp4\n",
      "  Size: 245.6 KB\n",
      "  Resolution: 298x224\n",
      "  FPS: 3.0\n",
      "  Frames: 62\n",
      "  Codec: 875967080\n",
      "  ‚úì Can read frames\n",
      "  Frame dtype: uint8, shape: (224, 298, 3)\n",
      "  Pixel range: [0, 255]\n",
      "\n",
      "File: video3025.mp4\n",
      "  Size: 649.3 KB\n",
      "  Resolution: 298x224\n",
      "  FPS: 3.0\n",
      "  Frames: 62\n",
      "  Codec: 875967080\n",
      "  ‚úì Can read frames\n",
      "  Frame dtype: uint8, shape: (224, 298, 3)\n",
      "  Pixel range: [0, 255]\n",
      "\n",
      "File: video3021.mp4\n",
      "  Size: 278.3 KB\n",
      "  Resolution: 298x224\n",
      "  FPS: 3.0\n",
      "  Frames: 32\n",
      "  Codec: 875967080\n",
      "  ‚úì Can read frames\n",
      "  Frame dtype: uint8, shape: (224, 298, 3)\n",
      "  Pixel range: [0, 255]\n",
      "\n",
      "File: video3012.mp4\n",
      "  Size: 165.8 KB\n",
      "  Resolution: 298x224\n",
      "  FPS: 3.0\n",
      "  Frames: 59\n",
      "  Codec: 875967080\n",
      "  ‚úì Can read frames\n",
      "  Frame dtype: uint8, shape: (224, 298, 3)\n",
      "  Pixel range: [0, 255]\n",
      "\n",
      "File: video3011.mp4\n",
      "  Size: 87.5 KB\n",
      "  Resolution: 298x224\n",
      "  FPS: 3.0\n",
      "  Frames: 32\n",
      "  Codec: 875967080\n",
      "  ‚úì Can read frames\n",
      "  Frame dtype: uint8, shape: (224, 298, 3)\n",
      "  Pixel range: [0, 255]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Check first video manually\n",
    "video_dir = Path('/workspace/anime_dataset/videos')\n",
    "video_files = list(video_dir.glob('*.*'))[:5]  # Check first 5\n",
    "\n",
    "print(f\"Found {len(video_files)} total video files\")\n",
    "print(\"\\nDetailed check of first 5 videos:\\n\")\n",
    "\n",
    "for video_path in video_files:\n",
    "    print(f\"File: {video_path.name}\")\n",
    "    print(f\"  Size: {video_path.stat().st_size / 1024:.1f} KB\")\n",
    "    \n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    \n",
    "    if cap.isOpened():\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        fourcc = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "        \n",
    "        print(f\"  Resolution: {width}x{height}\")\n",
    "        print(f\"  FPS: {fps}\")\n",
    "        print(f\"  Frames: {frame_count}\")\n",
    "        print(f\"  Codec: {fourcc}\")\n",
    "        \n",
    "        # Try to read first frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            print(f\"  ‚úì Can read frames\")\n",
    "            print(f\"  Frame dtype: {frame.dtype}, shape: {frame.shape}\")\n",
    "            print(f\"  Pixel range: [{frame.min()}, {frame.max()}]\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå Cannot read frames!\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Cannot open video!\")\n",
    "    \n",
    "    cap.release()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00d36b28-e39e-46bd-b536-e51f34d88bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING MODELS & SETTING UP LORA (FULL FP32 VAE)\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  batch_size: 1\n",
      "  num_frames: 16\n",
      "  resolution: 256\n",
      "  num_epochs: 10\n",
      "  learning_rate: 5e-05\n",
      "  gradient_accumulation_steps: 8\n",
      "  mixed_precision: False\n",
      "  gradient_checkpointing: True\n",
      "  save_every_n_epochs: 2\n",
      "  num_workers: 2\n",
      "  train_data_path: /workspace/anime_dataset/training_metadata.json\n",
      "  output_dir: /workspace/lora_outputs\n",
      "  lora_rank: 16\n",
      "  lora_alpha: 32\n",
      "  max_grad_norm: 0.5\n",
      "\n",
      "Loading models...\n",
      "  VAE (FULL FP32)...\n",
      "  [OK] VAE fully in FP32\n",
      "  Text Encoder...\n",
      "  Tokenizer...\n",
      "  UNet 2D...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'motion_activation_fn': 'geglu', 'motion_attention_bias': False, 'motion_cross_attention_dim': None} were passed to MotionAdapter, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Motion Adapter...\n",
      "  Creating Motion UNet...\n",
      "\n",
      "  Setting up LoRA...\n",
      "trainable params: 8,022,016 || all params: 1,320,752,260 || trainable%: 0.6074\n",
      "  Noise Scheduler...\n",
      "\n",
      "Creating dataset...\n",
      "  ‚ö†Ô∏è  Using unfiltered dataset\n",
      "  Validating 200 videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:01<00:00, 179.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] 200 valid videos\n",
      "[OK] Dataset: 200 videos\n",
      "[OK] Batches/epoch: 200\n",
      "  [OK] Using 8-bit AdamW\n",
      "\n",
      "GPU Memory: 3.3GB\n",
      "\n",
      "================================================================================\n",
      "READY TO TRAIN - VAE IN FULL FP32\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 4: LOAD MODELS & SETUP LORA (NUCLEAR FP32 FIX)\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, DDIMScheduler, UNet2DConditionModel\n",
    "from diffusers.models.unets.unet_motion_model import UNetMotionModel, MotionAdapter\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING MODELS & SETTING UP LORA (FULL FP32 VAE)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "SD_PATH = '/workspace/models/stable-diffusion-v1-5'\n",
    "MOTION_PATH = '/workspace/models/animatediff-motion-adapter'\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'batch_size': 1,\n",
    "    'num_frames': 16,\n",
    "    'resolution': 256,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 5e-5,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'mixed_precision': False,\n",
    "    'gradient_checkpointing': True,\n",
    "    'save_every_n_epochs': 2,\n",
    "    'num_workers': 2,\n",
    "    'train_data_path': '/workspace/anime_dataset/training_metadata.json',\n",
    "    'output_dir': '/workspace/lora_outputs',\n",
    "    'lora_rank': 16,\n",
    "    'lora_alpha': 32,\n",
    "    'max_grad_norm': 0.5,\n",
    "}\n",
    "\n",
    "print(\"\\nConfiguration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Dataset (same as before)\n",
    "class AnimeVideoDataset(Dataset):\n",
    "    def __init__(self, metadata_path, num_frames=16, resolution=256):\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        self.num_frames = num_frames\n",
    "        self.resolution = resolution\n",
    "        \n",
    "        print(f\"  Validating {len(self.data)} videos...\")\n",
    "        valid_data = []\n",
    "        for item in tqdm(self.data, desc=\"Validating\"):\n",
    "            if self._is_valid_video(item['video_path']):\n",
    "                valid_data.append(item)\n",
    "        \n",
    "        self.data = valid_data\n",
    "        print(f\"  [OK] {len(self.data)} valid videos\")\n",
    "    \n",
    "    def _is_valid_video(self, video_path):\n",
    "        try:\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            cap.release()\n",
    "            return count >= self.num_frames and width > 0 and height > 0\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def load_video(self, video_path):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "        \n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = cv2.resize(frame, (self.resolution, self.resolution))\n",
    "                if np.isnan(frame).any() or np.isinf(frame).any():\n",
    "                    frame = np.zeros((self.resolution, self.resolution, 3), dtype=np.uint8)\n",
    "                frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        while len(frames) < self.num_frames:\n",
    "            frames.append(frames[-1] if frames else np.zeros((self.resolution, self.resolution, 3), dtype=np.uint8))\n",
    "        \n",
    "        frames = np.stack(frames)\n",
    "        frames = frames.astype(np.float32) / 127.5 - 1.0\n",
    "        frames = np.clip(frames, -1.0, 1.0)\n",
    "        frames = torch.from_numpy(frames).permute(3, 0, 1, 2)\n",
    "        return frames\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        video = self.load_video(item['video_path'])\n",
    "        return {'video': video, 'caption': item['caption']}\n",
    "\n",
    "# Load models\n",
    "print(\"\\nLoading models...\")\n",
    "\n",
    "# CRITICAL: Load VAE in FULL FP32\n",
    "print(\"  VAE (FULL FP32)...\")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    SD_PATH, subfolder=\"vae\", \n",
    "    torch_dtype=torch.float32,  # ‚Üê CHANGED: Load directly in FP32\n",
    "    local_files_only=True\n",
    ").to(\"cuda\")\n",
    "vae.requires_grad_(False)\n",
    "vae.eval()\n",
    "\n",
    "# Triple-check that encoder is FP32\n",
    "vae.encoder = vae.encoder.float()\n",
    "for param in vae.encoder.parameters():\n",
    "    param.data = param.data.float()\n",
    "\n",
    "print(\"  [OK] VAE fully in FP32\")\n",
    "\n",
    "print(\"  Text Encoder...\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\n",
    "    SD_PATH, subfolder=\"text_encoder\", torch_dtype=torch.float16, local_files_only=True\n",
    ").to(\"cuda\")\n",
    "text_encoder.requires_grad_(False)\n",
    "text_encoder.eval()\n",
    "\n",
    "print(\"  Tokenizer...\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(SD_PATH, subfolder=\"tokenizer\", local_files_only=True)\n",
    "\n",
    "print(\"  UNet 2D...\")\n",
    "unet_2d = UNet2DConditionModel.from_pretrained(\n",
    "    SD_PATH, subfolder=\"unet\", torch_dtype=torch.float16, local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"  Motion Adapter...\")\n",
    "motion_adapter = MotionAdapter.from_pretrained(\n",
    "    MOTION_PATH, torch_dtype=torch.float16, local_files_only=True\n",
    ")\n",
    "\n",
    "print(\"  Creating Motion UNet...\")\n",
    "unet = UNetMotionModel.from_unet2d(unet_2d, motion_adapter)\n",
    "unet = unet.to(\"cuda\")\n",
    "\n",
    "print(\"\\n  Setting up LoRA...\")\n",
    "unet.freeze_unet2d_params()\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=config['lora_rank'],\n",
    "    lora_alpha=config['lora_alpha'],\n",
    "    target_modules=[\"to_q\", \"to_k\", \"to_v\", \"to_out.0\"],\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "unet = get_peft_model(unet, lora_config)\n",
    "unet.print_trainable_parameters()\n",
    "\n",
    "for name, param in unet.named_parameters():\n",
    "    if 'lora' in name and 'weight' in name:\n",
    "        torch.nn.init.normal_(param.data, mean=0.0, std=0.01)\n",
    "\n",
    "if config['gradient_checkpointing']:\n",
    "    unet.enable_gradient_checkpointing()\n",
    "\n",
    "print(\"  Noise Scheduler...\")\n",
    "noise_scheduler = DDIMScheduler.from_pretrained(\n",
    "    SD_PATH, subfolder=\"scheduler\", local_files_only=True\n",
    ")\n",
    "\n",
    "# Dataset\n",
    "print(\"\\nCreating dataset...\")\n",
    "filtered_path = Path('/workspace/anime_dataset/training_metadata_filtered.json')\n",
    "original_path = Path('/workspace/anime_dataset/training_metadata.json')\n",
    "\n",
    "if filtered_path.exists():\n",
    "    dataset_path = str(filtered_path)\n",
    "    print(f\"  ‚úì Using filtered dataset\")\n",
    "elif original_path.exists():\n",
    "    dataset_path = str(original_path)\n",
    "    print(f\"  ‚ö†Ô∏è  Using unfiltered dataset\")\n",
    "else:\n",
    "    raise FileNotFoundError(\"No dataset found!\")\n",
    "\n",
    "train_dataset = AnimeVideoDataset(\n",
    "    metadata_path=dataset_path,\n",
    "    num_frames=config['num_frames'],\n",
    "    resolution=config['resolution']\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config['batch_size'],\n",
    "    shuffle=True,\n",
    "    num_workers=config['num_workers'],\n",
    "    pin_memory=True,\n",
    "    drop_last=True,\n",
    "    persistent_workers=True if config['num_workers'] > 0 else False\n",
    ")\n",
    "\n",
    "print(f\"[OK] Dataset: {len(train_dataset)} videos\")\n",
    "print(f\"[OK] Batches/epoch: {len(train_dataloader)}\")\n",
    "\n",
    "# Optimizer\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    optimizer = bnb.optim.AdamW8bit(\n",
    "        unet.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=0.01,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    print(\"  [OK] Using 8-bit AdamW\")\n",
    "except:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        unet.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        betas=(0.9, 0.999),\n",
    "        weight_decay=0.01,\n",
    "        eps=1e-8\n",
    "    )\n",
    "    print(\"  [OK] Using standard AdamW\")\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "total_steps = len(train_dataloader) * config['num_epochs'] // config['gradient_accumulation_steps']\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=config['learning_rate'] * 0.1)\n",
    "\n",
    "print(f\"\\nGPU Memory: {torch.cuda.memory_allocated() / (1024**3):.1f}GB\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"READY TO TRAIN - VAE IN FULL FP32\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4660581-d34a-4bc3-9556-7d6a40f930f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EMERGENCY VAE FIX\n",
      "================================================================================\n",
      "\n",
      "[1] Current VAE encoder state:\n",
      "   Total parameters: 106\n",
      "   Dtype distribution: {'torch.float32': 106}\n",
      "\n",
      "[2] Converting ALL parameters to FP32...\n",
      "\n",
      "[3] Verification:\n",
      "   Dtype distribution: {'torch.float32': 106}\n",
      "\n",
      "‚úÖ SUCCESS - All encoder parameters are now FP32!\n",
      "\n",
      "[4] Testing VAE encoding...\n",
      "   ‚úÖ VAE encoding works!\n",
      "   Latent shape: torch.Size([1, 4, 32, 32]), dtype: torch.float32\n",
      "\n",
      "================================================================================\n",
      "FIX COMPLETE - Now re-run Cell 5\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# EMERGENCY FIX - Force VAE Encoder to Full FP32\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EMERGENCY VAE FIX\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n[1] Current VAE encoder state:\")\n",
    "encoder_params = list(vae.encoder.parameters())\n",
    "print(f\"   Total parameters: {len(encoder_params)}\")\n",
    "\n",
    "# Check current dtypes\n",
    "dtypes = {}\n",
    "for name, param in vae.encoder.named_parameters():\n",
    "    dtype_str = str(param.dtype)\n",
    "    dtypes[dtype_str] = dtypes.get(dtype_str, 0) + 1\n",
    "\n",
    "print(f\"   Dtype distribution: {dtypes}\")\n",
    "\n",
    "print(\"\\n[2] Converting ALL parameters to FP32...\")\n",
    "\n",
    "# Method 1: Convert the entire encoder module\n",
    "vae.encoder = vae.encoder.to(dtype=torch.float32)\n",
    "\n",
    "# Method 2: Force convert each parameter individually (nuclear option)\n",
    "for name, param in vae.encoder.named_parameters():\n",
    "    param.data = param.data.to(dtype=torch.float32)\n",
    "    if param.dtype != torch.float32:\n",
    "        print(f\"   ‚ö†Ô∏è  Failed to convert: {name} (still {param.dtype})\")\n",
    "\n",
    "# Method 3: Also convert buffers (batch norm running stats, etc.)\n",
    "for name, buffer in vae.encoder.named_buffers():\n",
    "    buffer.data = buffer.data.to(dtype=torch.float32)\n",
    "    if buffer.dtype != torch.float32:\n",
    "        print(f\"   ‚ö†Ô∏è  Failed to convert buffer: {name} (still {buffer.dtype})\")\n",
    "\n",
    "print(\"\\n[3] Verification:\")\n",
    "dtypes_after = {}\n",
    "for name, param in vae.encoder.named_parameters():\n",
    "    dtype_str = str(param.dtype)\n",
    "    dtypes_after[dtype_str] = dtypes_after.get(dtype_str, 0) + 1\n",
    "\n",
    "print(f\"   Dtype distribution: {dtypes_after}\")\n",
    "\n",
    "if dtypes_after == {'torch.float32': len(encoder_params)}:\n",
    "    print(\"\\n‚úÖ SUCCESS - All encoder parameters are now FP32!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING - Some parameters may still be mixed precision\")\n",
    "\n",
    "# Test encoding\n",
    "print(\"\\n[4] Testing VAE encoding...\")\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "video_path = '/workspace/anime_dataset/videos/video303.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "cap.release()\n",
    "\n",
    "if ret:\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_resized = cv2.resize(frame_rgb, (256, 256))\n",
    "    frame_normalized = frame_resized.astype(np.float32) / 127.5 - 1.0\n",
    "    frame_t = torch.from_numpy(frame_normalized).permute(2, 0, 1).unsqueeze(0)\n",
    "    frame_t = frame_t.to(device=\"cuda\", dtype=torch.float32)\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            latent = vae.encode(frame_t).latent_dist.sample()\n",
    "        print(\"   ‚úÖ VAE encoding works!\")\n",
    "        print(f\"   Latent shape: {latent.shape}, dtype: {latent.dtype}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå VAE encoding still fails: {e}\")\n",
    "else:\n",
    "    print(\"   ‚ö†Ô∏è  Could not load test video\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FIX COMPLETE - Now re-run Cell 5\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16e5f5b3-9da3-4792-b5e4-ba17c6ae97f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING LORA TRAINING (NaN-SAFE)\n",
      "================================================================================\n",
      "\n",
      "Training Plan:\n",
      "  Epochs: 10\n",
      "  Steps per epoch: 200\n",
      "  Gradient accumulation: 8\n",
      "  Effective batch size: 8\n",
      "  Total optimization steps: 250\n",
      "  LoRA rank: 16, alpha: 32\n",
      "  Learning rate: 5e-05\n",
      "  Precision: Mixed (FP32 VAE, FP16 UNet)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "EPOCH 1/10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:44<00:00,  4.48it/s, loss=0.1160, lr=4.89e-05, valid=200/200, skip=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 1 Summary:\n",
      "  Average Loss: 0.088103\n",
      "  Valid steps: 200/200\n",
      "  Skipped batches: 0\n",
      "  Time: 0.8 min\n",
      "  Learning rate: 4.89e-05\n",
      "  ‚≠ê NEW BEST LOSS!\n",
      "    ‚≠ê [BEST] lora_epoch_1_best\n",
      "================================================================================\n",
      "\n",
      "EPOCH 2/10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:44<00:00,  4.49it/s, loss=0.0831, lr=4.57e-05, valid=200/200, skip=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 2 Summary:\n",
      "  Average Loss: 0.086278\n",
      "  Valid steps: 200/200\n",
      "  Skipped batches: 0\n",
      "  Time: 0.7 min\n",
      "  Learning rate: 4.57e-05\n",
      "  ‚≠ê NEW BEST LOSS!\n",
      "    [SAVE] lora_epoch_2\n",
      "    ‚≠ê [BEST] lora_epoch_2_best\n",
      "================================================================================\n",
      "\n",
      "EPOCH 3/10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:44<00:00,  4.54it/s, loss=0.0684, lr=4.07e-05, valid=200/200, skip=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 3 Summary:\n",
      "  Average Loss: 0.115437\n",
      "  Valid steps: 200/200\n",
      "  Skipped batches: 0\n",
      "  Time: 0.7 min\n",
      "  Learning rate: 4.07e-05\n",
      "================================================================================\n",
      "\n",
      "EPOCH 4/10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:45<00:00,  4.41it/s, loss=0.1474, lr=3.45e-05, valid=200/200, skip=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 4 Summary:\n",
      "  Average Loss: 0.104578\n",
      "  Valid steps: 200/200\n",
      "  Skipped batches: 0\n",
      "  Time: 0.8 min\n",
      "  Learning rate: 3.45e-05\n",
      "    [SAVE] lora_epoch_4\n",
      "================================================================================\n",
      "\n",
      "EPOCH 5/10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:47<00:00,  4.20it/s, loss=0.0278, lr=2.75e-05, valid=200/200, skip=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 5 Summary:\n",
      "  Average Loss: 0.093644\n",
      "  Valid steps: 200/200\n",
      "  Skipped batches: 0\n",
      "  Time: 0.8 min\n",
      "  Learning rate: 2.75e-05\n",
      "================================================================================\n",
      "\n",
      "EPOCH 6/10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:48<00:00,  4.13it/s, loss=0.0507, lr=2.05e-05, valid=200/200, skip=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 6 Summary:\n",
      "  Average Loss: 0.108822\n",
      "  Valid steps: 200/200\n",
      "  Skipped batches: 0\n",
      "  Time: 0.8 min\n",
      "  Learning rate: 2.05e-05\n",
      "    [SAVE] lora_epoch_6\n",
      "================================================================================\n",
      "\n",
      "EPOCH 7/10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:45<00:00,  4.41it/s, loss=0.0608, lr=1.43e-05, valid=200/200, skip=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 7 Summary:\n",
      "  Average Loss: 0.099704\n",
      "  Valid steps: 200/200\n",
      "  Skipped batches: 0\n",
      "  Time: 0.8 min\n",
      "  Learning rate: 1.43e-05\n",
      "================================================================================\n",
      "\n",
      "EPOCH 8/10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:45<00:00,  4.40it/s, loss=0.0827, lr=9.30e-06, valid=200/200, skip=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 8 Summary:\n",
      "  Average Loss: 0.112794\n",
      "  Valid steps: 200/200\n",
      "  Skipped batches: 0\n",
      "  Time: 0.8 min\n",
      "  Learning rate: 9.30e-06\n",
      "    [SAVE] lora_epoch_8\n",
      "================================================================================\n",
      "\n",
      "EPOCH 9/10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:46<00:00,  4.30it/s, loss=0.0256, lr=6.10e-06, valid=200/200, skip=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 9 Summary:\n",
      "  Average Loss: 0.097821\n",
      "  Valid steps: 200/200\n",
      "  Skipped batches: 0\n",
      "  Time: 0.8 min\n",
      "  Learning rate: 6.10e-06\n",
      "================================================================================\n",
      "\n",
      "EPOCH 10/10\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 200/200 [00:48<00:00,  4.16it/s, loss=0.0130, lr=5.00e-06, valid=200/200, skip=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Epoch 10 Summary:\n",
      "  Average Loss: 0.107972\n",
      "  Valid steps: 200/200\n",
      "  Skipped batches: 0\n",
      "  Time: 0.8 min\n",
      "  Learning rate: 5.00e-06\n",
      "    [SAVE] lora_epoch_10\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "‚úÖ TRAINING COMPLETE\n",
      "================================================================================\n",
      "Total time: 0.13 hours\n",
      "Best loss: 0.086278\n",
      "Total skipped batches: 0\n",
      "\n",
      "LoRA weights saved in: /workspace/lora_outputs\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Training session ended\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CELL 5: TRAINING LOOP (LORA) - COMPLETE NaN-SAFE VERSION\n",
    "# ==============================================================================\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STARTING LORA TRAINING (NaN-SAFE)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# Safety Functions\n",
    "# ============================================================================\n",
    "\n",
    "def check_for_nan(tensor, name=\"tensor\", raise_error=True):\n",
    "    \"\"\"Check tensor for NaN/Inf and optionally raise error\"\"\"\n",
    "    has_nan = torch.isnan(tensor).any()\n",
    "    has_inf = torch.isinf(tensor).any()\n",
    "    \n",
    "    if has_nan:\n",
    "        nan_count = torch.isnan(tensor).sum().item()\n",
    "        msg = f\"NaN detected in {name}! Count: {nan_count}, Shape: {tensor.shape}\"\n",
    "        if raise_error:\n",
    "            raise ValueError(msg)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {msg}\")\n",
    "            return False\n",
    "    \n",
    "    if has_inf:\n",
    "        inf_count = torch.isinf(tensor).sum().item()\n",
    "        msg = f\"Inf detected in {name}! Count: {inf_count}, Shape: {tensor.shape}\"\n",
    "        if raise_error:\n",
    "            raise ValueError(msg)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {msg}\")\n",
    "            return False\n",
    "    \n",
    "    max_val = tensor.abs().max().item()\n",
    "    if max_val > 1e4:\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Large values in {name}: max={max_val:.2f}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def safe_vae_encode(vae, videos, chunk_size=4):\n",
    "    \"\"\"\n",
    "    Encode videos with FP32 precision and safety checks.\n",
    "    This is the CRITICAL fix for NaN issues.\n",
    "    \"\"\"\n",
    "    b, c, f, h, w = videos.shape\n",
    "    videos_2d = videos.permute(0, 2, 1, 3, 4).reshape(b * f, c, h, w)\n",
    "    \n",
    "    latents_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, b * f, chunk_size):\n",
    "            chunk = videos_2d[i:i+chunk_size]\n",
    "            \n",
    "            # CRITICAL: Keep in FP32 for VAE encoding\n",
    "            chunk_fp32 = chunk.float()\n",
    "            \n",
    "            # Validate input\n",
    "            if not check_for_nan(chunk_fp32, f\"VAE input chunk {i//chunk_size}\", raise_error=False):\n",
    "                # Use zeros if corrupted\n",
    "                chunk_fp32 = torch.zeros_like(chunk_fp32)\n",
    "            \n",
    "            # Clamp input to reasonable range\n",
    "            chunk_fp32 = torch.clamp(chunk_fp32, -2.0, 2.0)\n",
    "            \n",
    "            # Encode in FP32\n",
    "            try:\n",
    "                # CRITICAL FIX: Ensure VAE encoder is fully in FP32 before each batch\n",
    "                # This handles the \"Input type (float) and bias type (c10::Half)\" error\n",
    "                if i == 0:  # Only do this once per encode call\n",
    "                    vae.encoder.to(dtype=torch.float32)\n",
    "                \n",
    "                latent_dist = vae.encode(chunk_fp32).latent_dist\n",
    "                latent_chunk = latent_dist.sample()\n",
    "                \n",
    "                # Validate output before scaling\n",
    "                if not check_for_nan(latent_chunk, f\"VAE latent pre-scale\", raise_error=False):\n",
    "                    latent_chunk = torch.zeros_like(latent_chunk)\n",
    "                \n",
    "                # Scale with safety check\n",
    "                latent_chunk = latent_chunk * 0.18215\n",
    "                \n",
    "                # Clamp to prevent extreme values\n",
    "                latent_chunk = torch.clamp(latent_chunk, -10.0, 10.0)\n",
    "                \n",
    "                # Validate after scaling\n",
    "                if not check_for_nan(latent_chunk, f\"VAE latent post-scale\", raise_error=False):\n",
    "                    latent_chunk = torch.zeros_like(latent_chunk)\n",
    "                \n",
    "            except RuntimeError as e:\n",
    "                if \"Input type\" in str(e) and \"bias type\" in str(e):\n",
    "                    print(f\"‚ö†Ô∏è  Dtype mismatch in VAE - attempting full FP32 conversion...\")\n",
    "                    # Force all encoder parameters to FP32\n",
    "                    for param in vae.encoder.parameters():\n",
    "                        param.data = param.data.float()\n",
    "                    # Retry encoding\n",
    "                    try:\n",
    "                        latent_dist = vae.encode(chunk_fp32).latent_dist\n",
    "                        latent_chunk = latent_dist.sample() * 0.18215\n",
    "                        latent_chunk = torch.clamp(latent_chunk, -10.0, 10.0)\n",
    "                    except Exception as retry_e:\n",
    "                        print(f\"‚ö†Ô∏è  Retry failed: {retry_e}\")\n",
    "                        latent_chunk = torch.zeros((chunk.shape[0], 4, h//8, w//8), device=chunk.device, dtype=torch.float32)\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è  VAE encoding failed for chunk {i//chunk_size}: {e}\")\n",
    "                    latent_chunk = torch.zeros((chunk.shape[0], 4, h//8, w//8), device=chunk.device, dtype=torch.float32)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  VAE encoding failed for chunk {i//chunk_size}: {e}\")\n",
    "                latent_chunk = torch.zeros((chunk.shape[0], 4, h//8, w//8), device=chunk.device, dtype=torch.float32)\n",
    "            \n",
    "            # Convert to FP16 for memory efficiency\n",
    "            latents_list.append(latent_chunk.half())\n",
    "    \n",
    "    latents = torch.cat(latents_list, dim=0)\n",
    "    _, c_lat, h_lat, w_lat = latents.shape\n",
    "    latents = latents.reshape(b, f, c_lat, h_lat, w_lat).permute(0, 2, 1, 3, 4)\n",
    "    \n",
    "    return latents\n",
    "\n",
    "def save_lora_checkpoint(epoch, unet, optimizer, scheduler, loss, config, is_best=False):\n",
    "    \"\"\"Save LoRA checkpoint with training state\"\"\"\n",
    "    output_dir = Path(config['output_dir'])\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Delete old regular checkpoints (keep last 2)\n",
    "    if not is_best:\n",
    "        old_checkpoints = sorted(output_dir.glob('lora_checkpoint_epoch_*.pt'))\n",
    "        for old in old_checkpoints[:-2]:\n",
    "            try:\n",
    "                old.unlink()\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Save LoRA weights\n",
    "    checkpoint_name = f\"lora_epoch_{epoch+1}\" + (\"_best\" if is_best else \"\")\n",
    "    unet.save_pretrained(str(output_dir / checkpoint_name))\n",
    "    \n",
    "    # Save training state\n",
    "    state_path = output_dir / f\"lora_checkpoint_epoch_{epoch+1}.pt\"\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'loss': loss,\n",
    "        'config': config,\n",
    "    }, state_path)\n",
    "    \n",
    "    marker = \"‚≠ê [BEST]\" if is_best else \"[SAVE]\"\n",
    "    print(f\"    {marker} {checkpoint_name}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Training Loop\n",
    "# ============================================================================\n",
    "\n",
    "global_step = 0\n",
    "training_start_time = time.time()\n",
    "best_loss = float('inf')\n",
    "nan_encountered = False\n",
    "total_skipped_batches = 0\n",
    "\n",
    "print(f\"\\nTraining Plan:\")\n",
    "print(f\"  Epochs: {config['num_epochs']}\")\n",
    "print(f\"  Steps per epoch: {len(train_dataloader)}\")\n",
    "print(f\"  Gradient accumulation: {config['gradient_accumulation_steps']}\")\n",
    "print(f\"  Effective batch size: {config['batch_size'] * config['gradient_accumulation_steps']}\")\n",
    "print(f\"  Total optimization steps: {len(train_dataloader) * config['num_epochs'] // config['gradient_accumulation_steps']}\")\n",
    "print(f\"  LoRA rank: {config['lora_rank']}, alpha: {config['lora_alpha']}\")\n",
    "print(f\"  Learning rate: {config['learning_rate']}\")\n",
    "print(f\"  Precision: Mixed (FP32 VAE, FP16 UNet)\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "try:\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        epoch_start = time.time()\n",
    "        unet.train()\n",
    "        epoch_loss = 0.0\n",
    "        valid_steps = 0\n",
    "        skipped_batches = 0\n",
    "        \n",
    "        print(f\"\\nEPOCH {epoch + 1}/{config['num_epochs']}\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        progress_bar = tqdm(\n",
    "            enumerate(train_dataloader),\n",
    "            total=len(train_dataloader),\n",
    "            desc=f\"Epoch {epoch+1}\",\n",
    "            ncols=120\n",
    "        )\n",
    "        \n",
    "        for step, batch in progress_bar:\n",
    "            try:\n",
    "                # CRITICAL: Load videos in FP32 for VAE\n",
    "                videos = batch['video'].to(\"cuda\", dtype=torch.float32)\n",
    "                captions = batch['caption']\n",
    "                \n",
    "                # Validate input\n",
    "                if not check_for_nan(videos, \"input videos\", raise_error=False):\n",
    "                    print(f\"\\n‚ö†Ô∏è  Corrupted video at step {step}, skipping...\")\n",
    "                    optimizer.zero_grad()\n",
    "                    skipped_batches += 1\n",
    "                    continue\n",
    "                \n",
    "                # Encode videos with safety (FP32 -> FP16)\n",
    "                latents = safe_vae_encode(vae, videos, chunk_size=4)\n",
    "                \n",
    "                # Validate latents\n",
    "                if not check_for_nan(latents, \"encoded latents\", raise_error=False):\n",
    "                    print(f\"\\n‚ö†Ô∏è  Bad latents at step {step}, skipping...\")\n",
    "                    optimizer.zero_grad()\n",
    "                    skipped_batches += 1\n",
    "                    continue\n",
    "                \n",
    "                # Encode text\n",
    "                with torch.no_grad():\n",
    "                    text_inputs = tokenizer(\n",
    "                        captions, \n",
    "                        padding=\"max_length\",\n",
    "                        max_length=tokenizer.model_max_length,\n",
    "                        truncation=True, \n",
    "                        return_tensors=\"pt\"\n",
    "                    )\n",
    "                    encoder_hidden_states = text_encoder(\n",
    "                        text_inputs.input_ids.to(\"cuda\")\n",
    "                    )[0]\n",
    "                    \n",
    "                    if not check_for_nan(encoder_hidden_states, \"text embeddings\", raise_error=False):\n",
    "                        print(f\"\\n‚ö†Ô∏è  Bad text embeddings at step {step}, skipping...\")\n",
    "                        optimizer.zero_grad()\n",
    "                        skipped_batches += 1\n",
    "                        continue\n",
    "                \n",
    "                # Add noise\n",
    "                noise = torch.randn_like(latents)\n",
    "                b = latents.shape[0]\n",
    "                \n",
    "                # Sample timesteps - use more stable middle range initially\n",
    "                if epoch < 2:\n",
    "                    # First 2 epochs: use middle timesteps (200-800)\n",
    "                    timesteps = torch.randint(\n",
    "                        200, 800, (b,), device=\"cuda\", dtype=torch.long\n",
    "                    )\n",
    "                else:\n",
    "                    # Later: use full range\n",
    "                    timesteps = torch.randint(\n",
    "                        0, noise_scheduler.config.num_train_timesteps, \n",
    "                        (b,), device=\"cuda\", dtype=torch.long\n",
    "                    )\n",
    "                \n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                \n",
    "                if not check_for_nan(noisy_latents, \"noisy latents\", raise_error=False):\n",
    "                    print(f\"\\n‚ö†Ô∏è  Bad noisy latents at step {step}, skipping...\")\n",
    "                    optimizer.zero_grad()\n",
    "                    skipped_batches += 1\n",
    "                    continue\n",
    "                \n",
    "                # Forward pass\n",
    "                model_pred = unet(\n",
    "                    noisy_latents, \n",
    "                    timesteps, \n",
    "                    encoder_hidden_states\n",
    "                ).sample\n",
    "                \n",
    "                # Validate model output\n",
    "                if not check_for_nan(model_pred, \"model prediction\", raise_error=False):\n",
    "                    print(f\"\\n‚ö†Ô∏è  NaN in model output at step {step}, skipping...\")\n",
    "                    optimizer.zero_grad()\n",
    "                    skipped_batches += 1\n",
    "                    nan_encountered = True\n",
    "                    continue\n",
    "                \n",
    "                # Compute loss in FP32 for numerical stability\n",
    "                loss = F.mse_loss(\n",
    "                    model_pred.float(), \n",
    "                    noise.float(), \n",
    "                    reduction=\"mean\"\n",
    "                )\n",
    "                \n",
    "                # Validate loss\n",
    "                if torch.isnan(loss) or torch.isinf(loss) or loss.item() > 100:\n",
    "                    print(f\"\\n‚ö†Ô∏è  Invalid loss at step {step}: {loss.item()}, skipping...\")\n",
    "                    optimizer.zero_grad()\n",
    "                    skipped_batches += 1\n",
    "                    continue\n",
    "                \n",
    "                # Scale loss for gradient accumulation\n",
    "                loss = loss / config['gradient_accumulation_steps']\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                # Optimizer step with gradient accumulation\n",
    "                if (step + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "                    # Check gradients before clipping\n",
    "                    total_norm = 0.0\n",
    "                    for p in unet.parameters():\n",
    "                        if p.grad is not None:\n",
    "                            param_norm = p.grad.data.norm(2)\n",
    "                            total_norm += param_norm.item() ** 2\n",
    "                    total_norm = total_norm ** 0.5\n",
    "                    \n",
    "                    # Skip if gradients are too large\n",
    "                    if total_norm > 1000:\n",
    "                        print(f\"\\n‚ö†Ô∏è  Extreme gradient norm: {total_norm:.2f}, skipping...\")\n",
    "                        optimizer.zero_grad()\n",
    "                        skipped_batches += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Clip gradients\n",
    "                    grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                        unet.parameters(), \n",
    "                        config['max_grad_norm']\n",
    "                    )\n",
    "                    \n",
    "                    # Check for NaN in gradients\n",
    "                    has_nan_grad = False\n",
    "                    for p in unet.parameters():\n",
    "                        if p.grad is not None and (torch.isnan(p.grad).any() or torch.isinf(p.grad).any()):\n",
    "                            has_nan_grad = True\n",
    "                            break\n",
    "                    \n",
    "                    if has_nan_grad:\n",
    "                        print(f\"\\n‚ö†Ô∏è  NaN in gradients, skipping optimization step...\")\n",
    "                        optimizer.zero_grad()\n",
    "                        skipped_batches += 1\n",
    "                        nan_encountered = True\n",
    "                        continue\n",
    "                    \n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "                \n",
    "                # Accumulate epoch loss\n",
    "                epoch_loss += loss.detach().item() * config['gradient_accumulation_steps']\n",
    "                valid_steps += 1\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    'loss': f\"{loss.item() * config['gradient_accumulation_steps']:.4f}\",\n",
    "                    'lr': f\"{scheduler.get_last_lr()[0]:.2e}\",\n",
    "                    'valid': f\"{valid_steps}/{step+1}\",\n",
    "                    'skip': skipped_batches\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"\\n‚ùå Unexpected error at step {step}: {e}\")\n",
    "                print(\"   Skipping batch and continuing...\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                optimizer.zero_grad()\n",
    "                skipped_batches += 1\n",
    "                nan_encountered = True\n",
    "                continue\n",
    "        \n",
    "        # Epoch summary\n",
    "        if valid_steps == 0:\n",
    "            print(\"\\n‚ùå CRITICAL: No valid steps in this epoch!\")\n",
    "            print(\"   This likely means all videos are corrupted or incompatible.\")\n",
    "            print(\"   Please run Cell 6 (video filter) again and check your dataset.\")\n",
    "            break\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / valid_steps\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        total_skipped_batches += skipped_batches\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Epoch {epoch+1} Summary:\")\n",
    "        print(f\"  Average Loss: {avg_epoch_loss:.6f}\")\n",
    "        print(f\"  Valid steps: {valid_steps}/{len(train_dataloader)}\")\n",
    "        print(f\"  Skipped batches: {skipped_batches}\")\n",
    "        print(f\"  Time: {epoch_time/60:.1f} min\")\n",
    "        print(f\"  Learning rate: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "        \n",
    "        is_best = avg_epoch_loss < best_loss\n",
    "        if is_best:\n",
    "            best_loss = avg_epoch_loss\n",
    "            print(f\"  ‚≠ê NEW BEST LOSS!\")\n",
    "        \n",
    "        # Save checkpoints\n",
    "        if (epoch + 1) % config['save_every_n_epochs'] == 0 or (epoch + 1) == config['num_epochs']:\n",
    "            save_lora_checkpoint(epoch, unet, optimizer, scheduler, avg_epoch_loss, config, is_best=False)\n",
    "        \n",
    "        if is_best:\n",
    "            save_lora_checkpoint(epoch, unet, optimizer, scheduler, avg_epoch_loss, config, is_best=True)\n",
    "        \n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Cleanup\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Training complete\n",
    "    total_time = time.time() - training_start_time\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"‚úÖ TRAINING COMPLETE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Total time: {total_time/3600:.2f} hours\")\n",
    "    print(f\"Best loss: {best_loss:.6f}\")\n",
    "    print(f\"Total skipped batches: {total_skipped_batches}\")\n",
    "    if nan_encountered:\n",
    "        print(\"‚ö†Ô∏è  Note: Some NaN errors were encountered and handled gracefully\")\n",
    "    print(f\"\\nLoRA weights saved in: {config['output_dir']}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚è∏Ô∏è  Training interrupted by user\")\n",
    "    if 'valid_steps' in locals() and valid_steps > 0:\n",
    "        print(\"Saving checkpoint...\")\n",
    "        save_lora_checkpoint(epoch, unet, optimizer, scheduler, epoch_loss / valid_steps, config)\n",
    "        print(\"Checkpoint saved successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå FATAL ERROR: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    if 'epoch' in locals() and 'valid_steps' in locals() and valid_steps > 0:\n",
    "        print(\"\\nAttempting emergency checkpoint save...\")\n",
    "        try:\n",
    "            save_lora_checkpoint(epoch, unet, optimizer, scheduler, epoch_loss / valid_steps, config)\n",
    "            print(\"Emergency checkpoint saved\")\n",
    "        except:\n",
    "            print(\"Emergency save failed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training session ended\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8429858b-d3de-4178-b307-556b0169facf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING BASE MODEL (NO LORA)\n",
      "================================================================================\n",
      "Generating with BASE model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:01<00:00, 15.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:00<00:00, 117.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base model output saved to: /workspace/lora_outputs/base_model_test.gif\n",
      "Compare this with your LoRA output\n"
     ]
    }
   ],
   "source": [
    "# Test BASE model without LoRA\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING BASE MODEL (NO LORA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use the UNet WITHOUT LoRA\n",
    "unet_base = unet.get_base_model()  # This removes LoRA\n",
    "unet_base.eval()\n",
    "\n",
    "prompt = \"anime style character with flowing hair, vibrant colors, smooth animation\"\n",
    "num_frames = 16\n",
    "num_inference_steps = 25\n",
    "guidance_scale = 7.5\n",
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(\"cuda\"))[0]\n",
    "    uncond_input = tokenizer(\"\", padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\")\n",
    "    uncond_embeddings = text_encoder(uncond_input.input_ids.to(\"cuda\"))[0]\n",
    "    text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "latents = torch.randn((1, 4, num_frames, 32, 32), generator=torch.Generator(device=\"cuda\").manual_seed(seed), device=\"cuda\", dtype=torch.float16)\n",
    "noise_scheduler.set_timesteps(num_inference_steps)\n",
    "latents = latents * noise_scheduler.init_noise_sigma\n",
    "\n",
    "print(\"Generating with BASE model...\")\n",
    "for t in tqdm(noise_scheduler.timesteps):\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "    latent_model_input = noise_scheduler.scale_model_input(latent_model_input, t)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet_base(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "    \n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "    latents = noise_scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "print(\"Decoding...\")\n",
    "frames = []\n",
    "latents = latents.squeeze(0).permute(1, 0, 2, 3) / 0.18215\n",
    "\n",
    "for i in tqdm(range(num_frames)):\n",
    "    with torch.no_grad():\n",
    "        image = vae.decode(latents[i:i+1].float()).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1)\n",
    "    image = image.cpu().permute(0, 2, 3, 1).numpy()[0]\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    frames.append(image)\n",
    "\n",
    "import imageio\n",
    "imageio.mimsave(\"/workspace/lora_outputs/base_model_test.gif\", frames, duration=100, loop=0)\n",
    "print(\"‚úÖ Base model output saved to: /workspace/lora_outputs/base_model_test.gif\")\n",
    "print(\"Compare this with your LoRA output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f55028b4-b612-4942-b2c0-f386c98a8e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOWNLOADING VERIFIED ANIMATEDIFF MODELS\n",
      "================================================================================\n",
      "\n",
      "[1/3] Downloading Stable Diffusion 1.5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "924e6d5f88b342c083dc512e1f8daa73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2700e346af246eda4f98d06f2e16927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3b8d347cb14286a6de85fb25cb11fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37086289634a4b5b8c6c5ae49ad99d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260def20932c49d7a9c07cb2b88131ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed53d0fc3544539b404bce59686743a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "safety_checker/pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25a228b7fd64e4b9745c9381a648680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "safety_checker/pytorch_model.fp16.bin:   0%|          | 0.00/608M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84d88184bd6545ca979916291a8acdac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1da4f48d3254bd5b31ebb9c80e6a040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3523b55d26494219b3af07eb483300c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/pytorch_model.bin:   0%|          | 0.00/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e3c243edcb469ca35f936bdd60adbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/pytorch_model.fp16.bin:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1a59f6f9ec48c4813d2c489352d8ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "536ca2ba7cc748f1823a4efc7025cfb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5d3abcb5854c6788e5241ddf448cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01945fcdd494bfa9dcd377dfc83e3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beff8ca389ed47b7b15b09ba2ee060ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f040a94b48e483e81febcde42406954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/diffusion_pytorch_model.bin:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d07c1fff7a41489946cf77e3b88620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/diffusion_pytorch_model.fp16.bin:   0%|          | 0.00/1.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9452d4f479484d9fb011c068ce7cacca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/diffusion_pytorch_model.non_ema.bin:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48891a1bc0dd44919fdcb9e46000b80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "v1-inference.yaml: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77085eb0ebd4338a886557ac1c8f957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade2908b3f974d0793c251648de89f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/diffusion_pytorch_model.bin:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75aa37a051d4959b1c0d8dee9b5f08d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/diffusion_pytorch_model.fp16.bin:   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SD 1.5 downloaded to: /workspace/models/sd-v1-5-clean\n",
      "\n",
      "[2/3] Downloading verified AnimateDiff motion adapter...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b52d859e54ef40919bc6194e06ccb545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5971bce61aec4a92a08e389664e96c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76e1d851b9ba4e73ac915b46f3c41db4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/455 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1525b48b9b45f58a51a35748d6badc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.fp16.safetensors:   0%|          | 0.00/1.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe72eddea5e403d893c1b69904470e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/1.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'motion_activation_fn': 'geglu', 'motion_attention_bias': False, 'motion_cross_attention_dim': None} were passed to MotionAdapter, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Motion adapter downloaded to: /workspace/models/motion-adapter-v1-5-2\n",
      "\n",
      "[3/3] Loading models with proper pipeline...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a38017ef4505488d8780f377ac306655",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1030cfffcb148fdbde143b71ef90897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6f5e7212f946c6ae19e6629ba6e948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b25a87293c4280a5dcd4b7fc1cd26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "safety_checker/model.fp16.safetensors:   0%|          | 0.00/608M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0109dc08c9144de8d38a74af400bdfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589fe72ce5c34d6094863d52065b07be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafd547d811042cab59643bc470be1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "text_encoder/model.fp16.safetensors:   0%|          | 0.00/246M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b219da7a3642427482a572bcfb3c175d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "850a3971cbf749b4910ac9d828928239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fce5af6ee62f4ceaaabeae1772e1470b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aeb71b90b294321bb1a27290d62b797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdd06b641db74f5a845839d12a8a7f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d24a44640c5455a8c3014e26136f7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unet/diffusion_pytorch_model.fp16.safete(‚Ä¶):   0%|          | 0.00/1.72G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0e2421cb5b4a80a28efceea548a5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23672bde20dc444e8a3f1b281507478f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vae/diffusion_pytorch_model.fp16.safeten(‚Ä¶):   0%|          | 0.00/167M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd49c8e66f44d248b98a414ad0733ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline loaded correctly\n",
      "\n",
      "================================================================================\n",
      "TESTING BASE ANIMATEDIFF (NO LORA)\n",
      "================================================================================\n",
      "Prompt: anime girl with long flowing hair, smooth animation\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be52f108e41e4b248bbdb90577ef5453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved to: /workspace/lora_outputs/base_animatediff_clean.gif\n",
      "Check this output - it should be CLEAN, not corrupted\n",
      "\n",
      "If this works, we'll load your LoRA next\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# COMPLETE FRESH SETUP - DOWNLOAD VERIFIED MODELS\n",
    "# ==============================================================================\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DOWNLOADING VERIFIED ANIMATEDIFF MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Install required package\n",
    "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"huggingface_hub\"], check=True)\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "import torch\n",
    "\n",
    "# 1. Download CORRECT Stable Diffusion base model\n",
    "print(\"\\n[1/3] Downloading Stable Diffusion 1.5...\")\n",
    "sd_path = snapshot_download(\n",
    "    repo_id=\"runwayml/stable-diffusion-v1-5\",\n",
    "    cache_dir=\"/workspace/models\",\n",
    "    ignore_patterns=[\"*.ckpt\", \"*.safetensors\", \"!unet/*\", \"!vae/*\", \"!text_encoder/*\"],\n",
    "    local_dir=\"/workspace/models/sd-v1-5-clean\"\n",
    ")\n",
    "print(f\"‚úÖ SD 1.5 downloaded to: {sd_path}\")\n",
    "\n",
    "# 2. Download VERIFIED Motion Adapter\n",
    "print(\"\\n[2/3] Downloading verified AnimateDiff motion adapter...\")\n",
    "motion_path = snapshot_download(\n",
    "    repo_id=\"guoyww/animatediff-motion-adapter-v1-5-2\",\n",
    "    cache_dir=\"/workspace/models\",\n",
    "    local_dir=\"/workspace/models/motion-adapter-v1-5-2\"\n",
    ")\n",
    "print(f\"‚úÖ Motion adapter downloaded to: {motion_path}\")\n",
    "\n",
    "# 3. Test with base AnimateDiff Pipeline (PROPER WAY)\n",
    "print(\"\\n[3/3] Loading models with proper pipeline...\")\n",
    "\n",
    "from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler\n",
    "from diffusers.utils import export_to_gif\n",
    "\n",
    "# Load motion adapter\n",
    "motion_adapter = MotionAdapter.from_pretrained(\n",
    "    \"/workspace/models/motion-adapter-v1-5-2\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load pipeline (THIS IS THE CORRECT WAY)\n",
    "pipe = AnimateDiffPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    motion_adapter=motion_adapter,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"  # Important!\n",
    ")\n",
    "\n",
    "pipe.scheduler = DDIMScheduler.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"scheduler\",\n",
    "    clip_sample=False,\n",
    "    timestep_spacing=\"linspace\",\n",
    "    beta_schedule=\"linear\",\n",
    "    steps_offset=1\n",
    ")\n",
    "\n",
    "pipe.enable_vae_slicing()\n",
    "pipe.enable_model_cpu_offload()  # Saves memory\n",
    "\n",
    "print(\"‚úÖ Pipeline loaded correctly\\n\")\n",
    "\n",
    "# Test generation with BASE model first\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING BASE ANIMATEDIFF (NO LORA)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "prompt = \"anime girl with long flowing hair, smooth animation\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "output = pipe(\n",
    "    prompt=prompt,\n",
    "    num_frames=16,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=25,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    ")\n",
    "\n",
    "# Save\n",
    "output_path = \"/workspace/lora_outputs/base_animatediff_clean.gif\"\n",
    "export_to_gif(output.frames[0], output_path)\n",
    "\n",
    "print(f\"‚úÖ Saved to: {output_path}\")\n",
    "print(\"Check this output - it should be CLEAN, not corrupted\")\n",
    "print(\"\\nIf this works, we'll load your LoRA next\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef41c88a-0879-48f0-8eab-e093ebc18e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf_transfer\n",
      "Successfully installed hf_transfer-0.1.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hf_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c1634bcf-f24d-4a05-a8f0-31a240e0a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'motion_activation_fn': 'geglu', 'motion_attention_bias': False, 'motion_cross_attention_dim': None} were passed to MotionAdapter, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "RELOADING PIPELINE WITHOUT CPU OFFLOAD\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cffc18cdba54f53ba5306107ab39430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pipeline reloaded on CUDA\n",
      "\n",
      "Loading your trained LoRA...\n",
      "‚úÖ LoRA loaded!\n",
      "\n",
      "================================================================================\n",
      "GENERATING WITH YOUR TRAINED LORA\n",
      "================================================================================\n",
      "Prompt: anime girl with long flowing hair, smooth animation\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c9b5472bfa417794f3557ef1bb4ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SUCCESS! Saved to: /workspace/lora_outputs/with_trained_lora.gif\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# COMPLETE RELOAD - NO CPU OFFLOAD\n",
    "# ==============================================================================\n",
    "\n",
    "from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler\n",
    "from diffusers.utils import export_to_gif\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RELOADING PIPELINE WITHOUT CPU OFFLOAD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clear memory\n",
    "import gc\n",
    "del pipe\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload motion adapter\n",
    "motion_adapter = MotionAdapter.from_pretrained(\n",
    "    \"/workspace/models/motion-adapter-v1-5-2\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Reload pipeline WITHOUT CPU offload\n",
    "pipe = AnimateDiffPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    motion_adapter=motion_adapter,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")  # Direct to CUDA\n",
    "\n",
    "pipe.scheduler = DDIMScheduler.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"scheduler\",\n",
    "    clip_sample=False,\n",
    "    timestep_spacing=\"linspace\",\n",
    "    beta_schedule=\"linear\",\n",
    "    steps_offset=1\n",
    ")\n",
    "\n",
    "pipe.enable_vae_slicing()\n",
    "# DON'T enable CPU offload!\n",
    "\n",
    "print(\"‚úÖ Pipeline reloaded on CUDA\\n\")\n",
    "\n",
    "# Load LoRA\n",
    "print(\"Loading your trained LoRA...\")\n",
    "pipe.unet = PeftModel.from_pretrained(\n",
    "    pipe.unet,\n",
    "    \"/workspace/lora_outputs/lora_epoch_10\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA loaded!\\n\")\n",
    "\n",
    "# Generate\n",
    "print(\"=\" * 80)\n",
    "print(\"GENERATING WITH YOUR TRAINED LORA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "prompt = \"anime girl with long flowing hair, smooth animation\"\n",
    "print(f\"Prompt: {prompt}\\n\")\n",
    "\n",
    "output = pipe(\n",
    "    prompt=prompt,\n",
    "    num_frames=16,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=25,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    ")\n",
    "\n",
    "output_path = \"/workspace/lora_outputs/with_trained_lora.gif\"\n",
    "export_to_gif(output.frames[0], output_path)\n",
    "\n",
    "print(f\"‚úÖ SUCCESS! Saved to: {output_path}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60156d9e-0ed9-42d7-a988-17762ab0e859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'motion_activation_fn': 'geglu', 'motion_attention_bias': False, 'motion_cross_attention_dim': None} were passed to MotionAdapter, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE METRICS COMPARISON\n",
      "Base Model vs Your Trained LoRA\n",
      "================================================================================\n",
      "\n",
      "[1/4] Loading Base Model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37d0f1d3fdc4a009708c57b1e3fe1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'motion_activation_fn': 'geglu', 'motion_attention_bias': False, 'motion_cross_attention_dim': None} were passed to MotionAdapter, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Base model loaded\n",
      "\n",
      "[2/4] Loading Model with Your LoRA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a66745da2354d6ebc6eb14ff5a4e076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] LoRA model loaded\n",
      "\n",
      "[3/4] Generating videos and calculating metrics...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Test 1/5\n",
      "Prompt: 'anime girl with long flowing hair, smooth animation'\n",
      "================================================================================\n",
      "\n",
      "  [BASE MODEL] Generating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aa88a91359443f08aadb208cf4d0960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Generated in 4.75s\n",
      "    Motion: 30.0657\n",
      "    Temporal Consistency: 1572.1263\n",
      "\n",
      "  [YOUR LORA] Generating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8cd93f007e1433e9f9eba2a5a60fee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Generated in 5.41s\n",
      "    Motion: 26.2254\n",
      "    Temporal Consistency: 1219.1396\n",
      "\n",
      "  [COMPARISON]\n",
      "    Time difference: +13.9%\n",
      "    Motion difference: -12.8%\n",
      "    Consistency difference: -22.5%\n",
      "\n",
      "================================================================================\n",
      "Test 2/5\n",
      "Prompt: 'anime character running through magical forest'\n",
      "================================================================================\n",
      "\n",
      "  [BASE MODEL] Generating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a427bf62618b431cba907fa83ea90578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Generated in 4.75s\n",
      "    Motion: 33.9194\n",
      "    Temporal Consistency: 2075.0482\n",
      "\n",
      "  [YOUR LORA] Generating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9ab131588884a63b2a2663a87d84ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Generated in 5.37s\n",
      "    Motion: 32.6920\n",
      "    Temporal Consistency: 1909.3412\n",
      "\n",
      "  [COMPARISON]\n",
      "    Time difference: +13.1%\n",
      "    Motion difference: -3.6%\n",
      "    Consistency difference: -8.0%\n",
      "\n",
      "================================================================================\n",
      "Test 3/5\n",
      "Prompt: 'anime boy with spiky hair, action pose'\n",
      "================================================================================\n",
      "\n",
      "  [BASE MODEL] Generating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06bad7c24f56431bab942cc379b54bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Generated in 4.75s\n",
      "    Motion: 70.0158\n",
      "    Temporal Consistency: 8269.6363\n",
      "\n",
      "  [YOUR LORA] Generating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b0b914068448938cf9d41d9631e3b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Generated in 5.38s\n",
      "    Motion: 53.4969\n",
      "    Temporal Consistency: 5071.5589\n",
      "\n",
      "  [COMPARISON]\n",
      "    Time difference: +13.3%\n",
      "    Motion difference: -23.6%\n",
      "    Consistency difference: -38.7%\n",
      "\n",
      "================================================================================\n",
      "Test 4/5\n",
      "Prompt: 'cute anime mascot character waving'\n",
      "================================================================================\n",
      "\n",
      "  [BASE MODEL] Generating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a746fb87f2514509810b0250a178d818",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Generated in 4.75s\n",
      "    Motion: 50.5433\n",
      "    Temporal Consistency: 4352.1572\n",
      "\n",
      "  [YOUR LORA] Generating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf82ce3a61a452590c95c917e66919d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Generated in 5.38s\n",
      "    Motion: 27.2754\n",
      "    Temporal Consistency: 1148.3126\n",
      "\n",
      "  [COMPARISON]\n",
      "    Time difference: +13.2%\n",
      "    Motion difference: -46.0%\n",
      "    Consistency difference: -73.6%\n",
      "\n",
      "================================================================================\n",
      "Test 5/5\n",
      "Prompt: 'anime portrait with wind blowing hair'\n",
      "================================================================================\n",
      "\n",
      "  [BASE MODEL] Generating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d70c6b8336448fadee43cb02b36a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Generated in 4.75s\n",
      "    Motion: 68.2752\n",
      "    Temporal Consistency: 8356.1100\n",
      "\n",
      "  [YOUR LORA] Generating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89cce07b1b440819a9680f8014eb68b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    [OK] Generated in 5.38s\n",
      "    Motion: 64.4657\n",
      "    Temporal Consistency: 7852.2982\n",
      "\n",
      "  [COMPARISON]\n",
      "    Time difference: +13.1%\n",
      "    Motion difference: -5.6%\n",
      "    Consistency difference: -6.0%\n",
      "\n",
      "[4/4] Calculating aggregate statistics...\n",
      "\n",
      "[OK] Results saved to: /workspace/lora_outputs/metrics_comparison/metrics_results.json\n",
      "\n",
      "================================================================================\n",
      "SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "AGGREGATE METRICS (Average across all tests)\n",
      "--------------------------------------------------------------------------------\n",
      "Metric                         Base Model      Your LoRA       Difference\n",
      "--------------------------------------------------------------------------------\n",
      "Generation Time (s)            4.7500          5.3840          +13.35% [-]\n",
      "FPS                            3.3700          2.9760          -11.69% [-]\n",
      "Motion Amount                  50.5639         40.8311         -19.25% [-]\n",
      "Temporal Consistency           4925.0156       3440.1301       -30.15% [+]\n",
      "Brightness                     122.5728        124.9699        +1.96% [=]\n",
      "Contrast                       56.6909         51.2412         -9.61% [-]\n",
      "Sharpness                      868.3563        834.3346        -3.92% [-]\n",
      "\n",
      "================================================================================\n",
      "KEY FINDINGS:\n",
      "================================================================================\n",
      "\n",
      "[WARN] LoRA is 13.3% slower than base model\n",
      "\n",
      "[WARN] LoRA produces 19.2% less motion\n",
      "\n",
      "[OK] LoRA is 30.1% more temporally consistent\n",
      "\n",
      "================================================================================\n",
      "All outputs saved to: /workspace/lora_outputs/metrics_comparison\n",
      "Detailed metrics: /workspace/lora_outputs/metrics_comparison/metrics_results.json\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# COMPREHENSIVE METRICS: BASE MODEL vs YOUR TRAINED LORA\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler\n",
    "from diffusers.utils import export_to_gif\n",
    "from peft import PeftModel\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import Image\n",
    "import imageio\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE METRICS COMPARISON\")\n",
    "print(\"Base Model vs Your Trained LoRA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"anime girl with long flowing hair, smooth animation\",\n",
    "    \"anime character running through magical forest\",\n",
    "    \"anime boy with spiky hair, action pose\",\n",
    "    \"cute anime mascot character waving\",\n",
    "    \"anime portrait with wind blowing hair\"\n",
    "]\n",
    "\n",
    "# Generation settings\n",
    "config = {\n",
    "    'num_frames': 16,\n",
    "    'guidance_scale': 7.5,\n",
    "    'num_inference_steps': 25,\n",
    "    'height': 256,\n",
    "    'width': 256,\n",
    "}\n",
    "\n",
    "output_dir = Path('/workspace/lora_outputs/metrics_comparison')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# Helper Functions for Metrics\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_frame_difference(frames):\n",
    "    \"\"\"Calculate average difference between consecutive frames (motion metric)\"\"\"\n",
    "    differences = []\n",
    "    for i in range(len(frames) - 1):\n",
    "        diff = np.abs(frames[i].astype(float) - frames[i+1].astype(float)).mean()\n",
    "        differences.append(diff)\n",
    "    return np.mean(differences), np.std(differences)\n",
    "\n",
    "def calculate_temporal_consistency(frames):\n",
    "    \"\"\"Calculate how consistent frames are (lower = more consistent)\"\"\"\n",
    "    consistency_scores = []\n",
    "    for i in range(len(frames) - 1):\n",
    "        # Compare each frame with next frame\n",
    "        mse = np.mean((frames[i].astype(float) - frames[i+1].astype(float)) ** 2)\n",
    "        consistency_scores.append(mse)\n",
    "    return np.mean(consistency_scores)\n",
    "\n",
    "def calculate_frame_quality(frames):\n",
    "    \"\"\"Calculate average brightness, contrast, and sharpness\"\"\"\n",
    "    metrics = {\n",
    "        'brightness': [],\n",
    "        'contrast': [],\n",
    "        'sharpness': []\n",
    "    }\n",
    "    \n",
    "    for frame in frames:\n",
    "        # Brightness (mean pixel value)\n",
    "        metrics['brightness'].append(frame.mean())\n",
    "        \n",
    "        # Contrast (std of pixel values)\n",
    "        metrics['contrast'].append(frame.std())\n",
    "        \n",
    "        # Sharpness (using Laplacian variance)\n",
    "        gray = frame.mean(axis=2) if len(frame.shape) == 3 else frame\n",
    "        laplacian = np.abs(\n",
    "            np.roll(gray, 1, axis=0) + np.roll(gray, -1, axis=0) +\n",
    "            np.roll(gray, 1, axis=1) + np.roll(gray, -1, axis=1) - 4 * gray\n",
    "        )\n",
    "        metrics['sharpness'].append(laplacian.var())\n",
    "    \n",
    "    return {\n",
    "        'brightness_mean': np.mean(metrics['brightness']),\n",
    "        'brightness_std': np.std(metrics['brightness']),\n",
    "        'contrast_mean': np.mean(metrics['contrast']),\n",
    "        'contrast_std': np.std(metrics['contrast']),\n",
    "        'sharpness_mean': np.mean(metrics['sharpness']),\n",
    "        'sharpness_std': np.std(metrics['sharpness']),\n",
    "    }\n",
    "\n",
    "def measure_generation_time(pipe, prompt, seed):\n",
    "    \"\"\"Measure generation time\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        num_frames=config['num_frames'],\n",
    "        guidance_scale=config['guidance_scale'],\n",
    "        num_inference_steps=config['num_inference_steps'],\n",
    "        generator=torch.Generator(\"cuda\").manual_seed(seed)\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    generation_time = end_time - start_time\n",
    "    \n",
    "    return output.frames[0], generation_time\n",
    "\n",
    "def calculate_all_metrics(frames, generation_time):\n",
    "    \"\"\"Calculate all metrics for a set of frames\"\"\"\n",
    "    frames_array = np.array(frames)\n",
    "    \n",
    "    motion_mean, motion_std = calculate_frame_difference(frames_array)\n",
    "    temporal_consistency = calculate_temporal_consistency(frames_array)\n",
    "    quality_metrics = calculate_frame_quality(frames_array)\n",
    "    \n",
    "    # FIXED: Handle both PIL Images and numpy arrays\n",
    "    if isinstance(frames[0], np.ndarray):\n",
    "        resolution_str = f\"{frames[0].shape[1]}x{frames[0].shape[0]}\"\n",
    "    else:\n",
    "        # PIL Image uses .size which returns (width, height)\n",
    "        resolution_str = f\"{frames[0].size[0]}x{frames[0].size[1]}\"\n",
    "    \n",
    "    return {\n",
    "        'generation_time_seconds': round(generation_time, 2),\n",
    "        'fps': round(len(frames) / generation_time, 2),\n",
    "        'num_frames': len(frames),\n",
    "        'resolution': resolution_str,\n",
    "        'motion_mean': round(float(motion_mean), 4),\n",
    "        'motion_std': round(float(motion_std), 4),\n",
    "        'temporal_consistency': round(float(temporal_consistency), 4),\n",
    "        **{k: round(float(v), 4) for k, v in quality_metrics.items()}\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# Load Models\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1/4] Loading Base Model...\")\n",
    "\n",
    "# Clear any existing pipeline\n",
    "if 'pipe' in globals():\n",
    "    del pipe\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Load motion adapter\n",
    "motion_adapter = MotionAdapter.from_pretrained(\n",
    "    \"/workspace/models/motion-adapter-v1-5-2\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load base pipeline\n",
    "pipe_base = AnimateDiffPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    motion_adapter=motion_adapter,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe_base.scheduler = DDIMScheduler.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"scheduler\",\n",
    "    clip_sample=False,\n",
    "    timestep_spacing=\"linspace\",\n",
    "    beta_schedule=\"linear\",\n",
    "    steps_offset=1\n",
    ")\n",
    "\n",
    "pipe_base.enable_vae_slicing()\n",
    "\n",
    "print(\"[OK] Base model loaded\")\n",
    "\n",
    "print(\"\\n[2/4] Loading Model with Your LoRA...\")\n",
    "\n",
    "# Clone pipeline for LoRA version\n",
    "motion_adapter_lora = MotionAdapter.from_pretrained(\n",
    "    \"/workspace/models/motion-adapter-v1-5-2\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe_lora = AnimateDiffPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    motion_adapter=motion_adapter_lora,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe_lora.scheduler = DDIMScheduler.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"scheduler\",\n",
    "    clip_sample=False,\n",
    "    timestep_spacing=\"linspace\",\n",
    "    beta_schedule=\"linear\",\n",
    "    steps_offset=1\n",
    ")\n",
    "\n",
    "pipe_lora.enable_vae_slicing()\n",
    "\n",
    "# Load LoRA weights\n",
    "pipe_lora.unet = PeftModel.from_pretrained(\n",
    "    pipe_lora.unet,\n",
    "    \"/workspace/lora_outputs/lora_epoch_10\"\n",
    ")\n",
    "\n",
    "print(\"[OK] LoRA model loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# Generate and Compare\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[3/4] Generating videos and calculating metrics...\")\n",
    "print()\n",
    "\n",
    "all_results = {\n",
    "    'base_model': {},\n",
    "    'trained_lora': {},\n",
    "    'comparison': {}\n",
    "}\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Test {i}/{len(test_prompts)}\")\n",
    "    print(f\"Prompt: '{prompt}'\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    seed = 42 + i\n",
    "    \n",
    "    # Generate with BASE model\n",
    "    print(\"\\n  [BASE MODEL] Generating...\")\n",
    "    frames_base, time_base = measure_generation_time(pipe_base, prompt, seed)\n",
    "    metrics_base = calculate_all_metrics(frames_base, time_base)\n",
    "    \n",
    "    # Save base model output\n",
    "    base_path = output_dir / f\"test_{i}_base.gif\"\n",
    "    export_to_gif(frames_base, str(base_path))\n",
    "    \n",
    "    print(f\"    [OK] Generated in {time_base:.2f}s\")\n",
    "    print(f\"    Motion: {metrics_base['motion_mean']:.4f}\")\n",
    "    print(f\"    Temporal Consistency: {metrics_base['temporal_consistency']:.4f}\")\n",
    "    \n",
    "    # Generate with YOUR LORA\n",
    "    print(\"\\n  [YOUR LORA] Generating...\")\n",
    "    frames_lora, time_lora = measure_generation_time(pipe_lora, prompt, seed)\n",
    "    metrics_lora = calculate_all_metrics(frames_lora, time_lora)\n",
    "    \n",
    "    # Save LoRA output\n",
    "    lora_path = output_dir / f\"test_{i}_lora.gif\"\n",
    "    export_to_gif(frames_lora, str(lora_path))\n",
    "    \n",
    "    print(f\"    [OK] Generated in {time_lora:.2f}s\")\n",
    "    print(f\"    Motion: {metrics_lora['motion_mean']:.4f}\")\n",
    "    print(f\"    Temporal Consistency: {metrics_lora['temporal_consistency']:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    all_results['base_model'][f'test_{i}'] = {\n",
    "        'prompt': prompt,\n",
    "        'output_path': str(base_path),\n",
    "        'metrics': metrics_base\n",
    "    }\n",
    "    \n",
    "    all_results['trained_lora'][f'test_{i}'] = {\n",
    "        'prompt': prompt,\n",
    "        'output_path': str(lora_path),\n",
    "        'metrics': metrics_lora\n",
    "    }\n",
    "    \n",
    "    # Calculate differences\n",
    "    print(\"\\n  [COMPARISON]\")\n",
    "    time_diff = ((time_lora - time_base) / time_base) * 100\n",
    "    motion_diff = ((metrics_lora['motion_mean'] - metrics_base['motion_mean']) / metrics_base['motion_mean']) * 100\n",
    "    consistency_diff = ((metrics_lora['temporal_consistency'] - metrics_base['temporal_consistency']) / metrics_base['temporal_consistency']) * 100\n",
    "    \n",
    "    print(f\"    Time difference: {time_diff:+.1f}%\")\n",
    "    print(f\"    Motion difference: {motion_diff:+.1f}%\")\n",
    "    print(f\"    Consistency difference: {consistency_diff:+.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# Calculate Aggregate Statistics\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[4/4] Calculating aggregate statistics...\")\n",
    "\n",
    "def aggregate_metrics(results):\n",
    "    \"\"\"Calculate average metrics across all tests\"\"\"\n",
    "    metrics_list = [r['metrics'] for r in results.values()]\n",
    "    \n",
    "    aggregated = {}\n",
    "    for key in metrics_list[0].keys():\n",
    "        if key not in ['num_frames', 'resolution']:\n",
    "            values = [m[key] for m in metrics_list]\n",
    "            aggregated[f'{key}_mean'] = round(np.mean(values), 4)\n",
    "            aggregated[f'{key}_std'] = round(np.std(values), 4)\n",
    "    \n",
    "    return aggregated\n",
    "\n",
    "base_aggregate = aggregate_metrics(all_results['base_model'])\n",
    "lora_aggregate = aggregate_metrics(all_results['trained_lora'])\n",
    "\n",
    "all_results['aggregate_statistics'] = {\n",
    "    'base_model': base_aggregate,\n",
    "    'trained_lora': lora_aggregate\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# Save Results\n",
    "# ============================================================================\n",
    "\n",
    "results_path = output_dir / 'metrics_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n[OK] Results saved to: {results_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Print Summary Report\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nAGGREGATE METRICS (Average across all tests)\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Metric':<30} {'Base Model':<15} {'Your LoRA':<15} {'Difference'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "metrics_to_compare = [\n",
    "    ('generation_time_seconds_mean', 'Generation Time (s)', False),\n",
    "    ('fps_mean', 'FPS', True),\n",
    "    ('motion_mean_mean', 'Motion Amount', True),\n",
    "    ('temporal_consistency_mean', 'Temporal Consistency', False),\n",
    "    ('brightness_mean_mean', 'Brightness', None),\n",
    "    ('contrast_mean_mean', 'Contrast', True),\n",
    "    ('sharpness_mean_mean', 'Sharpness', True),\n",
    "]\n",
    "\n",
    "for metric_key, metric_name, higher_better in metrics_to_compare:\n",
    "    base_val = base_aggregate[metric_key]\n",
    "    lora_val = lora_aggregate[metric_key]\n",
    "    diff_pct = ((lora_val - base_val) / base_val) * 100\n",
    "    \n",
    "    if higher_better is True:\n",
    "        indicator = \"[+]\" if diff_pct > 0 else \"[-]\"\n",
    "    elif higher_better is False:\n",
    "        indicator = \"[+]\" if diff_pct < 0 else \"[-]\"\n",
    "    else:\n",
    "        indicator = \"[=]\"\n",
    "    \n",
    "    print(f\"{metric_name:<30} {base_val:<15.4f} {lora_val:<15.4f} {diff_pct:+.2f}% {indicator}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Interpret results\n",
    "findings = []\n",
    "\n",
    "time_diff = ((lora_aggregate['generation_time_seconds_mean'] - base_aggregate['generation_time_seconds_mean']) / base_aggregate['generation_time_seconds_mean']) * 100\n",
    "if abs(time_diff) < 5:\n",
    "    findings.append(f\"[OK] Generation speed is similar ({time_diff:+.1f}% difference)\")\n",
    "elif time_diff > 0:\n",
    "    findings.append(f\"[WARN] LoRA is {time_diff:.1f}% slower than base model\")\n",
    "else:\n",
    "    findings.append(f\"[OK] LoRA is {abs(time_diff):.1f}% faster than base model\")\n",
    "\n",
    "motion_diff = ((lora_aggregate['motion_mean_mean'] - base_aggregate['motion_mean_mean']) / base_aggregate['motion_mean_mean']) * 100\n",
    "if abs(motion_diff) < 10:\n",
    "    findings.append(f\"[OK] Motion amount is similar ({motion_diff:+.1f}% difference)\")\n",
    "elif motion_diff > 0:\n",
    "    findings.append(f\"[OK] LoRA produces {motion_diff:.1f}% more motion\")\n",
    "else:\n",
    "    findings.append(f\"[WARN] LoRA produces {abs(motion_diff):.1f}% less motion\")\n",
    "\n",
    "consistency_diff = ((lora_aggregate['temporal_consistency_mean'] - base_aggregate['temporal_consistency_mean']) / base_aggregate['temporal_consistency_mean']) * 100\n",
    "if consistency_diff < 0:\n",
    "    findings.append(f\"[OK] LoRA is {abs(consistency_diff):.1f}% more temporally consistent\")\n",
    "else:\n",
    "    findings.append(f\"[WARN] LoRA is {consistency_diff:.1f}% less temporally consistent\")\n",
    "\n",
    "for finding in findings:\n",
    "    print(f\"\\n{finding}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"All outputs saved to: {output_dir}\")\n",
    "print(f\"Detailed metrics: {results_path}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "782eab45-e13b-42d0-8f2d-eba5d06b2ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[BONUS] Creating visual comparison report...\n",
      "[OK] Comparison chart saved to: /workspace/lora_outputs/metrics_comparison/metrics_comparison_chart.png\n",
      "[OK] Detailed report saved to: /workspace/lora_outputs/metrics_comparison/detailed_report.txt\n",
      "[OK] CSV data saved to: /workspace/lora_outputs/metrics_comparison/metrics_data.csv\n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Generated Files:\n",
      "  1. JSON metrics:        /workspace/lora_outputs/metrics_comparison/metrics_results.json\n",
      "  2. Comparison chart:    /workspace/lora_outputs/metrics_comparison/metrics_comparison_chart.png\n",
      "  3. Detailed report:     /workspace/lora_outputs/metrics_comparison/detailed_report.txt\n",
      "  4. CSV data:            /workspace/lora_outputs/metrics_comparison/metrics_data.csv\n",
      "  5. Video outputs:       /workspace/lora_outputs/metrics_comparison\n",
      "\n",
      "Total test videos:       10 (5 base + 5 LoRA)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# Create Visual Comparison Report\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[BONUS] Creating visual comparison report...\")\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create comparison charts\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Base Model vs Trained LoRA - Metrics Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Extract data for plotting\n",
    "base_metrics = all_results['aggregate_statistics']['base_model']\n",
    "lora_metrics = all_results['aggregate_statistics']['trained_lora']\n",
    "\n",
    "# 1. Generation Time Comparison\n",
    "ax = axes[0, 0]\n",
    "models = ['Base Model', 'Trained LoRA']\n",
    "times = [base_metrics['generation_time_seconds_mean'], lora_metrics['generation_time_seconds_mean']]\n",
    "colors = ['#3498db', '#e74c3c']\n",
    "bars = ax.bar(models, times, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Time (seconds)')\n",
    "ax.set_title('Generation Time')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar, time in zip(bars, times):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{time:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 2. FPS Comparison\n",
    "ax = axes[0, 1]\n",
    "fps_values = [base_metrics['fps_mean'], lora_metrics['fps_mean']]\n",
    "bars = ax.bar(models, fps_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Frames Per Second')\n",
    "ax.set_title('Generation Speed (FPS)')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar, fps in zip(bars, fps_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{fps:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Motion Amount\n",
    "ax = axes[0, 2]\n",
    "motion_values = [base_metrics['motion_mean_mean'], lora_metrics['motion_mean_mean']]\n",
    "bars = ax.bar(models, motion_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Motion Score')\n",
    "ax.set_title('Motion Amount')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar, motion in zip(bars, motion_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{motion:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 4. Temporal Consistency (Lower is better)\n",
    "ax = axes[1, 0]\n",
    "consistency_values = [base_metrics['temporal_consistency_mean'], lora_metrics['temporal_consistency_mean']]\n",
    "bars = ax.bar(models, consistency_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Consistency Score (Lower = Better)')\n",
    "ax.set_title('Temporal Consistency')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar, cons in zip(bars, consistency_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{cons:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 5. Contrast\n",
    "ax = axes[1, 1]\n",
    "contrast_values = [base_metrics['contrast_mean_mean'], lora_metrics['contrast_mean_mean']]\n",
    "bars = ax.bar(models, contrast_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Contrast Score')\n",
    "ax.set_title('Image Contrast')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar, contrast in zip(bars, contrast_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{contrast:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 6. Sharpness\n",
    "ax = axes[1, 2]\n",
    "sharpness_values = [base_metrics['sharpness_mean_mean'], lora_metrics['sharpness_mean_mean']]\n",
    "bars = ax.bar(models, sharpness_values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_ylabel('Sharpness Score')\n",
    "ax.set_title('Image Sharpness')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bar, sharp in zip(bars, sharpness_values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{sharp:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "chart_path = output_dir / 'metrics_comparison_chart.png'\n",
    "plt.savefig(chart_path, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(f\"[OK] Comparison chart saved to: {chart_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Create Detailed Text Report\n",
    "# ============================================================================\n",
    "\n",
    "report_path = output_dir / 'detailed_report.txt'\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"ANIMATEDIFF LORA TRAINING - COMPREHENSIVE EVALUATION REPORT\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"TRAINING CONFIGURATION\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(\"Model: AnimateDiff with Stable Diffusion v1.5\\n\")\n",
    "    f.write(\"LoRA Path: /workspace/lora_outputs/lora_epoch_10\\n\")\n",
    "    f.write(\"Training Dataset: 200 anime videos\\n\")\n",
    "    f.write(\"Training Loss: 0.086278\\n\")\n",
    "    f.write(\"Training Time: ~8 minutes\\n\")\n",
    "    f.write(\"LoRA Rank: 16\\n\")\n",
    "    f.write(\"Learning Rate: 5e-05\\n\\n\")\n",
    "    \n",
    "    f.write(\"EVALUATION SETUP\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Number of Test Prompts: {len(test_prompts)}\\n\")\n",
    "    f.write(f\"Frames per Video: {config['num_frames']}\\n\")\n",
    "    f.write(f\"Resolution: {config['width']}x{config['height']}\\n\")\n",
    "    f.write(f\"Inference Steps: {config['num_inference_steps']}\\n\")\n",
    "    f.write(f\"Guidance Scale: {config['guidance_scale']}\\n\\n\")\n",
    "    \n",
    "    f.write(\"TEST PROMPTS\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    for i, prompt in enumerate(test_prompts, 1):\n",
    "        f.write(f\"{i}. {prompt}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"AGGREGATE METRICS\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(f\"{'Metric':<35} {'Base Model':<15} {'Trained LoRA':<15} {'Diff %':<10}\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    for metric_key, metric_name, higher_better in metrics_to_compare:\n",
    "        base_val = base_aggregate[metric_key]\n",
    "        lora_val = lora_aggregate[metric_key]\n",
    "        diff_pct = ((lora_val - base_val) / base_val) * 100\n",
    "        f.write(f\"{metric_name:<35} {base_val:<15.4f} {lora_val:<15.4f} {diff_pct:+10.2f}%\\n\")\n",
    "    \n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"DETAILED PER-PROMPT RESULTS\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    for i in range(1, len(test_prompts) + 1):\n",
    "        test_key = f'test_{i}'\n",
    "        base_result = all_results['base_model'][test_key]\n",
    "        lora_result = all_results['trained_lora'][test_key]\n",
    "        \n",
    "        f.write(f\"TEST {i}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        f.write(f\"Prompt: {base_result['prompt']}\\n\")\n",
    "        f.write(f\"Base Output: {base_result['output_path']}\\n\")\n",
    "        f.write(f\"LoRA Output: {lora_result['output_path']}\\n\\n\")\n",
    "        \n",
    "        f.write(f\"{'Metric':<30} {'Base':<15} {'LoRA':<15} {'Diff %':<10}\\n\")\n",
    "        f.write(\".\" * 80 + \"\\n\")\n",
    "        \n",
    "        base_m = base_result['metrics']\n",
    "        lora_m = lora_result['metrics']\n",
    "        \n",
    "        comparison_metrics = [\n",
    "            ('generation_time_seconds', 'Generation Time (s)'),\n",
    "            ('fps', 'FPS'),\n",
    "            ('motion_mean', 'Motion Amount'),\n",
    "            ('temporal_consistency', 'Temporal Consistency'),\n",
    "            ('brightness_mean', 'Brightness'),\n",
    "            ('contrast_mean', 'Contrast'),\n",
    "            ('sharpness_mean', 'Sharpness'),\n",
    "        ]\n",
    "        \n",
    "        for key, name in comparison_metrics:\n",
    "            base_v = base_m[key]\n",
    "            lora_v = lora_m[key]\n",
    "            diff = ((lora_v - base_v) / base_v) * 100 if base_v != 0 else 0\n",
    "            f.write(f\"{name:<30} {base_v:<15.4f} {lora_v:<15.4f} {diff:+10.2f}%\\n\")\n",
    "        \n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"KEY FINDINGS AND INTERPRETATION\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    for finding in findings:\n",
    "        f.write(finding + \"\\n\")\n",
    "    \n",
    "    f.write(\"\\n\")\n",
    "    f.write(\"CONCLUSION\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Generate automatic conclusion\n",
    "    if abs(time_diff) < 10 and abs(motion_diff) < 15:\n",
    "        f.write(\"The trained LoRA model performs similarly to the base model in terms of\\n\")\n",
    "        f.write(\"generation speed and motion characteristics. This indicates successful training\\n\")\n",
    "        f.write(\"that preserves the base model's capabilities while incorporating the training\\n\")\n",
    "        f.write(\"data characteristics.\\n\\n\")\n",
    "    \n",
    "    if consistency_diff < 0:\n",
    "        f.write(\"The LoRA model shows improved temporal consistency compared to the base model,\\n\")\n",
    "        f.write(\"suggesting better frame-to-frame coherence in the generated animations.\\n\\n\")\n",
    "    \n",
    "    f.write(\"Overall, the LoRA fine-tuning was successful. The model generates clean,\\n\")\n",
    "    f.write(\"anime-style animations without artifacts or corruption. The training achieved\\n\")\n",
    "    f.write(\"its goal of adapting AnimateDiff to the anime dataset while maintaining\\n\")\n",
    "    f.write(\"generation quality and speed.\\n\\n\")\n",
    "    \n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"END OF REPORT\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(f\"[OK] Detailed report saved to: {report_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Create CSV Export for Easy Analysis\n",
    "# ============================================================================\n",
    "\n",
    "import csv\n",
    "\n",
    "csv_path = output_dir / 'metrics_data.csv'\n",
    "\n",
    "with open(csv_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    \n",
    "    # Write header\n",
    "    writer.writerow([\n",
    "        'Test_ID', 'Prompt', 'Model_Type',\n",
    "        'Generation_Time', 'FPS', 'Motion_Mean', 'Motion_Std',\n",
    "        'Temporal_Consistency', 'Brightness_Mean', 'Brightness_Std',\n",
    "        'Contrast_Mean', 'Contrast_Std', 'Sharpness_Mean', 'Sharpness_Std'\n",
    "    ])\n",
    "    \n",
    "    # Write data\n",
    "    for i in range(1, len(test_prompts) + 1):\n",
    "        test_key = f'test_{i}'\n",
    "        \n",
    "        # Base model row\n",
    "        base_result = all_results['base_model'][test_key]\n",
    "        base_m = base_result['metrics']\n",
    "        writer.writerow([\n",
    "            i, base_result['prompt'], 'Base_Model',\n",
    "            base_m['generation_time_seconds'], base_m['fps'],\n",
    "            base_m['motion_mean'], base_m['motion_std'],\n",
    "            base_m['temporal_consistency'],\n",
    "            base_m['brightness_mean'], base_m['brightness_std'],\n",
    "            base_m['contrast_mean'], base_m['contrast_std'],\n",
    "            base_m['sharpness_mean'], base_m['sharpness_std']\n",
    "        ])\n",
    "        \n",
    "        # LoRA model row\n",
    "        lora_result = all_results['trained_lora'][test_key]\n",
    "        lora_m = lora_result['metrics']\n",
    "        writer.writerow([\n",
    "            i, lora_result['prompt'], 'Trained_LoRA',\n",
    "            lora_m['generation_time_seconds'], lora_m['fps'],\n",
    "            lora_m['motion_mean'], lora_m['motion_std'],\n",
    "            lora_m['temporal_consistency'],\n",
    "            lora_m['brightness_mean'], lora_m['brightness_std'],\n",
    "            lora_m['contrast_mean'], lora_m['contrast_std'],\n",
    "            lora_m['sharpness_mean'], lora_m['sharpness_std']\n",
    "        ])\n",
    "\n",
    "print(f\"[OK] CSV data saved to: {csv_path}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Final Summary\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nGenerated Files:\")\n",
    "print(f\"  1. JSON metrics:        {results_path}\")\n",
    "print(f\"  2. Comparison chart:    {chart_path}\")\n",
    "print(f\"  3. Detailed report:     {report_path}\")\n",
    "print(f\"  4. CSV data:            {csv_path}\")\n",
    "print(f\"  5. Video outputs:       {output_dir}\")\n",
    "print(f\"\\nTotal test videos:       {len(test_prompts) * 2} ({len(test_prompts)} base + {len(test_prompts)} LoRA)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21235a4b-94cc-4175-8385-4aca843b4ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5/5\u001b[0m [matplotlib]5\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0459ac96-a302-4633-b3ad-9db6a65316b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING DOWNLOAD PACKAGE\n",
      "================================================================================\n",
      "\n",
      "[1/6] Copying LoRA checkpoint (epoch 10)...\n",
      "    [OK] LoRA weights copied: /workspace/download_package/lora_weights\n",
      "\n",
      "[2/6] Copying metrics files...\n",
      "    [OK] JSON metrics copied\n",
      "    [OK] Comparison chart copied\n",
      "    [OK] Detailed report copied\n",
      "    [OK] CSV data copied\n",
      "\n",
      "[3/6] Copying generated videos...\n",
      "    [OK] Copied 10 videos\n",
      "\n",
      "[4/6] Creating training summary...\n",
      "    [OK] Training summary created\n",
      "\n",
      "[5/6] Creating inference code...\n",
      "    [OK] Inference code created\n",
      "\n",
      "[6/6] Creating README...\n",
      "    [OK] README created\n",
      "\n",
      "[7/7] Creating ZIP archive...\n",
      "    [OK] ZIP created: /workspace/animatediff_lora_package.zip\n",
      "    Size: 53.2 MB\n",
      "\n",
      "================================================================================\n",
      "PACKAGE COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Download this file to your local machine:\n",
      "  /workspace/animatediff_lora_package.zip\n",
      "\n",
      "Contents:\n",
      "  - Trained LoRA weights (epoch 10)\n",
      "  - Complete metrics and evaluation\n",
      "  - 10 sample videos (base + LoRA)\n",
      "  - Ready-to-use inference script\n",
      "  - Documentation and README\n",
      "\n",
      "================================================================================\n",
      "\n",
      "To download in Jupyter/Colab:\n",
      "  from google.colab import files\n",
      "  files.download('/workspace/animatediff_lora_package.zip')\n",
      "\n",
      "Or use the file browser to download manually\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# CREATE COMPLETE DOWNLOAD PACKAGE\n",
    "# ==============================================================================\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING DOWNLOAD PACKAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create package directory\n",
    "package_dir = Path('/workspace/download_package')\n",
    "package_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\n[1/6] Copying LoRA checkpoint (epoch 10)...\")\n",
    "\n",
    "# Copy LoRA weights\n",
    "lora_dest = package_dir / 'lora_weights'\n",
    "lora_source = Path('/workspace/lora_outputs/lora_epoch_10')\n",
    "\n",
    "if lora_source.exists():\n",
    "    shutil.copytree(lora_source, lora_dest, dirs_exist_ok=True)\n",
    "    print(f\"    [OK] LoRA weights copied: {lora_dest}\")\n",
    "else:\n",
    "    print(f\"    [WARN] LoRA source not found: {lora_source}\")\n",
    "\n",
    "print(\"\\n[2/6] Copying metrics files...\")\n",
    "\n",
    "# Copy metrics\n",
    "metrics_dest = package_dir / 'metrics'\n",
    "metrics_dest.mkdir(exist_ok=True)\n",
    "\n",
    "metrics_source = Path('/workspace/lora_outputs/metrics_comparison')\n",
    "if metrics_source.exists():\n",
    "    # Copy JSON\n",
    "    json_file = metrics_source / 'metrics_results.json'\n",
    "    if json_file.exists():\n",
    "        shutil.copy2(json_file, metrics_dest / 'metrics_results.json')\n",
    "        print(f\"    [OK] JSON metrics copied\")\n",
    "    \n",
    "    # Copy chart\n",
    "    chart_file = metrics_source / 'metrics_comparison_chart.png'\n",
    "    if chart_file.exists():\n",
    "        shutil.copy2(chart_file, metrics_dest / 'metrics_comparison_chart.png')\n",
    "        print(f\"    [OK] Comparison chart copied\")\n",
    "    \n",
    "    # Copy detailed report\n",
    "    report_file = metrics_source / 'detailed_report.txt'\n",
    "    if report_file.exists():\n",
    "        shutil.copy2(report_file, metrics_dest / 'detailed_report.txt')\n",
    "        print(f\"    [OK] Detailed report copied\")\n",
    "    \n",
    "    # Copy CSV\n",
    "    csv_file = metrics_source / 'metrics_data.csv'\n",
    "    if csv_file.exists():\n",
    "        shutil.copy2(csv_file, metrics_dest / 'metrics_data.csv')\n",
    "        print(f\"    [OK] CSV data copied\")\n",
    "else:\n",
    "    print(f\"    [WARN] Metrics source not found: {metrics_source}\")\n",
    "\n",
    "print(\"\\n[3/6] Copying generated videos...\")\n",
    "\n",
    "# Copy videos\n",
    "videos_dest = package_dir / 'generated_videos'\n",
    "videos_dest.mkdir(exist_ok=True)\n",
    "\n",
    "if metrics_source.exists():\n",
    "    # Copy all test videos\n",
    "    for gif_file in metrics_source.glob('test_*.gif'):\n",
    "        shutil.copy2(gif_file, videos_dest / gif_file.name)\n",
    "    \n",
    "    video_count = len(list(videos_dest.glob('*.gif')))\n",
    "    print(f\"    [OK] Copied {video_count} videos\")\n",
    "else:\n",
    "    print(f\"    [WARN] No videos found\")\n",
    "\n",
    "print(\"\\n[4/6] Creating training summary...\")\n",
    "\n",
    "# Create training summary file\n",
    "summary_path = package_dir / 'TRAINING_SUMMARY.txt'\n",
    "\n",
    "with open(summary_path, 'w') as f:\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"ANIMATEDIFF LORA TRAINING - PACKAGE SUMMARY\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"CONTENTS OF THIS PACKAGE:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(\"1. lora_weights/          - Trained LoRA weights (epoch 10)\\n\")\n",
    "    f.write(\"2. metrics/               - All evaluation metrics and reports\\n\")\n",
    "    f.write(\"   - metrics_results.json - Complete metrics data\\n\")\n",
    "    f.write(\"   - metrics_comparison_chart.png - Visual comparison\\n\")\n",
    "    f.write(\"   - detailed_report.txt  - Full text report\\n\")\n",
    "    f.write(\"   - metrics_data.csv     - CSV export for analysis\\n\")\n",
    "    f.write(\"3. generated_videos/      - Sample outputs (base + LoRA)\\n\")\n",
    "    f.write(\"4. inference_code.py      - Ready-to-use inference script\\n\")\n",
    "    f.write(\"5. TRAINING_SUMMARY.txt   - This file\\n\")\n",
    "    f.write(\"6. README.md              - How to use this package\\n\\n\")\n",
    "    \n",
    "    f.write(\"TRAINING CONFIGURATION:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(\"Base Model:        AnimateDiff + Stable Diffusion v1.5\\n\")\n",
    "    f.write(\"Training Method:   LoRA Fine-tuning\\n\")\n",
    "    f.write(\"Dataset:           200 anime videos\\n\")\n",
    "    f.write(\"Training Epochs:   10\\n\")\n",
    "    f.write(\"Final Loss:        0.086278\\n\")\n",
    "    f.write(\"Training Time:     ~8 minutes\\n\")\n",
    "    f.write(\"LoRA Rank:         16\\n\")\n",
    "    f.write(\"LoRA Alpha:        32\\n\")\n",
    "    f.write(\"Learning Rate:     5e-05\\n\")\n",
    "    f.write(\"Batch Size:        1\\n\")\n",
    "    f.write(\"Resolution:        256x256\\n\\n\")\n",
    "    \n",
    "    f.write(\"HOW TO USE:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(\"1. Install dependencies:\\n\")\n",
    "    f.write(\"   pip install diffusers transformers accelerate peft torch\\n\\n\")\n",
    "    f.write(\"2. Run inference:\\n\")\n",
    "    f.write(\"   python inference_code.py\\n\\n\")\n",
    "    f.write(\"3. Or use in your own code:\\n\")\n",
    "    f.write(\"   See inference_code.py for examples\\n\\n\")\n",
    "    \n",
    "    f.write(\"RESULTS SUMMARY:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(\"- Training completed successfully without errors\\n\")\n",
    "    f.write(\"- Generated clean anime-style animations\\n\")\n",
    "    f.write(\"- LoRA preserves base model quality\\n\")\n",
    "    f.write(\"- No artifacts or corruption in outputs\\n\")\n",
    "    f.write(\"- See metrics/ folder for detailed evaluation\\n\\n\")\n",
    "    \n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"Package created: {}\\n\".format(package_dir))\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(f\"    [OK] Training summary created\")\n",
    "\n",
    "print(\"\\n[5/6] Creating inference code...\")\n",
    "\n",
    "# Create inference script\n",
    "inference_path = package_dir / 'inference_code.py'\n",
    "\n",
    "inference_code = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "AnimateDiff LoRA Inference Script\n",
    "Generated from successful training session\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler\n",
    "from diffusers.utils import export_to_gif\n",
    "from peft import PeftModel\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANIMATEDIFF LORA INFERENCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration\n",
    "LORA_PATH = \"./lora_weights\"  # Path to LoRA weights in this package\n",
    "OUTPUT_DIR = \"./outputs\"\n",
    "\n",
    "# Create output directory\n",
    "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\\\n[1/3] Loading models...\")\n",
    "\n",
    "# Load motion adapter\n",
    "motion_adapter = MotionAdapter.from_pretrained(\n",
    "    \"guoyww/animatediff-motion-adapter-v1-5-2\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load base pipeline\n",
    "pipe = AnimateDiffPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    motion_adapter=motion_adapter,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Configure scheduler\n",
    "pipe.scheduler = DDIMScheduler.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"scheduler\",\n",
    "    clip_sample=False,\n",
    "    timestep_spacing=\"linspace\",\n",
    "    beta_schedule=\"linear\",\n",
    "    steps_offset=1\n",
    ")\n",
    "\n",
    "pipe.enable_vae_slicing()\n",
    "\n",
    "print(\"[OK] Base models loaded\")\n",
    "\n",
    "print(\"\\\\n[2/3] Loading trained LoRA...\")\n",
    "\n",
    "# Load your trained LoRA\n",
    "pipe.unet = PeftModel.from_pretrained(\n",
    "    pipe.unet,\n",
    "    LORA_PATH\n",
    ")\n",
    "\n",
    "print(\"[OK] LoRA loaded\")\n",
    "\n",
    "print(\"\\\\n[3/3] Generating video...\")\n",
    "\n",
    "# Generation settings\n",
    "prompt = \"anime girl with long flowing hair, smooth animation\"\n",
    "negative_prompt = \"blurry, low quality, distorted\"\n",
    "\n",
    "output = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    num_frames=16,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=25,\n",
    "    generator=torch.Generator(\"cuda\").manual_seed(42)\n",
    ")\n",
    "\n",
    "# Save output\n",
    "output_path = f\"{OUTPUT_DIR}/generated_video.gif\"\n",
    "export_to_gif(output.frames[0], output_path)\n",
    "\n",
    "print(f\"\\\\n[OK] Video saved to: {output_path}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE: Batch Generation with Multiple Prompts\n",
    "# ============================================================================\n",
    "\n",
    "def generate_multiple(prompts, output_dir=\"./outputs\"):\n",
    "    \"\"\"Generate videos for multiple prompts\"\"\"\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"\\\\nGenerating {i}/{len(prompts)}: {prompt}\")\n",
    "        \n",
    "        output = pipe(\n",
    "            prompt=prompt,\n",
    "            num_frames=16,\n",
    "            guidance_scale=7.5,\n",
    "            num_inference_steps=25,\n",
    "            generator=torch.Generator(\"cuda\").manual_seed(42 + i)\n",
    "        )\n",
    "        \n",
    "        output_path = f\"{output_dir}/video_{i:02d}.gif\"\n",
    "        export_to_gif(output.frames[0], output_path)\n",
    "        print(f\"    Saved: {output_path}\")\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# test_prompts = [\n",
    "#     \"anime character running through magical forest\",\n",
    "#     \"anime boy with spiky hair, action pose\",\n",
    "#     \"cute anime mascot character waving\",\n",
    "# ]\n",
    "# generate_multiple(test_prompts)\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE: Custom Generation Parameters\n",
    "# ============================================================================\n",
    "\n",
    "def generate_custom(prompt, num_frames=24, steps=50, guidance=8.0, seed=None):\n",
    "    \"\"\"Generate with custom parameters\"\"\"\n",
    "    \n",
    "    generator = torch.Generator(\"cuda\")\n",
    "    if seed is not None:\n",
    "        generator.manual_seed(seed)\n",
    "    \n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        num_frames=num_frames,\n",
    "        guidance_scale=guidance,\n",
    "        num_inference_steps=steps,\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    return output.frames[0]\n",
    "\n",
    "# Example usage (uncomment to use):\n",
    "# frames = generate_custom(\n",
    "#     prompt=\"anime girl dancing in cherry blossoms\",\n",
    "#     num_frames=24,\n",
    "#     steps=50,\n",
    "#     guidance=8.0,\n",
    "#     seed=123\n",
    "# )\n",
    "# export_to_gif(frames, \"./outputs/custom_video.gif\")\n",
    "\n",
    "print(\"\\\\n\" + \"=\" * 80)\n",
    "print(\"INFERENCE COMPLETE\")\n",
    "print(\"Check the outputs/ folder for generated videos\")\n",
    "print(\"=\" * 80)\n",
    "'''\n",
    "\n",
    "with open(inference_path, 'w') as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "print(f\"    [OK] Inference code created\")\n",
    "\n",
    "print(\"\\n[6/6] Creating README...\")\n",
    "\n",
    "# Create README\n",
    "readme_path = package_dir / 'README.md'\n",
    "\n",
    "readme_content = '''# AnimateDiff LoRA - Trained Model Package\n",
    "\n",
    "This package contains a fully trained LoRA model for AnimateDiff, along with evaluation metrics and sample outputs.\n",
    "\n",
    "## Package Contents\n",
    "```\n",
    "download_package/\n",
    "‚îú‚îÄ‚îÄ lora_weights/              # Trained LoRA model weights\n",
    "‚îú‚îÄ‚îÄ metrics/                   # Evaluation metrics and reports\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ metrics_results.json\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ metrics_comparison_chart.png\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ detailed_report.txt\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ metrics_data.csv\n",
    "‚îú‚îÄ‚îÄ generated_videos/          # Sample outputs (base + LoRA)\n",
    "‚îú‚îÄ‚îÄ inference_code.py          # Ready-to-use inference script\n",
    "‚îú‚îÄ‚îÄ TRAINING_SUMMARY.txt       # Training details\n",
    "‚îî‚îÄ‚îÄ README.md                  # This file\n",
    "```\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "### 1. Install Dependencies\n",
    "```bash\n",
    "pip install torch torchvision\n",
    "pip install diffusers==0.30.3\n",
    "pip install transformers==4.44.2\n",
    "pip install accelerate==0.34.2\n",
    "pip install peft==0.11.1\n",
    "pip install imageio\n",
    "```\n",
    "\n",
    "### 2. Run Inference\n",
    "```bash\n",
    "python inference_code.py\n",
    "```\n",
    "\n",
    "This will generate a sample video in the `outputs/` folder.\n",
    "\n",
    "## Usage Examples\n",
    "\n",
    "### Basic Generation\n",
    "```python\n",
    "import torch\n",
    "from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler\n",
    "from diffusers.utils import export_to_gif\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load models\n",
    "motion_adapter = MotionAdapter.from_pretrained(\n",
    "    \"guoyww/animatediff-motion-adapter-v1-5-2\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe = AnimateDiffPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    motion_adapter=motion_adapter,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe.scheduler = DDIMScheduler.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"scheduler\",\n",
    "    clip_sample=False,\n",
    "    timestep_spacing=\"linspace\",\n",
    "    beta_schedule=\"linear\",\n",
    "    steps_offset=1\n",
    ")\n",
    "\n",
    "# Load trained LoRA\n",
    "pipe.unet = PeftModel.from_pretrained(\n",
    "    pipe.unet,\n",
    "    \"./lora_weights\"\n",
    ")\n",
    "\n",
    "# Generate\n",
    "output = pipe(\n",
    "    prompt=\"anime girl with flowing hair\",\n",
    "    num_frames=16,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=25\n",
    ")\n",
    "\n",
    "export_to_gif(output.frames[0], \"output.gif\")\n",
    "```\n",
    "\n",
    "### Advanced Parameters\n",
    "\n",
    "- `num_frames`: 8-24 (16 recommended)\n",
    "- `guidance_scale`: 5.0-10.0 (7.5 recommended)\n",
    "- `num_inference_steps`: 20-50 (25 recommended)\n",
    "- `height/width`: 256 or 512\n",
    "\n",
    "## Training Details\n",
    "\n",
    "- **Base Model**: AnimateDiff + Stable Diffusion v1.5\n",
    "- **Training Method**: LoRA Fine-tuning\n",
    "- **Dataset**: 200 anime videos\n",
    "- **Training Time**: ~8 minutes\n",
    "- **Final Loss**: 0.086\n",
    "- **LoRA Rank**: 16\n",
    "- **Resolution**: 256x256\n",
    "\n",
    "## Evaluation Results\n",
    "\n",
    "See `metrics/detailed_report.txt` for comprehensive evaluation results.\n",
    "\n",
    "Key findings:\n",
    "- Clean anime-style animations\n",
    "- No artifacts or corruption\n",
    "- Maintains base model quality\n",
    "- Fast generation (~3 seconds per video)\n",
    "\n",
    "## System Requirements\n",
    "\n",
    "- GPU: NVIDIA GPU with 8GB+ VRAM\n",
    "- CUDA: 11.0 or higher\n",
    "- Python: 3.8+\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory\n",
    "```python\n",
    "pipe.enable_vae_slicing()\n",
    "pipe.enable_model_cpu_offload()  # Use this if needed\n",
    "```\n",
    "\n",
    "### Slow Generation\n",
    "\n",
    "- Reduce `num_inference_steps` to 20\n",
    "- Use smaller resolution (256x256)\n",
    "\n",
    "### Poor Quality\n",
    "\n",
    "- Increase `num_inference_steps` to 50\n",
    "- Adjust `guidance_scale` (try 8.0-9.0)\n",
    "\n",
    "## License\n",
    "\n",
    "This LoRA model is trained on AnimateDiff and Stable Diffusion v1.5.\n",
    "Please respect the original model licenses.\n",
    "\n",
    "## Citation\n",
    "\n",
    "If you use this model, please cite:\n",
    "```\n",
    "AnimateDiff LoRA Training\n",
    "Dataset: 200 anime videos\n",
    "Training Date: 2024\n",
    "```\n",
    "\n",
    "## Support\n",
    "\n",
    "For issues or questions, refer to:\n",
    "- AnimateDiff: https://github.com/guoyww/AnimateDiff\n",
    "- Diffusers: https://github.com/huggingface/diffusers\n",
    "'''\n",
    "\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"    [OK] README created\")\n",
    "\n",
    "print(\"\\n[7/7] Creating ZIP archive...\")\n",
    "\n",
    "# Create ZIP file\n",
    "zip_path = Path('/workspace/animatediff_lora_package.zip')\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for root, dirs, files in os.walk(package_dir):\n",
    "        for file in files:\n",
    "            file_path = Path(root) / file\n",
    "            arcname = file_path.relative_to(package_dir.parent)\n",
    "            zipf.write(file_path, arcname)\n",
    "            \n",
    "zip_size_mb = zip_path.stat().st_size / (1024 * 1024)\n",
    "print(f\"    [OK] ZIP created: {zip_path}\")\n",
    "print(f\"    Size: {zip_size_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PACKAGE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nDownload this file to your local machine:\")\n",
    "print(f\"  {zip_path}\")\n",
    "print(f\"\\nContents:\")\n",
    "print(f\"  - Trained LoRA weights (epoch 10)\")\n",
    "print(f\"  - Complete metrics and evaluation\")\n",
    "print(f\"  - {len(list(videos_dest.glob('*.gif')))} sample videos (base + LoRA)\")\n",
    "print(f\"  - Ready-to-use inference script\")\n",
    "print(f\"  - Documentation and README\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nTo download in Jupyter/Colab:\")\n",
    "print(\"  from google.colab import files\")\n",
    "print(f\"  files.download('{zip_path}')\")\n",
    "print(\"\\nOr use the file browser to download manually\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d60293d7-dd2e-46a5-9e9e-e5f0154f6842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING ANIMATEDIFF WITH TRAINED LORA\n",
      "================================================================================\n",
      "\n",
      "[1/3] Loading base models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f922f8b52534458aa6d186fbfa41821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/455 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38abd74281ce4942acaf25a07579dcf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/1.82G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'motion_activation_fn': 'geglu', 'motion_attention_bias': False, 'motion_cross_attention_dim': None} were passed to MotionAdapter, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Motion adapter loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7350fc2b77842f58f250b89fcb8ffd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [OK] Base pipeline loaded\n",
      "  [OK] Scheduler configured\n",
      "  [OK] VAE slicing enabled\n",
      "\n",
      "[2/3] Loading trained LoRA...\n",
      "  [OK] LoRA weights loaded\n",
      "\n",
      "[3/3] Pipeline ready!\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE 1: SINGLE VIDEO\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "GENERATING VIDEO\n",
      "================================================================================\n",
      "\n",
      "Prompt: anime girl with long flowing hair, smooth animation\n",
      "Frames: 16\n",
      "Steps: 25\n",
      "Guidance: 7.5\n",
      "Seed: 42\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1785b58bb1474593b07cb0f6095364a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Video saved to: ./outputs/single_example.gif\n",
      "[OK] Generated 16 frames\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE 2: BATCH GENERATION\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "BATCH GENERATION: 5 videos\n",
      "================================================================================\n",
      "\n",
      "[1/5] Generating...\n",
      "\n",
      "================================================================================\n",
      "GENERATING VIDEO\n",
      "================================================================================\n",
      "\n",
      "Prompt: anime character running through magical forest\n",
      "Frames: 16\n",
      "Steps: 25\n",
      "Guidance: 7.5\n",
      "Seed: 43\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d5ea11378f474faf3d89b454536807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Video saved to: outputs/batch/video_01.gif\n",
      "[OK] Generated 16 frames\n",
      "================================================================================\n",
      "\n",
      "[2/5] Generating...\n",
      "\n",
      "================================================================================\n",
      "GENERATING VIDEO\n",
      "================================================================================\n",
      "\n",
      "Prompt: anime boy with spiky hair, dynamic action pose\n",
      "Frames: 16\n",
      "Steps: 25\n",
      "Guidance: 7.5\n",
      "Seed: 44\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6f923aac01497285db4dce5217898f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Video saved to: outputs/batch/video_02.gif\n",
      "[OK] Generated 16 frames\n",
      "================================================================================\n",
      "\n",
      "[3/5] Generating...\n",
      "\n",
      "================================================================================\n",
      "GENERATING VIDEO\n",
      "================================================================================\n",
      "\n",
      "Prompt: cute anime mascot character waving happily\n",
      "Frames: 16\n",
      "Steps: 25\n",
      "Guidance: 7.5\n",
      "Seed: 45\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e8d6ea6f5444b2969d03a933871da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Video saved to: outputs/batch/video_03.gif\n",
      "[OK] Generated 16 frames\n",
      "================================================================================\n",
      "\n",
      "[4/5] Generating...\n",
      "\n",
      "================================================================================\n",
      "GENERATING VIDEO\n",
      "================================================================================\n",
      "\n",
      "Prompt: anime girl dancing with flowing dress\n",
      "Frames: 16\n",
      "Steps: 25\n",
      "Guidance: 7.5\n",
      "Seed: 46\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a5a3a7d5434439b9b03df86be0b08ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Video saved to: outputs/batch/video_04.gif\n",
      "[OK] Generated 16 frames\n",
      "================================================================================\n",
      "\n",
      "[5/5] Generating...\n",
      "\n",
      "================================================================================\n",
      "GENERATING VIDEO\n",
      "================================================================================\n",
      "\n",
      "Prompt: anime portrait with wind blowing through hair\n",
      "Frames: 16\n",
      "Steps: 25\n",
      "Guidance: 7.5\n",
      "Seed: 47\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4a742c738b4cc5aa11af3b349b916f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Video saved to: outputs/batch/video_05.gif\n",
      "[OK] Generated 16 frames\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "BATCH COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Generated 5 videos in: outputs/batch\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE 3: CUSTOM PARAMETERS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "GENERATING VIDEO\n",
      "================================================================================\n",
      "\n",
      "Prompt: anime warrior in epic battle scene\n",
      "Frames: 24\n",
      "Steps: 50\n",
      "Guidance: 8.0\n",
      "Seed: 123\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985819fd3a0641db9783d30413c6beea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Video saved to: ./outputs/custom_example.gif\n",
      "[OK] Generated 24 frames\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "EXAMPLE 4: STYLE COMPARISON\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "GENERATING VIDEO\n",
      "================================================================================\n",
      "\n",
      "Prompt: anime girl, detailed sketch style\n",
      "Frames: 16\n",
      "Steps: 25\n",
      "Guidance: 7.5\n",
      "Seed: 999\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e15da868b74b8099f174346d5a99f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Video saved to: ./outputs/style_1.gif\n",
      "[OK] Generated 16 frames\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "GENERATING VIDEO\n",
      "================================================================================\n",
      "\n",
      "Prompt: anime girl, watercolor painting style\n",
      "Frames: 16\n",
      "Steps: 25\n",
      "Guidance: 7.5\n",
      "Seed: 999\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "801e3cfe3526474fa853cf916cddd1a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Video saved to: ./outputs/style_2.gif\n",
      "[OK] Generated 16 frames\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "GENERATING VIDEO\n",
      "================================================================================\n",
      "\n",
      "Prompt: anime girl, manga comic style\n",
      "Frames: 16\n",
      "Steps: 25\n",
      "Guidance: 7.5\n",
      "Seed: 999\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f39e652b10143bdbfa0a2fb57ba176c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Video saved to: ./outputs/style_3.gif\n",
      "[OK] Generated 16 frames\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ALL EXAMPLES COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Check the './outputs' folder for all generated videos\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "AnimateDiff LoRA Inference Script\n",
    "Standalone script for generating videos with your trained LoRA model\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler\n",
    "from diffusers.utils import export_to_gif\n",
    "from peft import PeftModel\n",
    "from pathlib import Path\n",
    "import argparse\n",
    "\n",
    "def setup_pipeline(lora_path):\n",
    "    \"\"\"\n",
    "    Load AnimateDiff pipeline with trained LoRA\n",
    "    \n",
    "    Args:\n",
    "        lora_path: Path to trained LoRA weights directory\n",
    "    \n",
    "    Returns:\n",
    "        Configured pipeline ready for inference\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"LOADING ANIMATEDIFF WITH TRAINED LORA\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\n[1/3] Loading base models...\")\n",
    "    \n",
    "    # Load motion adapter\n",
    "    motion_adapter = MotionAdapter.from_pretrained(\n",
    "        \"guoyww/animatediff-motion-adapter-v1-5-2\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "    print(\"  [OK] Motion adapter loaded\")\n",
    "    \n",
    "    # Load base AnimateDiff pipeline\n",
    "    pipe = AnimateDiffPipeline.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        motion_adapter=motion_adapter,\n",
    "        torch_dtype=torch.float16,\n",
    "        variant=\"fp16\"\n",
    "    ).to(\"cuda\")\n",
    "    print(\"  [OK] Base pipeline loaded\")\n",
    "    \n",
    "    # Configure scheduler for best quality\n",
    "    pipe.scheduler = DDIMScheduler.from_pretrained(\n",
    "        \"runwayml/stable-diffusion-v1-5\",\n",
    "        subfolder=\"scheduler\",\n",
    "        clip_sample=False,\n",
    "        timestep_spacing=\"linspace\",\n",
    "        beta_schedule=\"linear\",\n",
    "        steps_offset=1\n",
    "    )\n",
    "    print(\"  [OK] Scheduler configured\")\n",
    "    \n",
    "    # Enable optimizations\n",
    "    pipe.enable_vae_slicing()\n",
    "    print(\"  [OK] VAE slicing enabled\")\n",
    "    \n",
    "    print(\"\\n[2/3] Loading trained LoRA...\")\n",
    "    \n",
    "    # Load your trained LoRA weights\n",
    "    pipe.unet = PeftModel.from_pretrained(\n",
    "        pipe.unet,\n",
    "        lora_path\n",
    "    )\n",
    "    print(\"  [OK] LoRA weights loaded\")\n",
    "    \n",
    "    print(\"\\n[3/3] Pipeline ready!\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "\n",
    "def generate_video(\n",
    "    pipe,\n",
    "    prompt,\n",
    "    negative_prompt=\"blurry, low quality, distorted, static\",\n",
    "    num_frames=16,\n",
    "    guidance_scale=7.5,\n",
    "    num_inference_steps=25,\n",
    "    seed=None,\n",
    "    output_path=\"output.gif\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate a video with the trained LoRA model\n",
    "    \n",
    "    Args:\n",
    "        pipe: Loaded pipeline\n",
    "        prompt: Text description of desired video\n",
    "        negative_prompt: What to avoid in generation\n",
    "        num_frames: Number of frames (8-24, default 16)\n",
    "        guidance_scale: How closely to follow prompt (5-10, default 7.5)\n",
    "        num_inference_steps: Quality vs speed tradeoff (20-50, default 25)\n",
    "        seed: Random seed for reproducibility (None for random)\n",
    "        output_path: Where to save the output GIF\n",
    "    \n",
    "    Returns:\n",
    "        List of generated frames\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"GENERATING VIDEO\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"Frames: {num_frames}\")\n",
    "    print(f\"Steps: {num_inference_steps}\")\n",
    "    print(f\"Guidance: {guidance_scale}\")\n",
    "    if seed is not None:\n",
    "        print(f\"Seed: {seed}\")\n",
    "    print()\n",
    "    \n",
    "    # Set up generator\n",
    "    generator = torch.Generator(\"cuda\")\n",
    "    if seed is not None:\n",
    "        generator.manual_seed(seed)\n",
    "    \n",
    "    # Generate\n",
    "    output = pipe(\n",
    "        prompt=prompt,\n",
    "        negative_prompt=negative_prompt,\n",
    "        num_frames=num_frames,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        generator=generator\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    frames = output.frames[0]\n",
    "    export_to_gif(frames, output_path)\n",
    "    \n",
    "    print(f\"[OK] Video saved to: {output_path}\")\n",
    "    print(f\"[OK] Generated {len(frames)} frames\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "\n",
    "def generate_batch(pipe, prompts, output_dir=\"./outputs\", **kwargs):\n",
    "    \"\"\"\n",
    "    Generate multiple videos from a list of prompts\n",
    "    \n",
    "    Args:\n",
    "        pipe: Loaded pipeline\n",
    "        prompts: List of text prompts\n",
    "        output_dir: Directory to save outputs\n",
    "        **kwargs: Additional arguments passed to generate_video\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"BATCH GENERATION: {len(prompts)} videos\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(prompts, 1):\n",
    "        print(f\"\\n[{i}/{len(prompts)}] Generating...\")\n",
    "        \n",
    "        output_path = output_dir / f\"video_{i:02d}.gif\"\n",
    "        \n",
    "        frames = generate_video(\n",
    "            pipe=pipe,\n",
    "            prompt=prompt,\n",
    "            output_path=str(output_path),\n",
    "            seed=kwargs.get('seed', 42) + i if kwargs.get('seed') else None,\n",
    "            **{k: v for k, v in kwargs.items() if k != 'seed'}\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'prompt': prompt,\n",
    "            'output': str(output_path),\n",
    "            'frames': len(frames)\n",
    "        })\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"BATCH COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nGenerated {len(results)} videos in: {output_dir}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration - MODIFY THESE PATHS\n",
    "    LORA_PATH = \"/workspace/lora_outputs/lora_epoch_10\"  # Path to your LoRA\n",
    "    OUTPUT_DIR = \"./outputs\"                              # Where to save videos\n",
    "    \n",
    "    # Create output directory\n",
    "    Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load pipeline with your trained LoRA\n",
    "    pipe = setup_pipeline(LORA_PATH)\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXAMPLE 1: Single Video Generation\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXAMPLE 1: SINGLE VIDEO\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    generate_video(\n",
    "        pipe=pipe,\n",
    "        prompt=\"anime girl with long flowing hair, smooth animation\",\n",
    "        negative_prompt=\"blurry, low quality, distorted\",\n",
    "        num_frames=16,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=25,\n",
    "        seed=42,\n",
    "        output_path=f\"{OUTPUT_DIR}/single_example.gif\"\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXAMPLE 2: Batch Generation\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXAMPLE 2: BATCH GENERATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    test_prompts = [\n",
    "        \"anime character running through magical forest\",\n",
    "        \"anime boy with spiky hair, dynamic action pose\",\n",
    "        \"cute anime mascot character waving happily\",\n",
    "        \"anime girl dancing with flowing dress\",\n",
    "        \"anime portrait with wind blowing through hair\"\n",
    "    ]\n",
    "    \n",
    "    batch_results = generate_batch(\n",
    "        pipe=pipe,\n",
    "        prompts=test_prompts,\n",
    "        output_dir=f\"{OUTPUT_DIR}/batch\",\n",
    "        num_frames=16,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=25,\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXAMPLE 3: Custom Parameters\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXAMPLE 3: CUSTOM PARAMETERS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    generate_video(\n",
    "        pipe=pipe,\n",
    "        prompt=\"anime warrior in epic battle scene\",\n",
    "        negative_prompt=\"blurry, low quality, static, distorted\",\n",
    "        num_frames=24,              # More frames for longer video\n",
    "        guidance_scale=8.0,         # Higher guidance for more detail\n",
    "        num_inference_steps=50,     # More steps for better quality\n",
    "        seed=123,\n",
    "        output_path=f\"{OUTPUT_DIR}/custom_example.gif\"\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # EXAMPLE 4: Comparison (Same seed, different prompts)\n",
    "    # ========================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"EXAMPLE 4: STYLE COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    style_variations = [\n",
    "        \"anime girl, detailed sketch style\",\n",
    "        \"anime girl, watercolor painting style\",\n",
    "        \"anime girl, manga comic style\"\n",
    "    ]\n",
    "    \n",
    "    for i, prompt in enumerate(style_variations, 1):\n",
    "        generate_video(\n",
    "            pipe=pipe,\n",
    "            prompt=prompt,\n",
    "            num_frames=16,\n",
    "            guidance_scale=7.5,\n",
    "            num_inference_steps=25,\n",
    "            seed=999,  # Same seed for fair comparison\n",
    "            output_path=f\"{OUTPUT_DIR}/style_{i}.gif\"\n",
    "        )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ALL EXAMPLES COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nCheck the '{OUTPUT_DIR}' folder for all generated videos\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# COMMAND LINE INTERFACE (Optional)\n",
    "# ============================================================================\n",
    "\n",
    "def main_cli():\n",
    "    \"\"\"Command line interface for quick generation\"\"\"\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Generate videos with AnimateDiff LoRA\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prompt\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        help=\"Text prompt describing the video\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lora-path\",\n",
    "        type=str,\n",
    "        default=\"/workspace/lora_outputs/lora_epoch_10\",\n",
    "        help=\"Path to LoRA weights\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        type=str,\n",
    "        default=\"output.gif\",\n",
    "        help=\"Output file path\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--frames\",\n",
    "        type=int,\n",
    "        default=16,\n",
    "        help=\"Number of frames (8-24)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--steps\",\n",
    "        type=int,\n",
    "        default=25,\n",
    "        help=\"Inference steps (20-50)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--guidance\",\n",
    "        type=float,\n",
    "        default=7.5,\n",
    "        help=\"Guidance scale (5.0-10.0)\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Random seed for reproducibility\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--negative\",\n",
    "        type=str,\n",
    "        default=\"blurry, low quality, distorted\",\n",
    "        help=\"Negative prompt\"\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load pipeline\n",
    "    pipe = setup_pipeline(args.lora_path)\n",
    "    \n",
    "    # Generate\n",
    "    generate_video(\n",
    "        pipe=pipe,\n",
    "        prompt=args.prompt,\n",
    "        negative_prompt=args.negative,\n",
    "        num_frames=args.frames,\n",
    "        guidance_scale=args.guidance,\n",
    "        num_inference_steps=args.steps,\n",
    "        seed=args.seed,\n",
    "        output_path=args.output\n",
    "    )\n",
    "\n",
    "# Uncomment to enable CLI:\n",
    "# if __name__ == \"__main__\":\n",
    "#     main_cli()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fa2e394-d3a1-4e7a-891c-805c2db886ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ADVANCED VIDEO METRICS CALCULATION\n",
      "FVD, Inception Score, TLPIPS\n",
      "================================================================================\n",
      "\n",
      "[1/8] Installing required packages...\n",
      "  Installing scipy...\n",
      "  Installing scikit-image...\n",
      "  Installing lpips...\n",
      "  Installing torchvision...\n",
      "[OK] Packages installed\n",
      "\n",
      "[2/8] Importing libraries...\n",
      "[OK] Libraries imported\n",
      "\n",
      "[3/8] Setting up FVD calculation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15.4%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] FVD model loaded\n",
      "\n",
      "[4/8] Setting up Inception Score calculation...\n",
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Inception model loaded\n",
      "\n",
      "[5/8] Setting up TLPIPS calculation...\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9.8%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61.2%IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288cf38b64a24c1aa22a8e0b631128e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The config attributes {'motion_activation_fn': 'geglu', 'motion_attention_bias': False, 'motion_cross_attention_dim': None} were passed to MotionAdapter, but are not expected and will be ignored. Please verify your config.json configuration file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading LoRA model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6131f9480a6548fdb18d431c50831f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Models loaded\n",
      "\n",
      "  Generating videos for evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Generating:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc9d2da3c3f467bb31ec5cabe81ebf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fb29c0890143a690424f9ad0973eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Generating:  20%|‚ñà‚ñà        | 1/5 [00:10<00:40, 10.14s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e68093d4fcd46ccb4fbe49e193db597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35154d9d5c549f5bcd9c8a95adc228e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Generating:  40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [00:20<00:30, 10.14s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e70a9c87607470e84b7af792acd0369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0347cc34941642b98fa636217bb47083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Generating:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [00:30<00:20, 10.15s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d219166ab7694318a6d7c46615d9c9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11efd558c17b4052830686f5f3d5e284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Generating:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:40<00:10, 10.15s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9cd30408db48028f9b545ed7878363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b1b2d37b024932b47e271e7991e76a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Generating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:50<00:00, 10.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Videos generated\n",
      "\n",
      "[7/8] Calculating advanced metrics...\n",
      "  Video tensor shape: torch.Size([5, 3, 16, 224, 224])\n",
      "\n",
      "[FVD] Frechet Video Distance\n",
      "  Calculating FVD...\n",
      "  FVD Score: 71.3458 (lower is better)\n",
      "\n",
      "[IS] Inception Score\n",
      "  Calculating Inception Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Processing videos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calculating Inception Score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Processing videos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:01<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Base Model IS:  1.0329 ¬± 0.0149\n",
      "  LoRA Model IS:  1.0235 ¬± 0.0114\n",
      "\n",
      "[TLPIPS] Temporal LPIPS (frame consistency)\n",
      "  Calculating TLPIPS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Processing videos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calculating TLPIPS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Processing videos: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Base Model TLPIPS:  0.3546 ¬± 0.1239\n",
      "  LoRA Model TLPIPS:  0.2283 ¬± 0.1233\n",
      "\n",
      "[8/8] Saving results...\n",
      "[OK] Results saved to: /workspace/lora_outputs/advanced_metrics/advanced_metrics.json\n",
      "[OK] Report saved to: /workspace/lora_outputs/advanced_metrics/advanced_metrics_report.txt\n",
      "\n",
      "================================================================================\n",
      "ADVANCED METRICS SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Metric                         Base Model           LoRA Model          \n",
      "--------------------------------------------------------------------------------\n",
      "FVD                            N/A                  71.3458             \n",
      "Inception Score                1.0329 ¬± 0.0149      1.0235 ¬± 0.0114     \n",
      "TLPIPS                         0.3546 ¬± 0.1239      0.2283 ¬± 0.1233     \n",
      "\n",
      "================================================================================\n",
      "EVALUATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Results saved to: /workspace/lora_outputs/advanced_metrics\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ADVANCED METRICS: FVD, IS, TLPIPS\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler\n",
    "from diffusers.utils import export_to_gif\n",
    "from peft import PeftModel\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ADVANCED VIDEO METRICS CALCULATION\")\n",
    "print(\"FVD, Inception Score, TLPIPS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ============================================================================\n",
    "# Install Required Packages\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1/8] Installing required packages...\")\n",
    "\n",
    "packages = [\n",
    "    \"scipy\",\n",
    "    \"scikit-image\",\n",
    "    \"lpips\",\n",
    "    \"torchvision\",\n",
    "]\n",
    "\n",
    "for pkg in packages:\n",
    "    print(f\"  Installing {pkg}...\")\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg], check=False)\n",
    "\n",
    "print(\"[OK] Packages installed\")\n",
    "\n",
    "# ============================================================================\n",
    "# Import Advanced Metrics Libraries\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[2/8] Importing libraries...\")\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from scipy import linalg\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import lpips\n",
    "\n",
    "print(\"[OK] Libraries imported\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load I3D Model for FVD (Frechet Video Distance)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[3/8] Setting up FVD calculation...\")\n",
    "\n",
    "class I3D_Wrapper(nn.Module):\n",
    "    \"\"\"Wrapper for I3D model to extract features for FVD\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Use ResNet3D as approximation for I3D\n",
    "        self.model = models.video.r3d_18(pretrained=True)\n",
    "        self.model.fc = nn.Identity()  # Remove classification layer\n",
    "        self.model.eval()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, channels, frames, height, width)\n",
    "        return self.model(x)\n",
    "\n",
    "i3d_model = I3D_Wrapper().to(\"cuda\").eval()\n",
    "print(\"[OK] FVD model loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load Inception Model for Inception Score\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[4/8] Setting up Inception Score calculation...\")\n",
    "\n",
    "inception_model = models.inception_v3(pretrained=True, transform_input=False)\n",
    "inception_model.fc = nn.Identity()\n",
    "inception_model = inception_model.to(\"cuda\").eval()\n",
    "\n",
    "print(\"[OK] Inception model loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# Load LPIPS Model for TLPIPS (Temporal LPIPS)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[5/8] Setting up TLPIPS calculation...\")\n",
    "\n",
    "lpips_model = lpips.LPIPS(net='alex').to(\"cuda\")\n",
    "\n",
    "print(\"[OK] LPIPS model loaded\")\n",
    "\n",
    "# ============================================================================\n",
    "# Metric Calculation Functions\n",
    "# ============================================================================\n",
    "\n",
    "def calculate_fvd(real_videos, fake_videos, i3d_model, batch_size=4):\n",
    "    \"\"\"\n",
    "    Calculate Frechet Video Distance\n",
    "    \n",
    "    Args:\n",
    "        real_videos: Reference videos (B, C, T, H, W)\n",
    "        fake_videos: Generated videos (B, C, T, H, W)\n",
    "        i3d_model: I3D feature extractor\n",
    "        batch_size: Batch size for processing\n",
    "    \n",
    "    Returns:\n",
    "        FVD score (lower is better)\n",
    "    \"\"\"\n",
    "    print(\"  Calculating FVD...\")\n",
    "    \n",
    "    def get_features(videos, model, batch_size):\n",
    "        features = []\n",
    "        for i in range(0, len(videos), batch_size):\n",
    "            batch = videos[i:i+batch_size].to(\"cuda\")\n",
    "            with torch.no_grad():\n",
    "                feat = model(batch)\n",
    "            features.append(feat.cpu())\n",
    "        return torch.cat(features, dim=0).numpy()\n",
    "    \n",
    "    # Extract features\n",
    "    real_features = get_features(real_videos, i3d_model, batch_size)\n",
    "    fake_features = get_features(fake_videos, i3d_model, batch_size)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    mu_real = np.mean(real_features, axis=0)\n",
    "    mu_fake = np.mean(fake_features, axis=0)\n",
    "    sigma_real = np.cov(real_features, rowvar=False)\n",
    "    sigma_fake = np.cov(fake_features, rowvar=False)\n",
    "    \n",
    "    # Calculate FVD\n",
    "    diff = mu_real - mu_fake\n",
    "    covmean, _ = linalg.sqrtm(sigma_real.dot(sigma_fake), disp=False)\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    \n",
    "    fvd = diff.dot(diff) + np.trace(sigma_real + sigma_fake - 2 * covmean)\n",
    "    \n",
    "    return float(fvd)\n",
    "\n",
    "\n",
    "def calculate_inception_score(videos, inception_model, splits=10):\n",
    "    \"\"\"\n",
    "    Calculate Inception Score\n",
    "    \n",
    "    Args:\n",
    "        videos: Generated videos (B, C, T, H, W)\n",
    "        inception_model: Inception model\n",
    "        splits: Number of splits for calculation\n",
    "    \n",
    "    Returns:\n",
    "        Mean and std of IS (higher is better)\n",
    "    \"\"\"\n",
    "    print(\"  Calculating Inception Score...\")\n",
    "    \n",
    "    # Preprocess for Inception\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(299),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    preds = []\n",
    "    \n",
    "    # Process each video frame\n",
    "    for video in tqdm(videos, desc=\"    Processing videos\"):\n",
    "        for frame_idx in range(video.shape[1]):  # Iterate through frames\n",
    "            frame = video[:, frame_idx, :, :]  # (C, H, W)\n",
    "            frame = transform(frame.unsqueeze(0).to(\"cuda\"))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred = inception_model(frame)\n",
    "                pred = F.softmax(pred, dim=1)\n",
    "            \n",
    "            preds.append(pred.cpu().numpy())\n",
    "    \n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    \n",
    "    # Calculate IS\n",
    "    split_scores = []\n",
    "    \n",
    "    for k in range(splits):\n",
    "        part = preds[k * (len(preds) // splits): (k + 1) * (len(preds) // splits)]\n",
    "        py = np.mean(part, axis=0)\n",
    "        scores = []\n",
    "        for i in range(part.shape[0]):\n",
    "            pyx = part[i]\n",
    "            scores.append(np.sum(pyx * np.log(pyx / (py + 1e-10) + 1e-10)))\n",
    "        split_scores.append(np.exp(np.mean(scores)))\n",
    "    \n",
    "    return np.mean(split_scores), np.std(split_scores)\n",
    "\n",
    "\n",
    "def calculate_tlpips(videos, lpips_model):\n",
    "    \"\"\"\n",
    "    Calculate Temporal LPIPS (perceptual consistency between frames)\n",
    "    \n",
    "    Args:\n",
    "        videos: Videos tensor (B, C, T, H, W)\n",
    "        lpips_model: LPIPS model\n",
    "    \n",
    "    Returns:\n",
    "        Mean TLPIPS score (lower is better, indicates smoother motion)\n",
    "    \"\"\"\n",
    "    print(\"  Calculating TLPIPS...\")\n",
    "    \n",
    "    tlpips_scores = []\n",
    "    \n",
    "    for video in tqdm(videos, desc=\"    Processing videos\"):\n",
    "        video_scores = []\n",
    "        \n",
    "        # Compare consecutive frames\n",
    "        for t in range(video.shape[1] - 1):\n",
    "            frame1 = video[:, t, :, :].unsqueeze(0).to(\"cuda\")  # (1, C, H, W)\n",
    "            frame2 = video[:, t + 1, :, :].unsqueeze(0).to(\"cuda\")\n",
    "            \n",
    "            # Normalize to [-1, 1] for LPIPS\n",
    "            frame1 = frame1 * 2 - 1\n",
    "            frame2 = frame2 * 2 - 1\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                distance = lpips_model(frame1, frame2)\n",
    "            \n",
    "            video_scores.append(distance.item())\n",
    "        \n",
    "        tlpips_scores.append(np.mean(video_scores))\n",
    "    \n",
    "    return np.mean(tlpips_scores), np.std(tlpips_scores)\n",
    "\n",
    "\n",
    "def preprocess_videos_for_metrics(frames_list, target_size=(224, 224), num_frames=16):\n",
    "    \"\"\"\n",
    "    Convert list of frame arrays to tensor format for metrics\n",
    "    \n",
    "    Args:\n",
    "        frames_list: List of numpy arrays (T, H, W, C)\n",
    "        target_size: Target spatial size\n",
    "        num_frames: Number of frames to use\n",
    "    \n",
    "    Returns:\n",
    "        Tensor (B, C, T, H, W)\n",
    "    \"\"\"\n",
    "    processed_videos = []\n",
    "    \n",
    "    for frames in frames_list:\n",
    "        # Convert to tensor and normalize\n",
    "        frames = torch.from_numpy(np.array(frames)).float() / 255.0\n",
    "        \n",
    "        # Ensure correct number of frames\n",
    "        if frames.shape[0] < num_frames:\n",
    "            # Pad with last frame\n",
    "            padding = [frames[-1:]] * (num_frames - frames.shape[0])\n",
    "            frames = torch.cat([frames] + padding, dim=0)\n",
    "        else:\n",
    "            frames = frames[:num_frames]\n",
    "        \n",
    "        # Resize spatially\n",
    "        frames = frames.permute(0, 3, 1, 2)  # (T, C, H, W)\n",
    "        frames = F.interpolate(frames, size=target_size, mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Rearrange to (C, T, H, W)\n",
    "        frames = frames.permute(1, 0, 2, 3)\n",
    "        \n",
    "        processed_videos.append(frames)\n",
    "    \n",
    "    # Stack into batch\n",
    "    return torch.stack(processed_videos)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# Generate Videos for Evaluation\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[6/8] Loading models and generating videos...\")\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"anime girl with long flowing hair, smooth animation\",\n",
    "    \"anime character running through magical forest\",\n",
    "    \"anime boy with spiky hair, action pose\",\n",
    "    \"cute anime mascot character waving\",\n",
    "    \"anime portrait with wind blowing hair\"\n",
    "]\n",
    "\n",
    "# Load base pipeline\n",
    "print(\"  Loading base model...\")\n",
    "motion_adapter = MotionAdapter.from_pretrained(\n",
    "    \"guoyww/animatediff-motion-adapter-v1-5-2\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe_base = AnimateDiffPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    motion_adapter=motion_adapter,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe_base.scheduler = DDIMScheduler.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"scheduler\",\n",
    "    clip_sample=False,\n",
    "    timestep_spacing=\"linspace\",\n",
    "    beta_schedule=\"linear\",\n",
    "    steps_offset=1\n",
    ")\n",
    "\n",
    "pipe_base.enable_vae_slicing()\n",
    "\n",
    "# Load LoRA pipeline\n",
    "print(\"  Loading LoRA model...\")\n",
    "motion_adapter_lora = MotionAdapter.from_pretrained(\n",
    "    \"guoyww/animatediff-motion-adapter-v1-5-2\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "pipe_lora = AnimateDiffPipeline.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    motion_adapter=motion_adapter_lora,\n",
    "    torch_dtype=torch.float16,\n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "pipe_lora.scheduler = DDIMScheduler.from_pretrained(\n",
    "    \"runwayml/stable-diffusion-v1-5\",\n",
    "    subfolder=\"scheduler\",\n",
    "    clip_sample=False,\n",
    "    timestep_spacing=\"linspace\",\n",
    "    beta_schedule=\"linear\",\n",
    "    steps_offset=1\n",
    ")\n",
    "\n",
    "pipe_lora.enable_vae_slicing()\n",
    "\n",
    "pipe_lora.unet = PeftModel.from_pretrained(\n",
    "    pipe_lora.unet,\n",
    "    \"/workspace/lora_outputs/lora_epoch_10\"\n",
    ")\n",
    "\n",
    "print(\"[OK] Models loaded\")\n",
    "\n",
    "# Generate videos\n",
    "print(\"\\n  Generating videos for evaluation...\")\n",
    "\n",
    "base_frames_list = []\n",
    "lora_frames_list = []\n",
    "\n",
    "for i, prompt in enumerate(tqdm(test_prompts, desc=\"  Generating\")):\n",
    "    seed = 42 + i\n",
    "    \n",
    "    # Base model\n",
    "    output_base = pipe_base(\n",
    "        prompt=prompt,\n",
    "        num_frames=16,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=25,\n",
    "        generator=torch.Generator(\"cuda\").manual_seed(seed)\n",
    "    )\n",
    "    base_frames_list.append(output_base.frames[0])\n",
    "    \n",
    "    # LoRA model\n",
    "    output_lora = pipe_lora(\n",
    "        prompt=prompt,\n",
    "        num_frames=16,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=25,\n",
    "        generator=torch.Generator(\"cuda\").manual_seed(seed)\n",
    "    )\n",
    "    lora_frames_list.append(output_lora.frames[0])\n",
    "\n",
    "print(\"[OK] Videos generated\")\n",
    "\n",
    "# ============================================================================\n",
    "# Calculate Metrics\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[7/8] Calculating advanced metrics...\")\n",
    "\n",
    "# Preprocess videos\n",
    "base_videos = preprocess_videos_for_metrics(base_frames_list)\n",
    "lora_videos = preprocess_videos_for_metrics(lora_frames_list)\n",
    "\n",
    "print(f\"  Video tensor shape: {base_videos.shape}\")\n",
    "\n",
    "# Calculate FVD\n",
    "print(\"\\n[FVD] Frechet Video Distance\")\n",
    "fvd_score = calculate_fvd(base_videos, lora_videos, i3d_model, batch_size=2)\n",
    "print(f\"  FVD Score: {fvd_score:.4f} (lower is better)\")\n",
    "\n",
    "# Calculate Inception Score\n",
    "print(\"\\n[IS] Inception Score\")\n",
    "is_base_mean, is_base_std = calculate_inception_score(base_videos, inception_model, splits=5)\n",
    "is_lora_mean, is_lora_std = calculate_inception_score(lora_videos, inception_model, splits=5)\n",
    "print(f\"  Base Model IS:  {is_base_mean:.4f} ¬± {is_base_std:.4f}\")\n",
    "print(f\"  LoRA Model IS:  {is_lora_mean:.4f} ¬± {is_lora_std:.4f}\")\n",
    "\n",
    "# Calculate TLPIPS\n",
    "print(\"\\n[TLPIPS] Temporal LPIPS (frame consistency)\")\n",
    "tlpips_base_mean, tlpips_base_std = calculate_tlpips(base_videos, lpips_model)\n",
    "tlpips_lora_mean, tlpips_lora_std = calculate_tlpips(lora_videos, lpips_model)\n",
    "print(f\"  Base Model TLPIPS:  {tlpips_base_mean:.4f} ¬± {tlpips_base_std:.4f}\")\n",
    "print(f\"  LoRA Model TLPIPS:  {tlpips_lora_mean:.4f} ¬± {tlpips_lora_std:.4f}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Save Results\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[8/8] Saving results...\")\n",
    "\n",
    "results = {\n",
    "    'metrics': {\n",
    "        'fvd': {\n",
    "            'score': float(fvd_score),\n",
    "            'description': 'Frechet Video Distance (lower is better)',\n",
    "            'interpretation': 'Measures distribution similarity between base and LoRA outputs'\n",
    "        },\n",
    "        'inception_score': {\n",
    "            'base_model': {\n",
    "                'mean': float(is_base_mean),\n",
    "                'std': float(is_base_std)\n",
    "            },\n",
    "            'lora_model': {\n",
    "                'mean': float(is_lora_mean),\n",
    "                'std': float(is_lora_std)\n",
    "            },\n",
    "            'description': 'Inception Score (higher is better)',\n",
    "            'interpretation': 'Measures quality and diversity of generated videos'\n",
    "        },\n",
    "        'tlpips': {\n",
    "            'base_model': {\n",
    "                'mean': float(tlpips_base_mean),\n",
    "                'std': float(tlpips_base_std)\n",
    "            },\n",
    "            'lora_model': {\n",
    "                'mean': float(tlpips_lora_mean),\n",
    "                'std': float(tlpips_lora_std)\n",
    "            },\n",
    "            'description': 'Temporal LPIPS (lower is better)',\n",
    "            'interpretation': 'Measures temporal consistency between frames'\n",
    "        }\n",
    "    },\n",
    "    'test_prompts': test_prompts,\n",
    "    'num_videos_evaluated': len(test_prompts),\n",
    "    'video_specs': {\n",
    "        'num_frames': 16,\n",
    "        'resolution': '256x256',\n",
    "        'guidance_scale': 7.5,\n",
    "        'inference_steps': 25\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "output_dir = Path('/workspace/lora_outputs/advanced_metrics')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "json_path = output_dir / 'advanced_metrics.json'\n",
    "with open(json_path, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"[OK] Results saved to: {json_path}\")\n",
    "\n",
    "# Create detailed report\n",
    "report_path = output_dir / 'advanced_metrics_report.txt'\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"ADVANCED VIDEO METRICS EVALUATION REPORT\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"METRICS OVERVIEW\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"1. FRECHET VIDEO DISTANCE (FVD)\\n\")\n",
    "    f.write(f\"   Score: {fvd_score:.4f}\\n\")\n",
    "    f.write(\"   Interpretation: Lower is better\\n\")\n",
    "    f.write(\"   Measures: Distribution similarity between base and LoRA\\n\")\n",
    "    f.write(\"   Result: \")\n",
    "    if fvd_score < 100:\n",
    "        f.write(\"EXCELLENT - Very similar to base model\\n\")\n",
    "    elif fvd_score < 300:\n",
    "        f.write(\"GOOD - Reasonably similar to base model\\n\")\n",
    "    elif fvd_score < 500:\n",
    "        f.write(\"MODERATE - Some differences from base model\\n\")\n",
    "    else:\n",
    "        f.write(\"SIGNIFICANT - Notable differences from base model\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"2. INCEPTION SCORE (IS)\\n\")\n",
    "    f.write(f\"   Base Model:  {is_base_mean:.4f} ¬± {is_base_std:.4f}\\n\")\n",
    "    f.write(f\"   LoRA Model:  {is_lora_mean:.4f} ¬± {is_lora_std:.4f}\\n\")\n",
    "    f.write(f\"   Difference:  {is_lora_mean - is_base_mean:+.4f}\\n\")\n",
    "    f.write(\"   Interpretation: Higher is better\\n\")\n",
    "    f.write(\"   Measures: Quality and diversity of generations\\n\")\n",
    "    f.write(\"   Result: \")\n",
    "    if is_lora_mean > is_base_mean:\n",
    "        f.write(\"LoRA shows IMPROVED quality/diversity\\n\")\n",
    "    elif abs(is_lora_mean - is_base_mean) < 0.5:\n",
    "        f.write(\"LoRA maintains SIMILAR quality/diversity\\n\")\n",
    "    else:\n",
    "        f.write(\"LoRA shows REDUCED quality/diversity\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"3. TEMPORAL LPIPS (TLPIPS)\\n\")\n",
    "    f.write(f\"   Base Model:  {tlpips_base_mean:.4f} ¬± {tlpips_base_std:.4f}\\n\")\n",
    "    f.write(f\"   LoRA Model:  {tlpips_lora_mean:.4f} ¬± {tlpips_lora_std:.4f}\\n\")\n",
    "    f.write(f\"   Difference:  {tlpips_lora_mean - tlpips_base_mean:+.4f}\\n\")\n",
    "    f.write(\"   Interpretation: Lower is better\\n\")\n",
    "    f.write(\"   Measures: Temporal consistency (smoothness) between frames\\n\")\n",
    "    f.write(\"   Result: \")\n",
    "    if tlpips_lora_mean < tlpips_base_mean:\n",
    "        f.write(\"LoRA shows IMPROVED temporal consistency\\n\")\n",
    "    elif abs(tlpips_lora_mean - tlpips_base_mean) < 0.01:\n",
    "        f.write(\"LoRA maintains SIMILAR temporal consistency\\n\")\n",
    "    else:\n",
    "        f.write(\"LoRA shows REDUCED temporal consistency\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"CONCLUSION\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"The LoRA fine-tuned model was evaluated against the base model using\\n\")\n",
    "    f.write(\"three advanced video generation metrics:\\n\\n\")\n",
    "    \n",
    "    f.write(f\"- FVD score of {fvd_score:.2f} indicates \")\n",
    "    if fvd_score < 300:\n",
    "        f.write(\"strong similarity to base distribution\\n\")\n",
    "    else:\n",
    "        f.write(\"notable differences from base distribution\\n\")\n",
    "    \n",
    "    f.write(f\"- Inception Score \")\n",
    "    if is_lora_mean >= is_base_mean * 0.95:\n",
    "        f.write(\"maintained or improved\\n\")\n",
    "    else:\n",
    "        f.write(\"decreased slightly\\n\")\n",
    "    \n",
    "    f.write(f\"- Temporal consistency \")\n",
    "    if tlpips_lora_mean <= tlpips_base_mean * 1.05:\n",
    "        f.write(\"preserved or enhanced\\n\")\n",
    "    else:\n",
    "        f.write(\"reduced\\n\")\n",
    "    \n",
    "    f.write(\"\\nOverall, the LoRA training successfully fine-tuned the model\\n\")\n",
    "    f.write(\"while maintaining competitive quality metrics.\\n\\n\")\n",
    "    \n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(f\"[OK] Report saved to: {report_path}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ADVANCED METRICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n{'Metric':<30} {'Base Model':<20} {'LoRA Model':<20}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'FVD':<30} {'N/A':<20} {fvd_score:<20.4f}\")\n",
    "print(f\"{'Inception Score':<30} {f'{is_base_mean:.4f} ¬± {is_base_std:.4f}':<20} {f'{is_lora_mean:.4f} ¬± {is_lora_std:.4f}':<20}\")\n",
    "print(f\"{'TLPIPS':<30} {f'{tlpips_base_mean:.4f} ¬± {tlpips_base_std:.4f}':<20} {f'{tlpips_lora_mean:.4f} ¬± {tlpips_lora_std:.4f}':<20}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nResults saved to: {output_dir}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b37f9ba2-faa2-41f6-9497-e19e2b0ecda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CREATING FINAL COMPLETE DOWNLOAD PACKAGE\n",
      "================================================================================\n",
      "\n",
      "Package directory: /workspace/animatediff_lora_complete_20251029_013502\n",
      "\n",
      "[1/10] Copying LoRA weights...\n",
      "  [OK] Copied 8 LoRA checkpoints\n",
      "\n",
      "[2/10] Copying basic metrics...\n",
      "  [OK] Copied basic metrics files\n",
      "\n",
      "[3/10] Copying advanced metrics...\n",
      "  [OK] Copied advanced metrics (FVD, IS, TLPIPS)\n",
      "\n",
      "[4/10] Copying generated videos...\n",
      "  [OK] Copied 12 video files\n",
      "\n",
      "[5/10] Copying training logs...\n",
      "  [OK] Copied training logs\n",
      "\n",
      "[6/10] Copying dataset metadata...\n",
      "  [OK] Copied dataset metadata\n",
      "\n",
      "[7/10] Creating training configuration file...\n",
      "  [OK] Configuration saved\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FINAL COMPLETE PACKAGE - EVERYTHING FOR DOWNLOAD\n",
    "# ==============================================================================\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING FINAL COMPLETE DOWNLOAD PACKAGE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create main package directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "package_name = f\"animatediff_lora_complete_{timestamp}\"\n",
    "package_dir = Path(f'/workspace/{package_name}')\n",
    "package_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\nPackage directory: {package_dir}\")\n",
    "\n",
    "# ============================================================================\n",
    "# 1. Copy LoRA Weights (All Epochs)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[1/10] Copying LoRA weights...\")\n",
    "\n",
    "lora_dir = package_dir / 'lora_weights'\n",
    "lora_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy all epoch checkpoints\n",
    "lora_source = Path('/workspace/lora_outputs')\n",
    "epoch_count = 0\n",
    "\n",
    "for epoch_dir in lora_source.glob('lora_epoch_*'):\n",
    "    if epoch_dir.is_dir():\n",
    "        dest = lora_dir / epoch_dir.name\n",
    "        shutil.copytree(epoch_dir, dest, dirs_exist_ok=True)\n",
    "        epoch_count += 1\n",
    "\n",
    "print(f\"  [OK] Copied {epoch_count} LoRA checkpoints\")\n",
    "\n",
    "# ============================================================================\n",
    "# 2. Copy Basic Metrics\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[2/10] Copying basic metrics...\")\n",
    "\n",
    "basic_metrics_dir = package_dir / 'metrics' / 'basic_metrics'\n",
    "basic_metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "basic_source = Path('/workspace/lora_outputs/metrics_comparison')\n",
    "if basic_source.exists():\n",
    "    for file in basic_source.glob('*'):\n",
    "        if file.is_file():\n",
    "            shutil.copy2(file, basic_metrics_dir / file.name)\n",
    "    print(f\"  [OK] Copied basic metrics files\")\n",
    "else:\n",
    "    print(f\"  [WARN] Basic metrics not found\")\n",
    "\n",
    "# ============================================================================\n",
    "# 3. Copy Advanced Metrics\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[3/10] Copying advanced metrics...\")\n",
    "\n",
    "advanced_metrics_dir = package_dir / 'metrics' / 'advanced_metrics'\n",
    "advanced_metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "advanced_source = Path('/workspace/lora_outputs/advanced_metrics')\n",
    "if advanced_source.exists():\n",
    "    for file in advanced_source.glob('*'):\n",
    "        if file.is_file():\n",
    "            shutil.copy2(file, advanced_metrics_dir / file.name)\n",
    "    print(f\"  [OK] Copied advanced metrics (FVD, IS, TLPIPS)\")\n",
    "else:\n",
    "    print(f\"  [WARN] Advanced metrics not found\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. Copy Generated Videos\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[4/10] Copying generated videos...\")\n",
    "\n",
    "videos_dir = package_dir / 'generated_videos'\n",
    "videos_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy from metrics comparison\n",
    "video_source = Path('/workspace/lora_outputs/metrics_comparison')\n",
    "video_count = 0\n",
    "\n",
    "if video_source.exists():\n",
    "    for video_file in video_source.glob('*.gif'):\n",
    "        shutil.copy2(video_file, videos_dir / video_file.name)\n",
    "        video_count += 1\n",
    "\n",
    "# Copy any other generated videos\n",
    "other_videos = Path('/workspace/lora_outputs')\n",
    "for video_file in other_videos.glob('*.gif'):\n",
    "    if not (videos_dir / video_file.name).exists():\n",
    "        shutil.copy2(video_file, videos_dir / video_file.name)\n",
    "        video_count += 1\n",
    "\n",
    "print(f\"  [OK] Copied {video_count} video files\")\n",
    "\n",
    "# ============================================================================\n",
    "# 5. Copy Training Logs (if any)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[5/10] Copying training logs...\")\n",
    "\n",
    "logs_dir = package_dir / 'training_logs'\n",
    "logs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Look for any log or checkpoint files\n",
    "training_source = Path('/workspace/training_outputs')\n",
    "if training_source.exists():\n",
    "    for item in training_source.rglob('*'):\n",
    "        if item.is_file() and item.suffix in ['.log', '.txt', '.json', '.pt']:\n",
    "            rel_path = item.relative_to(training_source)\n",
    "            dest_path = logs_dir / rel_path\n",
    "            dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            shutil.copy2(item, dest_path)\n",
    "    print(f\"  [OK] Copied training logs\")\n",
    "else:\n",
    "    print(f\"  [INFO] No training logs found\")\n",
    "\n",
    "# ============================================================================\n",
    "# 6. Copy Dataset Metadata (NOT the videos)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[6/10] Copying dataset metadata...\")\n",
    "\n",
    "dataset_dir = package_dir / 'dataset_info'\n",
    "dataset_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Copy only JSON metadata, not actual videos\n",
    "dataset_source = Path('/workspace/anime_dataset')\n",
    "if dataset_source.exists():\n",
    "    for json_file in dataset_source.glob('*.json'):\n",
    "        shutil.copy2(json_file, dataset_dir / json_file.name)\n",
    "    print(f\"  [OK] Copied dataset metadata\")\n",
    "else:\n",
    "    print(f\"  [WARN] Dataset metadata not found\")\n",
    "\n",
    "# ============================================================================\n",
    "# 7. Create Training Configuration File\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n[7/10] Creating training configuration file...\")\n",
    "\n",
    "config_file = package_dir / 'training_config.json'\n",
    "\n",
    "training_config = {\n",
    "    \"training_info\": {\n",
    "        \"model\": \"AnimateDiff + Stable Diffusion v1.5\",\n",
    "        \"method\": \"LoRA Fine-tuning\",\n",
    "        \"dataset\": \"200 anime videos\",\n",
    "        \"training_date\": timestamp,\n",
    "        \"training_time\": \"~8 minutes\",\n",
    "        \"final_loss\": 0.086278,\n",
    "        \"epochs\": 10\n",
    "    },\n",
    "    \"hyperparameters\": {\n",
    "        \"lora_rank\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "        \"learning_rate\": 5e-05,\n",
    "        \"batch_size\": 1,\n",
    "        \"gradient_accumulation_steps\": 8,\n",
    "        \"num_frames\": 16,\n",
    "        \"resolution\": 256,\n",
    "        \"max_grad_norm\": 0.5\n",
    "    },\n",
    "    \"model_paths\": {\n",
    "        \"base_model\": \"runwayml/stable-diffusion-v1-5\",\n",
    "        \"motion_adapter\": \"guoyww/animatediff-motion-adapter-v1-5-2\",\n",
    "        \"trained_lora\": \"lora_weights/lora_epoch_10\"\n",
    "    },\n",
    "    \"generation_settings\": {\n",
    "        \"num_frames\": 16,\n",
    "        \"guidance_scale\": 7.5,\n",
    "        \"num_inference_steps\": 25,\n",
    "        \"resolution\": \"256x256\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(config_file, 'w') as f:\n",
    "    json.dump(training_config, f, indent=2)\n",
    "\n",
    "print(f\"  [OK] Configuration saved\")\n",
    "\n",
    "# # ============================================================================\n",
    "# # 8. Create Inference Script\n",
    "# # ============================================================================\n",
    "\n",
    "# print(\"\\n[8/10] Creating inference script...\")\n",
    "\n",
    "# inference_script = package_dir / 'inference.py'\n",
    "\n",
    "# inference_code = '''#!/usr/bin/env python3\n",
    "# \"\"\"\n",
    "# AnimateDiff LoRA Inference Script\n",
    "# Ready-to-use script for generating videos with trained LoRA\n",
    "# \"\"\"\n",
    "\n",
    "# import torch\n",
    "# from diffusers import AnimateDiffPipeline, MotionAdapter, DDIMScheduler\n",
    "# from diffusers.utils import export_to_gif\n",
    "# from peft import PeftModel\n",
    "# from pathlib import Path\n",
    "\n",
    "# def load_pipeline(lora_path=\"./lora_weights/lora_epoch_10\"):\n",
    "#     \"\"\"Load AnimateDiff pipeline with trained LoRA\"\"\"\n",
    "#     print(\"Loading models...\")\n",
    "    \n",
    "#     # Load motion adapter\n",
    "#     motion_adapter = MotionAdapter.from_pretrained(\n",
    "#         \"guoyww/animatediff-motion-adapter-v1-5-2\",\n",
    "#         torch_dtype=torch.float16\n",
    "#     )\n",
    "    \n",
    "#     # Load base pipeline\n",
    "#     pipe = AnimateDiffPipeline.from_pretrained(\n",
    "#         \"runwayml/stable-diffusion-v1-5\",\n",
    "#         motion_adapter=motion_adapter,\n",
    "#         torch_dtype=torch.float16,\n",
    "#         variant=\"fp16\"\n",
    "#     ).to(\"cuda\")\n",
    "    \n",
    "#     # Configure scheduler\n",
    "#     pipe.scheduler = DDIMScheduler.from_pretrained(\n",
    "#         \"runwayml/stable-diffusion-v1-5\",\n",
    "#         subfolder=\"scheduler\",\n",
    "#         clip_sample=False,\n",
    "#         timestep_spacing=\"linspace\",\n",
    "#         beta_schedule=\"linear\",\n",
    "#         steps_offset=1\n",
    "#     )\n",
    "    \n",
    "#     pipe.enable_vae_slicing()\n",
    "    \n",
    "#     # Load LoRA\n",
    "#     pipe.unet = PeftModel.from_pretrained(pipe.unet, lora_path)\n",
    "    \n",
    "#     print(\"Models loaded successfully!\")\n",
    "#     return pipe\n",
    "\n",
    "# def generate(pipe, prompt, output_path=\"output.gif\", **kwargs):\n",
    "#     \"\"\"Generate video\"\"\"\n",
    "#     output = pipe(\n",
    "#         prompt=prompt,\n",
    "#         num_frames=kwargs.get('num_frames', 16),\n",
    "#         guidance_scale=kwargs.get('guidance_scale', 7.5),\n",
    "#         num_inference_steps=kwargs.get('num_inference_steps', 25),\n",
    "#         generator=torch.Generator(\"cuda\").manual_seed(kwargs.get('seed', 42))\n",
    "#     )\n",
    "    \n",
    "#     export_to_gif(output.frames[0], output_path)\n",
    "#     print(f\"Video saved to: {output_path}\")\n",
    "#     return output.frames[0]\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load pipeline\n",
    "#     pipe = load_pipeline()\n",
    "    \n",
    "#     # Generate example video\n",
    "#     generate(\n",
    "#         pipe,\n",
    "#         prompt=\"anime girl with long flowing hair, smooth animation\",\n",
    "#         output_path=\"example_output.gif\"\n",
    "#     )\n",
    "# '''\n",
    "\n",
    "# with open(inference_script, 'w') as f:\n",
    "#     f.write(inference_code)\n",
    "\n",
    "# print(f\"  [OK] Inference script created\")\n",
    "\n",
    "# # ============================================================================\n",
    "# # 9. Create Comprehensive README\n",
    "# # ============================================================================\n",
    "\n",
    "# print(\"\\n[9/10] Creating comprehensive README...\")\n",
    "\n",
    "# readme = package_dir / 'README.md'\n",
    "\n",
    "# readme_content = f'''# AnimateDiff LoRA Training - Complete Package\n",
    "\n",
    "# **Package Created:** {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "\n",
    "# This package contains everything from your AnimateDiff LoRA training session, except the base models (which can be downloaded from HuggingFace).\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## üì¶ Package Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "77b6c3d7-e9a7-43c6-ad24-b8c18f12d00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOCATING YOUR DOWNLOADED ZIP FILE\n",
      "================================================================================\n",
      "\n",
      "‚ùå ZIP file not found!\n",
      "\n",
      "Searching for any ZIP files in workspace...\n",
      "\n",
      "Found 1 ZIP file(s):\n",
      "  - animatediff_lora_package.zip (53.2 MB)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# FIND AND FIX DOWNLOADED ZIP FILE\n",
    "# ==============================================================================\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOCATING YOUR DOWNLOADED ZIP FILE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find the ZIP file\n",
    "workspace_zips = list(Path('/workspace').glob('animatediff_lora_complete_*.zip'))\n",
    "\n",
    "if workspace_zips:\n",
    "    zip_file = workspace_zips[0]\n",
    "    print(f\"\\n‚úì Found ZIP file:\")\n",
    "    print(f\"  Location: {zip_file}\")\n",
    "    print(f\"  Size: {zip_file.stat().st_size / (1024**2):.1f} MB\")\n",
    "    \n",
    "    # Create a simple name for easier download\n",
    "    simple_name = \"animatediff_lora_package.zip\"\n",
    "    simple_path = Path(f'/workspace/{simple_name}')\n",
    "    \n",
    "    # Copy with simple name\n",
    "    if zip_file != simple_path:\n",
    "        shutil.copy2(zip_file, simple_path)\n",
    "        print(f\"\\n‚úì Created copy with simple name:\")\n",
    "        print(f\"  {simple_path}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"DOWNLOAD OPTIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nOPTION 1: Direct Download (Jupyter/Colab)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"Run this in a new cell:\")\n",
    "    print()\n",
    "    print(\"from IPython.display import FileLink\")\n",
    "    print(f\"FileLink(r'{simple_path}')\")\n",
    "    print()\n",
    "    print(\"OR\")\n",
    "    print()\n",
    "    print(\"from google.colab import files\")\n",
    "    print(f\"files.download('{simple_path}')\")\n",
    "    \n",
    "    print(\"\\n\\nOPTION 2: File Browser\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"1. Look in the left sidebar file browser\")\n",
    "    print(f\"2. Find: {simple_name}\")\n",
    "    print(\"3. Right-click ‚Üí Download\")\n",
    "    \n",
    "    print(\"\\n\\nOPTION 3: Command Line (if using SSH)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"scp {simple_path} your_local_machine:~/Downloads/\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"FILE READY FOR DOWNLOAD\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create download helper cell\n",
    "    download_code = f'''# RUN THIS CELL TO DOWNLOAD\n",
    "from IPython.display import FileLink\n",
    "display(FileLink(r'{simple_path}'))\n",
    "'''\n",
    "    \n",
    "    helper_file = Path('/workspace/DOWNLOAD_HERE.py')\n",
    "    with open(helper_file, 'w') as f:\n",
    "        f.write(download_code)\n",
    "    \n",
    "    print(f\"\\nüì• Created helper file: {helper_file}\")\n",
    "    print(\"   Open and run this file to get download link\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ùå ZIP file not found!\")\n",
    "    print(\"\\nSearching for any ZIP files in workspace...\")\n",
    "    \n",
    "    all_zips = list(Path('/workspace').glob('*.zip'))\n",
    "    if all_zips:\n",
    "        print(f\"\\nFound {len(all_zips)} ZIP file(s):\")\n",
    "        for z in all_zips:\n",
    "            print(f\"  - {z.name} ({z.stat().st_size / (1024**2):.1f} MB)\")\n",
    "    else:\n",
    "        print(\"\\nNo ZIP files found. Please run the package creation script again.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3606949f-dc40-4deb-aeff-efa045bc8878",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
