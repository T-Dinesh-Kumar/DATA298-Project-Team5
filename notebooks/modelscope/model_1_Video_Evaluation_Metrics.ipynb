{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Video Model Evaluation Metrics\n",
        "## Comprehensive Quality Assessment for Fine-tuned Text-to-Video Model\n",
        "\n",
        "**This notebook evaluates your generated videos using 7 metrics:**\n",
        "\n",
        "1. Temporal Consistency - Frame-to-frame coherence\n",
        "2. Sharpness - Image clarity and detail\n",
        "3. Contrast - Tonal range\n",
        "4. Brightness - Proper exposure\n",
        "5. Motion Smoothness - Fluid movement\n",
        "6. CLIP Score - Text-video alignment\n",
        "7. Inception Score - Diversity and realism\n",
        "\n",
        "---\n",
        "\n",
        "**Expected Results for 10K Model:**\n",
        "- Temporal Consistency: 0.75 - 0.85\n",
        "- Sharpness: 150 - 300\n",
        "- Motion Smoothness: 0.55 - 0.70\n",
        "- CLIP Score: 0.27 - 0.33\n",
        "- Inception Score: 2.5 - 3.5"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install Required Libraries"
      ],
      "metadata": {
        "id": "install_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b3d7277-9dd4-42cf-e7fd-d202cb401c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing evaluation libraries...\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "All libraries installed successfully\n"
          ]
        }
      ],
      "source": [
        "# Install evaluation libraries\n",
        "print(\"Installing evaluation libraries...\")\n",
        "!pip install -q lpips pytorch-fid torchmetrics scikit-image scikit-learn\n",
        "!pip install -q git+https://github.com/openai/CLIP.git\n",
        "!pip install -q opencv-python imageio matplotlib seaborn\n",
        "\n",
        "print(\"All libraries installed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install torch-fidelity for InceptionScore\n",
        "print(\"Installing torch-fidelity...\")\n",
        "!pip install -q torch-fidelity\n",
        "print(\"torch-fidelity installed successfully\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgPqOlpnwtZ6",
        "outputId": "fdc87fec-9455-46d1-abbc-7c039f953986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing torch-fidelity...\n",
            "torch-fidelity installed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Import Libraries"
      ],
      "metadata": {
        "id": "import_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import imageio\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Evaluation libraries\n",
        "import lpips\n",
        "from torchmetrics.image.fid import FrechetInceptionDistance\n",
        "from torchmetrics.image.inception import InceptionScore\n",
        "\n",
        "# CLIP for text-video alignment\n",
        "try:\n",
        "    import clip\n",
        "    CLIP_AVAILABLE = True\n",
        "    print(\"CLIP loaded successfully\")\n",
        "except:\n",
        "    CLIP_AVAILABLE = False\n",
        "    print(\"Warning: CLIP not available, text-video alignment score will be skipped\")\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"All libraries imported successfully\")"
      ],
      "metadata": {
        "id": "imports",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716654c7-841a-4cff-c4e2-50fe8e2d1dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP loaded successfully\n",
            "Using device: cuda\n",
            "All libraries imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Initialize Evaluation Models"
      ],
      "metadata": {
        "id": "init_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize evaluation models\n",
        "print(\"Initializing evaluation models...\")\n",
        "\n",
        "# LPIPS for perceptual similarity\n",
        "lpips_model = lpips.LPIPS(net='alex').to(device).eval()\n",
        "print(\"LPIPS model loaded\")\n",
        "\n",
        "# CLIP for text-video alignment\n",
        "if CLIP_AVAILABLE:\n",
        "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "    print(\"CLIP model loaded\")\n",
        "\n",
        "# Inception Score calculator\n",
        "inception_score_calculator = InceptionScore(normalize=True).to(device)\n",
        "print(\"Inception Score calculator loaded\")\n",
        "\n",
        "print(\"All evaluation models initialized successfully\")"
      ],
      "metadata": {
        "id": "init_models",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68391278-fec8-4d50-e85f-7edb07deeb40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing evaluation models...\n",
            "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
            "Loading model from: /usr/local/lib/python3.12/dist-packages/lpips/weights/v0.1/alex.pth\n",
            "LPIPS model loaded\n",
            "CLIP model loaded\n",
            "Inception Score calculator loaded\n",
            "All evaluation models initialized successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Define Evaluation Functions"
      ],
      "metadata": {
        "id": "functions_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Metric 1: Temporal Consistency\n",
        "def calculate_temporal_consistency(video_frames):\n",
        "    \"\"\"\n",
        "    Measures frame-to-frame consistency using LPIPS perceptual similarity.\n",
        "\n",
        "    Args:\n",
        "        video_frames: Tensor of shape [T, C, H, W] normalized to [0, 1]\n",
        "\n",
        "    Returns:\n",
        "        float: Temporal consistency score (higher is better)\n",
        "               Range: [0, 1], Good: >0.75\n",
        "    \"\"\"\n",
        "    if len(video_frames) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    consistencies = []\n",
        "\n",
        "    for i in range(len(video_frames) - 1):\n",
        "        frame1 = video_frames[i].unsqueeze(0).to(device)\n",
        "        frame2 = video_frames[i + 1].unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # LPIPS gives distance (lower = more similar)\n",
        "            distance = lpips_model(frame1, frame2).item()\n",
        "            # Convert to similarity score\n",
        "            similarity = max(0, 1.0 - distance)\n",
        "            consistencies.append(similarity)\n",
        "\n",
        "    avg_consistency = np.mean(consistencies)\n",
        "    return avg_consistency\n",
        "\n",
        "\n",
        "# Metric 2: Frame Quality (Sharpness, Contrast, Brightness)\n",
        "def calculate_frame_quality(video_frames):\n",
        "    \"\"\"\n",
        "    Measures visual quality of individual frames.\n",
        "\n",
        "    Args:\n",
        "        video_frames: Tensor of shape [T, C, H, W] normalized to [0, 1]\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with sharpness, contrast, and brightness scores\n",
        "              Sharpness - Good: >150\n",
        "              Contrast - Good: >50\n",
        "              Brightness - Good: 100-150\n",
        "    \"\"\"\n",
        "    sharpness_scores = []\n",
        "    contrast_scores = []\n",
        "    brightness_scores = []\n",
        "\n",
        "    for frame in video_frames:\n",
        "        # Convert to numpy\n",
        "        frame_np = frame.permute(1, 2, 0).cpu().numpy()\n",
        "        frame_np = (frame_np * 255).clip(0, 255).astype(np.uint8)\n",
        "\n",
        "        # Sharpness (Laplacian variance)\n",
        "        gray = cv2.cvtColor(frame_np, cv2.COLOR_RGB2GRAY)\n",
        "        laplacian = cv2.Laplacian(gray, cv2.CV_64F)\n",
        "        sharpness = laplacian.var()\n",
        "        sharpness_scores.append(sharpness)\n",
        "\n",
        "        # Contrast (standard deviation)\n",
        "        contrast = frame_np.std()\n",
        "        contrast_scores.append(contrast)\n",
        "\n",
        "        # Brightness (mean)\n",
        "        brightness = frame_np.mean()\n",
        "        brightness_scores.append(brightness)\n",
        "\n",
        "    return {\n",
        "        'sharpness': np.mean(sharpness_scores),\n",
        "        'contrast': np.mean(contrast_scores),\n",
        "        'brightness': np.mean(brightness_scores)\n",
        "    }\n",
        "\n",
        "\n",
        "# Metric 3: Motion Smoothness\n",
        "def calculate_motion_smoothness(video_frames):\n",
        "    \"\"\"\n",
        "    Measures smoothness of motion using optical flow.\n",
        "\n",
        "    Args:\n",
        "        video_frames: Tensor of shape [T, C, H, W] normalized to [0, 1]\n",
        "\n",
        "    Returns:\n",
        "        float: Motion smoothness score (higher is better)\n",
        "               Range: [0, 1], Good: >0.55\n",
        "    \"\"\"\n",
        "    if len(video_frames) < 2:\n",
        "        return 0.0\n",
        "\n",
        "    flow_magnitudes = []\n",
        "\n",
        "    for i in range(len(video_frames) - 1):\n",
        "        # Convert to grayscale numpy\n",
        "        frame1_np = (video_frames[i].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
        "        frame2_np = (video_frames[i + 1].permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)\n",
        "\n",
        "        gray1 = cv2.cvtColor(frame1_np, cv2.COLOR_RGB2GRAY)\n",
        "        gray2 = cv2.cvtColor(frame2_np, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # Calculate optical flow\n",
        "        flow = cv2.calcOpticalFlowFarneback(\n",
        "            gray1, gray2, None, 0.5, 3, 15, 3, 5, 1.2, 0\n",
        "        )\n",
        "\n",
        "        # Flow magnitude\n",
        "        magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)\n",
        "        flow_magnitudes.append(np.mean(magnitude))\n",
        "\n",
        "    # Smoothness = inverse of variance in flow\n",
        "    if len(flow_magnitudes) > 1:\n",
        "        smoothness = 1.0 / (1.0 + np.var(flow_magnitudes))\n",
        "    else:\n",
        "        smoothness = 1.0\n",
        "\n",
        "    return smoothness\n",
        "\n",
        "\n",
        "# Metric 4: CLIP Score\n",
        "def calculate_clip_score(video_frames, text_prompt):\n",
        "    \"\"\"\n",
        "    Measures how well video matches text prompt using CLIP.\n",
        "\n",
        "    Args:\n",
        "        video_frames: Tensor of shape [T, C, H, W] normalized to [0, 1]\n",
        "        text_prompt: str, text description of video\n",
        "\n",
        "    Returns:\n",
        "        float: CLIP score (higher is better)\n",
        "               Range: [0, 1], Good: >0.25\n",
        "    \"\"\"\n",
        "    if not CLIP_AVAILABLE:\n",
        "        return None\n",
        "\n",
        "    # Encode text\n",
        "    text_tokens = clip.tokenize([text_prompt]).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_model.encode_text(text_tokens)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Encode each frame\n",
        "    clip_scores = []\n",
        "    for frame in video_frames:\n",
        "        # Convert to PIL\n",
        "        frame_np = (frame.permute(1, 2, 0).cpu().numpy() * 255).clip(0, 255).astype(np.uint8)\n",
        "        frame_pil = Image.fromarray(frame_np)\n",
        "\n",
        "        # Preprocess and encode\n",
        "        frame_input = clip_preprocess(frame_pil).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = clip_model.encode_image(frame_input)\n",
        "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "            # Cosine similarity\n",
        "            similarity = (image_features @ text_features.T).item()\n",
        "            clip_scores.append(similarity)\n",
        "\n",
        "    return np.mean(clip_scores)\n",
        "\n",
        "\n",
        "# Metric 5: Inception Score\n",
        "def calculate_inception_score(video_frames):\n",
        "    \"\"\"\n",
        "    Measures diversity and quality using Inception network.\n",
        "\n",
        "    Args:\n",
        "        video_frames: Tensor of shape [T, C, H, W] normalized to [0, 1]\n",
        "\n",
        "    Returns:\n",
        "        tuple: (mean, std) of inception score\n",
        "               Good: mean >2.5\n",
        "    \"\"\"\n",
        "    inception_score_calculator.reset()\n",
        "\n",
        "    for frame in video_frames:\n",
        "        # Convert to uint8 [0, 255]\n",
        "        frame_uint8 = (frame * 255).clip(0, 255).byte()\n",
        "        inception_score_calculator.update(frame_uint8.unsqueeze(0).to(device))\n",
        "\n",
        "    is_mean, is_std = inception_score_calculator.compute()\n",
        "    return is_mean.item(), is_std.item()\n",
        "\n",
        "\n",
        "print(\"All evaluation functions defined successfully\")"
      ],
      "metadata": {
        "id": "functions",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d2fbe5c-9733-4e19-9014-6477ce7f6e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All evaluation functions defined successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Comprehensive Evaluation Function"
      ],
      "metadata": {
        "id": "comprehensive_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_video_comprehensive(video_frames, text_prompt=None, verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluate a video with all metrics.\n",
        "\n",
        "    Args:\n",
        "        video_frames: Tensor [T, C, H, W] normalized to [0, 1]\n",
        "        text_prompt: Optional text prompt for CLIP score\n",
        "        verbose: Print progress\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with all metrics\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(\"Evaluating video...\")\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Temporal consistency\n",
        "    if verbose:\n",
        "        print(\"  Calculating temporal consistency...\")\n",
        "    results['temporal_consistency'] = calculate_temporal_consistency(video_frames)\n",
        "\n",
        "    # Frame quality\n",
        "    if verbose:\n",
        "        print(\"  Calculating frame quality...\")\n",
        "    quality = calculate_frame_quality(video_frames)\n",
        "    results['sharpness'] = quality['sharpness']\n",
        "    results['contrast'] = quality['contrast']\n",
        "    results['brightness'] = quality['brightness']\n",
        "\n",
        "    # Motion smoothness\n",
        "    if verbose:\n",
        "        print(\"  Calculating motion smoothness...\")\n",
        "    results['motion_smoothness'] = calculate_motion_smoothness(video_frames)\n",
        "\n",
        "    # CLIP score\n",
        "    if text_prompt and CLIP_AVAILABLE:\n",
        "        if verbose:\n",
        "            print(\"  Calculating CLIP score...\")\n",
        "        results['clip_score'] = calculate_clip_score(video_frames, text_prompt)\n",
        "\n",
        "    # Inception score\n",
        "    if verbose:\n",
        "        print(\"  Calculating Inception score...\")\n",
        "    is_mean, is_std = calculate_inception_score(video_frames)\n",
        "    results['inception_score'] = is_mean\n",
        "    results['inception_score_std'] = is_std\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Evaluation complete\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def print_evaluation_results(metrics, prompt=None):\n",
        "    \"\"\"\n",
        "    Print evaluation results in a formatted way.\n",
        "\n",
        "    Args:\n",
        "        metrics: Dictionary of evaluation metrics\n",
        "        prompt: Optional text prompt\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if prompt:\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "    # Temporal consistency\n",
        "    tc = metrics['temporal_consistency']\n",
        "    tc_status = \"Excellent\" if tc > 0.8 else \"Good\" if tc > 0.7 else \"Fair\" if tc > 0.6 else \"Poor\"\n",
        "    print(f\"Temporal Consistency: {tc:.4f} [{tc_status}]\")\n",
        "    print(f\"  Target: >0.75, Higher = more consistent motion\")\n",
        "\n",
        "    # Sharpness\n",
        "    sharp = metrics['sharpness']\n",
        "    sharp_status = \"Excellent\" if sharp > 300 else \"Good\" if sharp > 150 else \"Fair\" if sharp > 50 else \"Poor\"\n",
        "    print(f\"\\nSharpness: {sharp:.2f} [{sharp_status}]\")\n",
        "    print(f\"  Target: >150, Higher = sharper image\")\n",
        "\n",
        "    # Contrast\n",
        "    contrast = metrics['contrast']\n",
        "    contrast_status = \"Excellent\" if contrast > 70 else \"Good\" if contrast > 50 else \"Fair\" if contrast > 30 else \"Poor\"\n",
        "    print(f\"\\nContrast: {contrast:.2f} [{contrast_status}]\")\n",
        "    print(f\"  Target: >50, Higher = better contrast\")\n",
        "\n",
        "    # Brightness\n",
        "    brightness = metrics['brightness']\n",
        "    brightness_status = \"Good\" if 100 <= brightness <= 150 else \"Too Bright\" if brightness > 150 else \"Too Dark\"\n",
        "    print(f\"\\nBrightness: {brightness:.2f} [{brightness_status}]\")\n",
        "    print(f\"  Target: 100-150, Proper exposure\")\n",
        "\n",
        "    # Motion smoothness\n",
        "    motion = metrics['motion_smoothness']\n",
        "    motion_status = \"Excellent\" if motion > 0.7 else \"Good\" if motion > 0.5 else \"Fair\" if motion > 0.3 else \"Poor\"\n",
        "    print(f\"\\nMotion Smoothness: {motion:.4f} [{motion_status}]\")\n",
        "    print(f\"  Target: >0.55, Higher = smoother motion\")\n",
        "\n",
        "    # CLIP score\n",
        "    if 'clip_score' in metrics:\n",
        "        clip = metrics['clip_score']\n",
        "        clip_status = \"Excellent\" if clip > 0.3 else \"Good\" if clip > 0.25 else \"Fair\" if clip > 0.2 else \"Poor\"\n",
        "        print(f\"\\nCLIP Score: {clip:.4f} [{clip_status}]\")\n",
        "        print(f\"  Target: >0.25, Higher = better text alignment\")\n",
        "\n",
        "    # Inception score\n",
        "    inception = metrics['inception_score']\n",
        "    inception_status = \"Excellent\" if inception > 3.5 else \"Good\" if inception > 2.5 else \"Fair\" if inception > 1.5 else \"Poor\"\n",
        "    print(f\"\\nInception Score: {inception:.4f} +/- {metrics['inception_score_std']:.4f} [{inception_status}]\")\n",
        "    print(f\"  Target: >2.5, Higher = more realistic\")\n",
        "\n",
        "    # Overall assessment\n",
        "    scores = [tc, motion, sharp/500, contrast/80]\n",
        "    if 'clip_score' in metrics:\n",
        "        scores.append(metrics['clip_score'] * 2.5)\n",
        "    overall = np.mean(scores)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"OVERALL ASSESSMENT\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Overall Score: {overall:.3f}\")\n",
        "\n",
        "    if overall > 0.8:\n",
        "        assessment = \"EXCELLENT - Production ready\"\n",
        "    elif overall > 0.7:\n",
        "        assessment = \"GOOD - High quality\"\n",
        "    elif overall > 0.6:\n",
        "        assessment = \"FAIR - Acceptable quality\"\n",
        "    else:\n",
        "        assessment = \"NEEDS IMPROVEMENT\"\n",
        "\n",
        "    print(f\"Assessment: {assessment}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "print(\"Comprehensive evaluation function ready\")"
      ],
      "metadata": {
        "id": "comprehensive_function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785e6732-a31c-42b6-cb2d-a9a859fb69df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comprehensive evaluation function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Batch Evaluation Function"
      ],
      "metadata": {
        "id": "batch_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_multiple_videos(video_list, prompts=None, output_dir=\"evaluation_results\"):\n",
        "    \"\"\"\n",
        "    Evaluate multiple videos and generate comparative analysis.\n",
        "\n",
        "    Args:\n",
        "        video_list: List of video frame tensors\n",
        "        prompts: Optional list of text prompts\n",
        "        output_dir: Directory to save results\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with individual and aggregate results\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    print(\"=\"*70)\n",
        "    print(f\"EVALUATING {len(video_list)} VIDEOS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    all_results = []\n",
        "\n",
        "    # Evaluate each video\n",
        "    for i, video_frames in enumerate(video_list):\n",
        "        prompt = prompts[i] if prompts and i < len(prompts) else None\n",
        "\n",
        "        print(f\"\\nVideo {i+1}/{len(video_list)}\")\n",
        "        if prompt:\n",
        "            print(f\"Prompt: {prompt}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Evaluate\n",
        "        metrics = evaluate_video_comprehensive(video_frames, text_prompt=prompt, verbose=False)\n",
        "\n",
        "        # Print key metrics\n",
        "        print(f\"Temporal Consistency: {metrics['temporal_consistency']:.4f}\")\n",
        "        print(f\"Sharpness: {metrics['sharpness']:.2f}\")\n",
        "        print(f\"Motion Smoothness: {metrics['motion_smoothness']:.4f}\")\n",
        "        if 'clip_score' in metrics:\n",
        "            print(f\"CLIP Score: {metrics['clip_score']:.4f}\")\n",
        "        print(f\"Inception Score: {metrics['inception_score']:.4f}\")\n",
        "\n",
        "        result = {\n",
        "            'video_index': i,\n",
        "            'prompt': prompt,\n",
        "            'metrics': metrics\n",
        "        }\n",
        "        all_results.append(result)\n",
        "\n",
        "    # Calculate aggregate statistics\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"AGGREGATE STATISTICS\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    metric_names = ['temporal_consistency', 'sharpness', 'contrast', 'brightness',\n",
        "                   'motion_smoothness', 'clip_score', 'inception_score']\n",
        "\n",
        "    aggregate = {}\n",
        "    for metric_name in metric_names:\n",
        "        values = [r['metrics'][metric_name] for r in all_results if metric_name in r['metrics']]\n",
        "        if values:\n",
        "            aggregate[metric_name] = {\n",
        "                'mean': np.mean(values),\n",
        "                'std': np.std(values),\n",
        "                'min': np.min(values),\n",
        "                'max': np.max(values)\n",
        "            }\n",
        "\n",
        "            print(f\"\\n{metric_name.replace('_', ' ').title()}:\")\n",
        "            print(f\"  Mean: {aggregate[metric_name]['mean']:.4f} +/- {aggregate[metric_name]['std']:.4f}\")\n",
        "            print(f\"  Range: [{aggregate[metric_name]['min']:.4f}, {aggregate[metric_name]['max']:.4f}]\")\n",
        "\n",
        "    # Create visualization\n",
        "    print(\"\\nCreating visualization...\")\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    fig.suptitle('Video Evaluation Results', fontsize=16)\n",
        "\n",
        "    metrics_to_plot = ['temporal_consistency', 'sharpness', 'contrast',\n",
        "                      'motion_smoothness', 'clip_score', 'inception_score']\n",
        "\n",
        "    for idx, metric_name in enumerate(metrics_to_plot):\n",
        "        if idx >= 6:\n",
        "            break\n",
        "\n",
        "        row = idx // 3\n",
        "        col = idx % 3\n",
        "        ax = axes[row, col]\n",
        "\n",
        "        values = [r['metrics'].get(metric_name, 0) for r in all_results]\n",
        "\n",
        "        bars = ax.bar(range(len(values)), values, color='steelblue', alpha=0.7)\n",
        "        ax.axhline(y=np.mean(values), color='red', linestyle='--',\n",
        "                   label=f'Mean: {np.mean(values):.3f}', linewidth=2)\n",
        "\n",
        "        ax.set_xlabel('Video', fontsize=10)\n",
        "        ax.set_ylabel('Score', fontsize=10)\n",
        "        ax.set_title(metric_name.replace('_', ' ').title(), fontsize=11)\n",
        "        ax.set_xticks(range(len(values)))\n",
        "        ax.set_xticklabels([f\"V{i+1}\" for i in range(len(values))])\n",
        "        ax.legend(fontsize=9)\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plot_path = os.path.join(output_dir, 'evaluation_results.png')\n",
        "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"Visualization saved: {plot_path}\")\n",
        "\n",
        "    # Save results to JSON\n",
        "    results_json = {\n",
        "        'individual_results': [\n",
        "            {\n",
        "                'video_index': r['video_index'],\n",
        "                'prompt': r['prompt'],\n",
        "                'metrics': r['metrics']\n",
        "            }\n",
        "            for r in all_results\n",
        "        ],\n",
        "        'aggregate_statistics': aggregate\n",
        "    }\n",
        "\n",
        "    json_path = os.path.join(output_dir, 'evaluation_results.json')\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(results_json, f, indent=2)\n",
        "\n",
        "    print(f\"Results saved: {json_path}\")\n",
        "    print(\"\\nEvaluation complete\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return results_json\n",
        "\n",
        "\n",
        "print(\"Batch evaluation function ready\")"
      ],
      "metadata": {
        "id": "batch_function",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08b81236-d412-44ad-c173-2842ff7465e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch evaluation function ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_video_from_file(video_path):\n",
        "    \"\"\"\n",
        "    Load video file and convert to tensor format.\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to video file (.mp4)\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [T, C, H, W] normalized to [0, 1]\n",
        "    \"\"\"\n",
        "    import imageio\n",
        "\n",
        "    print(f\"Loading video from: {video_path}\")\n",
        "\n",
        "    # Read video\n",
        "    reader = imageio.get_reader(video_path)\n",
        "    frames = []\n",
        "\n",
        "    for frame in reader:\n",
        "        # Convert to tensor [H, W, C] -> [C, H, W]\n",
        "        frame_tensor = torch.from_numpy(frame).permute(2, 0, 1).float()\n",
        "        # Normalize to [0, 1]\n",
        "        frame_tensor = frame_tensor / 255.0\n",
        "        frames.append(frame_tensor)\n",
        "\n",
        "    reader.close()\n",
        "\n",
        "    # Stack to [T, C, H, W]\n",
        "    video_tensor = torch.stack(frames)\n",
        "\n",
        "    print(f\"Loaded {len(frames)} frames, shape: {video_tensor.shape}\")\n",
        "\n",
        "    return video_tensor"
      ],
      "metadata": {
        "id": "BLGy1QB7zKWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Example Usage\n",
        "\n",
        "Below is an example of how to use the evaluation functions. Replace `video_frames` with your actual generated video tensor."
      ],
      "metadata": {
        "id": "example_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Evaluate a single video from file\n",
        "\n",
        "# Path to your generated video\n",
        "video_path = \"production_video_3.mp4\"\n",
        "text_prompt = \"pouring water into glass\"\n",
        "\n",
        "# Load video frames from file\n",
        "video_frames = load_video_from_file(video_path)\n",
        "\n",
        "# Evaluate\n",
        "metrics = evaluate_video_comprehensive(video_frames, text_prompt=text_prompt)\n",
        "\n",
        "# Print results\n",
        "print_evaluation_results(metrics, prompt=text_prompt)\n",
        "# print(\"Example usage code ready\")\n",
        "# print(\"Uncomment the lines above and replace with your actual video data\")"
      ],
      "metadata": {
        "id": "example_usage",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1eaa7fa-af69-4384-9660-514515a5e4d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading video from: production_video_3.mp4\n",
            "Loaded 20 frames, shape: torch.Size([20, 3, 320, 320])\n",
            "Evaluating video...\n",
            "  Calculating temporal consistency...\n",
            "  Calculating frame quality...\n",
            "  Calculating motion smoothness...\n",
            "  Calculating CLIP score...\n",
            "  Calculating Inception score...\n",
            "Evaluation complete\n",
            "======================================================================\n",
            "EVALUATION RESULTS\n",
            "======================================================================\n",
            "Prompt: pouring water into glass\n",
            "----------------------------------------------------------------------\n",
            "Temporal Consistency: 0.8947 [Excellent]\n",
            "  Target: >0.75, Higher = more consistent motion\n",
            "\n",
            "Sharpness: 133.00 [Fair]\n",
            "  Target: >150, Higher = sharper image\n",
            "\n",
            "Contrast: 15.14 [Poor]\n",
            "  Target: >50, Higher = better contrast\n",
            "\n",
            "Brightness: 121.70 [Good]\n",
            "  Target: 100-150, Proper exposure\n",
            "\n",
            "Motion Smoothness: 0.7165 [Excellent]\n",
            "  Target: >0.55, Higher = smoother motion\n",
            "\n",
            "CLIP Score: 0.3292 [Excellent]\n",
            "  Target: >0.25, Higher = better text alignment\n",
            "\n",
            "Inception Score: 1.2334 +/- 0.1114 [Poor]\n",
            "  Target: >2.5, Higher = more realistic\n",
            "\n",
            "======================================================================\n",
            "OVERALL ASSESSMENT\n",
            "======================================================================\n",
            "Overall Score: 0.578\n",
            "Assessment: NEEDS IMPROVEMENT\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Evaluate a single video from file\n",
        "\n",
        "# Path to your generated video\n",
        "video_path = \"production_video_4.mp4\"\n",
        "text_prompt = \"putting spoon on table\"\n",
        "\n",
        "# Load video frames from file\n",
        "video_frames = load_video_from_file(video_path)\n",
        "\n",
        "# Evaluate\n",
        "metrics = evaluate_video_comprehensive(video_frames, text_prompt=text_prompt)\n",
        "\n",
        "# Print results\n",
        "print_evaluation_results(metrics, prompt=text_prompt)\n",
        "# print(\"Example usage code ready\")\n",
        "# print(\"Uncomment the lines above and replace with your actual video data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCO1exj11ky4",
        "outputId": "6f00d075-1584-434b-b9f2-43735ec82055"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading video from: production_video_4.mp4\n",
            "Loaded 20 frames, shape: torch.Size([20, 3, 320, 320])\n",
            "Evaluating video...\n",
            "  Calculating temporal consistency...\n",
            "  Calculating frame quality...\n",
            "  Calculating motion smoothness...\n",
            "  Calculating CLIP score...\n",
            "  Calculating Inception score...\n",
            "Evaluation complete\n",
            "======================================================================\n",
            "EVALUATION RESULTS\n",
            "======================================================================\n",
            "Prompt: putting spoon on table\n",
            "----------------------------------------------------------------------\n",
            "Temporal Consistency: 0.8820 [Excellent]\n",
            "  Target: >0.75, Higher = more consistent motion\n",
            "\n",
            "Sharpness: 66.92 [Fair]\n",
            "  Target: >150, Higher = sharper image\n",
            "\n",
            "Contrast: 16.71 [Poor]\n",
            "  Target: >50, Higher = better contrast\n",
            "\n",
            "Brightness: 120.36 [Good]\n",
            "  Target: 100-150, Proper exposure\n",
            "\n",
            "Motion Smoothness: 0.5418 [Good]\n",
            "  Target: >0.55, Higher = smoother motion\n",
            "\n",
            "CLIP Score: 0.3010 [Excellent]\n",
            "  Target: >0.25, Higher = better text alignment\n",
            "\n",
            "Inception Score: 1.4980 +/- 0.2023 [Poor]\n",
            "  Target: >2.5, Higher = more realistic\n",
            "\n",
            "======================================================================\n",
            "OVERALL ASSESSMENT\n",
            "======================================================================\n",
            "Overall Score: 0.504\n",
            "Assessment: NEEDS IMPROVEMENT\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Batch Evaluation Example\n",
        "\n",
        "Example of evaluating multiple videos at once."
      ],
      "metadata": {
        "id": "batch_example_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Evaluate multiple videos\n",
        "# Assuming you have a list of video tensors\n",
        "\n",
        "# Example setup (replace with your actual videos)\n",
        "# video_list = [video1, video2, video3]  # Each shape: [T, C, H, W]\n",
        "# prompts = [\n",
        "#     \"placing bottle on table\",\n",
        "#     \"pouring water into glass\",\n",
        "#     \"person placing objects\"\n",
        "# ]\n",
        "\n",
        "# Evaluate all videos\n",
        "# results = evaluate_multiple_videos(\n",
        "#     video_list=video_list,\n",
        "#     prompts=prompts,\n",
        "#     output_dir=\"evaluation_results\"\n",
        "# )\n",
        "\n",
        "# Access results\n",
        "# print(results['aggregate_statistics'])\n",
        "\n",
        "print(\"Batch evaluation example ready\")\n",
        "print(\"Uncomment the lines above and replace with your actual video data\")"
      ],
      "metadata": {
        "id": "batch_example"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides comprehensive evaluation metrics for your text-to-video model:\n",
        "\n",
        "**Functions available:**\n",
        "- `calculate_temporal_consistency(video_frames)` - Frame-to-frame coherence\n",
        "- `calculate_frame_quality(video_frames)` - Sharpness, contrast, brightness\n",
        "- `calculate_motion_smoothness(video_frames)` - Optical flow smoothness\n",
        "- `calculate_clip_score(video_frames, text_prompt)` - Text-video alignment\n",
        "- `calculate_inception_score(video_frames)` - Diversity and realism\n",
        "- `evaluate_video_comprehensive(video_frames, text_prompt)` - All metrics at once\n",
        "- `evaluate_multiple_videos(video_list, prompts)` - Batch evaluation\n",
        "\n",
        "**Expected Performance (10K Model):**\n",
        "- Temporal Consistency: 0.75 - 0.85\n",
        "- Sharpness: 150 - 300\n",
        "- Motion Smoothness: 0.55 - 0.70\n",
        "- CLIP Score: 0.27 - 0.33\n",
        "- Inception Score: 2.5 - 3.5\n",
        "\n",
        "**To use:**\n",
        "1. Generate your video with the model\n",
        "2. Pass the video tensor to `evaluate_video_comprehensive()`\n",
        "3. Review the metrics\n",
        "4. Use `print_evaluation_results()` for formatted output"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}